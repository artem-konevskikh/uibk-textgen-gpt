
 
In the last decade, the proliferation of machine learning, the adoption of multi-scalar robotization protocols, the progress in genetic engineering and synthetic biology, all have conjured the evolution of novel synthetic fabrication strategies in architecture. These are now actualizing cybernertic design methods and embedding bio-computational intelligence which changes our understanding of the relationship between the human and the machinic bodies as well as the relationship between the architectural and the natural landscape. The milieu is co-designed by the agents that inhabit it. From today's perspective, we differentiate between more species than just humans and animals. Sensory perception of the environment applies just as much to intelligent technologies equipped with artificial sensorial systems and to the artificial intelligence agents. Intelligent mechanical agents are no longer a mere mechanism in the Cartesian sense, they take part in the milieu by playing the role of perception organs, organs of analysis, and organs of synthesis. An aesthetical dimension is defined by this system of interactions now taking place between biological, human and machinic agents. In the context of architecture this dimension is closely related to the cybernetic concept of autopoiesis, which in turn, relates to self-organization and self-creation in biological and social systems. There is an important implication worth underlining here: “the architect is no longer a designer of discrete objects, matter and space, but a designer of systems with complex components and multi-layered relationships. This notion has stimulated the emergence of new fabrication strategies and new design methods, suggesting that it is necessary to think about not only design for humans, but about complex design systems, which includes human, machinic and biological agents. With the implementation of synthetic intelligence in novel design protocols, synthetic landscape design becomes a model of interaction between human, machinic and biological agents and contributes to the dissolution of the conceptual boundaries between these  systems. Cybernetic design morphs natural and artificial systems, proposing an emergent bio-machinic aesthetics.
 
From within this milieu the object of design cannot be limited to assemblages of discrete parts with homogeneous properties, like in modular systems, or to continuous gradient fields of material articulation, as in parametricism. Rather it engenders bio-synthetically programmed systems that are materially heterogeneous, morphologically convoluted and functionally opportunistic. The apparent beauty of these systems is a measure of  their “bio-computational intelligence”.
 
Synthetic Landscape – a design philosophy that encompasses all the processes and systems, human, animal, microbiological and digital, that are currently accelerating the transformation of our Urbansphere. The stratum of the synthetic landscape is a repository of the microclimates, microchemical compositions, digital data flows, geological catastrophes, technological traces and multispecies interactions, recorded in time. An enormous amount of data trapped in the layers of landscape. If we decode these dataspaces by drawing and simulating the complex web of relationships, we might unfold synthetic forms of intelligence embedded in the morphology of the landscape. From the numerical perspective of machine learning, notions such as flow, movement, form and style can be all described as distributions of a pattern. Thus, to study intelligence of synthetic landscape would mean to study the manner by which information flows and patterns are absorbed and treated by AI.
 
Instead of treating AI as a machine hallucination, we consider machine learning as an instrument of knowledge magnification that helps to perceive features, patterns, and correlations through vast spaces of data beyond human reach. A machine learning system can be compared to a device that maps and perceives complex patterns through vast spaces of data. However, “what machine learning calculates is not an exact pattern but the statistical distribution of a pattern”. Thus, to study the impact of AI is to study the degree by which information flows are diffracted, distorted and lost by AI. 
 
Rather than treating it as an alien cognition, AI will be considered as an instrument to see and navigate the space of knowledge. We will use computational methods to analyse patterns in historical culture in the available digitized text archives in order to form new definitions of “synthetic landscape” through reading, coding and drawing. We will aim to create a framework of conversation with the intelligence of technological, algorithmic, biological, geological and ecological systems which are shaping micro-chemical and bio-digital composition of synthetic landscape.
 
We live in an age of neo-mechanism, in which technical objects are becoming organic”
 
Architecture, as one of the principal agents in modelling the interaction between bodies and space, must now actualize this morphing relationship by dissolving the boundaries between human and non-human systems. The interaction of these heterogeneous systems becomes an ecology itself, which is based on the concept of the organism. 
 
developing a new bio-computational design workflow that enables pairing what is algorithmically drawn with what is biologically grown.
The workflow includes four main levels of computation. Input data reading, biotic-abiotic analysis, network analysis and finally scenario modelling.

This new aesthetic of nature projects the design practice into the realm of urban “waste”, the byproduct of urbanisation, where micro-organisms such as virus, bacteria and fungi become protagonists.

Developing a specific bio-digital language is a way of enhancing intra-systemic communication. A communication that is not just digital or verbal, but is enabled by a visual language that people as well as non-human species and intelligent machines can read and reproduce. It may be a provocative language to many, since it questions their assumptions about the concept of nature, or the relation- ship between nature and technology in the Anthropocene age. 

the impact of artificial systems on the natural bio- sphere will be indeed global, but their agency will no longer be entirely human. New bio- computational design workflows will eventual- ly evolve, pairing what is algorithmically drawn to what is biologically grown.
 
 
As an instrument of knowledge, machine learning is composed of an object to be observed (training dataset), an instrument of observation (learning algorithm) and a final representation (statistical model). The assemblage of these three elements is proposed here as a spurious and baroque diagram of machine learning, extravagantly termed Nooscope. Staying with the analogy of optical media, the information flow of machine learning is like a light beam that is projected by the training data, compressed by the algorithm and diffracted towards the world by the lens of the statistical model.

AI is not a thinking automaton but an algorithm that performs pattern recognition. The notion of pattern recognition contains issues that must be elaborated upon. 

The algorithms of AI are often evoked as alchemic formulas, capable of distilling ‘alien’ forms of intelligence.
 
The age of biopolitics demonstrates how life and art are both infused with technology. Life is no longer a natural event. Life can be artificially altered to the point now that viewers of life are unable to distinguish between what is natural and what has been a creation of science. For example, there are no visual distinctions between a G.M apple and one that grew naturally, or, how could you tell by looking if your friend was conceived naturally or by IVF? The difference between real life and artificial life can only be determined through narratives.
‘aesthetics’ is a responsiveness to the pattern which connects. The pattern which connects is a meta-pattern. It is a pattern of patterns. It is that meta-pattern which defines the vast generalisation that indeed it is patterns which connect.”

the emergence of artificial intelligence in technology and in the arts
already warrants a more than robust amount of natural discomfort, and the feeling of
“alienation”  which is the result of the ongoing post industrial separation of the
minds of the thinkers from the tools of computation.  In this sense and from an
aesthetic point of view, machine learning can be interpreted as the “strangement
device” . 
We live in an age of neo-mechanism, in which technical objects are becoming organic. Towards the end of the eighteenth century, Kant wanted to give a new life to philosophy in the wake of mechanism, so he set up a new condition of philosophizing, namely the organic. Being mechanistic doesn’t necessarily mean being related to machines; rather, it refers to machines that are built on linear causality, for example clocks, or thermodynamic machines like the steam engine. 
If philosophy since Kant has mechanism as its counterpart, it seems that today, as you and others have observed, this counterpart has been transformed into an organic being. Our computers, smartphones, and domestic robots are no longer mechanical but are rather becoming organic. I propose this as a new condition of philosophizing. Philosophy has to painfully break away from the self-contentment of organicity, and open up new realms of thinking.

Philosophy has to negate the totalizing tendency in organic thinking, which is in the process of being implemented in different technical apparatuses, from social credit systems to the “superintelligence.”
What if these machines are no longer simply “organized inorganic” entities, but rather gigantic systems in the making? These systems are now the organizing agents of human lives and social orders. It seems to me necessary to return to these questions and to extend the concept of organology already developed by anthropologists and philosophers to the analysis of our actual situation.
The concept of cosmotechnics concerns the idea that different cultures and epochs have different ways of thinking about technology.
The notion of recursivity represents an epistemological break from the mechanistic worldview that dominated the seventeenth and eighteenth centuries, especially Cartesian mechanism. 
Contingency is central to recursivity. In the mechanical mode of operation, which is built on linear causation, a contingent event may lead to the collapse of the system. For example, machinery may malfunction and cause an industrial catastrophe. But in the recursive mode of operation, contingency is necessary since it enriches the system and allows it to develop. A living organism can absorb contingency and render it valuable. So can today’s machine learning.

as a technical definition of intelligence and, second, as a political form that would be autonomous from society and the human. In the expression ‘artificial intelligence’ the adjective ‘artificial’ carries the myth of the technology’s autonomy: it hints to caricatural ‘alien minds’ that self-reproduce in silico but, actually, mystifies two processes of proper alienation: the growing geopolitical autonomy of hi-tech companies and the invisibilization of workers’ autonomy worldwide. The modern project to mechanise human reason has clearly mutated, in the 21st century, into a corporate regime of knowledge extractivism and epistemic colonialism. This is unsurprising, since machine learning algorithms are the most powerful algorithms for information compression.

Machine learning is not bringing a new dark age but one of diffracted rationality, in which, as it will be shown, an episteme of causation is replaced by one of automated correlations. More in general, AI is a new regime of truth, scientific proof, social normativity and rationality, which often does take the shape of a statistical hallucination. This diagram manifesto is another way to say that AI, the king of computation (patriarchal fantasy of mechanised knowledge, ‘master algorithm’ and alpha machine) is naked.

As an instrument of knowledge, machine learning is composed of an object to be observed (training dataset), an instrument of observation (learning algorithm) and a final representation (statistical model). The assemblage of these three elements is proposed here as a spurious and baroque diagram of machine learning, extravagantly termed Nooscope. Staying with the analogy of optical media, the information flow of machine learning is like a light beam that is projected by the training data, compressed by the algorithm and diffracted towards the world by the lens of the statistical model. 
AI is not a thinking automaton but an algorithm that performs pattern recognition. The notion of pattern recognition contains issues that must be elaborated upon. What is a pattern, by the way? Is a pattern uniquely a visual entity? What does it mean to read social behaviours as patterns? 
What a neural network computes, is not an exact pattern but the statistical distribution of a pattern. Just scraping the surface of the anthropomorphic marketing of AI, one finds another technical and cultural object that needs examination: the statistical model. 


The end of unilateral globalization and the arrival of the Anthropocene force us to talk about cosmopolitics. These two factors correlate with one another and correspond to two different senses of the word “cosmopolitics”: cosmopolitics as a commercial regime, and cosmopolitics as a politics of nature.

 
While engendering a new concept of ecology, our new understanding of bio-computational intelligence also implies a change in human nature. The postmodern concept of corporeality and physicality is expanded and rethought, considering the symbiotic processes that are now occurring across previously segregated realms. Among them analog/digital, biological/technological, intimate/social, natural/artificial processes. The procedural biotechnological expansion of human embodiment brings it to a new round of hybridity, progressively absorbing the ambience. The human body is perceived as a set of organo-machinical processes, regulated via bacterial and informational exchanges with the surrounding environment. Thus, the embodiment is reorganized and dispersed within the boundaries of nature, technology and environment, which become an immanent component of the body. Consequently, (in)organic body turns to be an elastic material for further transformations. Borders between the human body, the architectural body and the territorial body are dissolved. All are now described as an “assemblage” of information rather then a collage of separate entities. Design becomes a transcalar instrument for reading and decoding these sets of data. Our minds are now ecologically extended, and the aesthetic dimension is “responsiveness to the pattern which connects.
 
For a more nuanced philosophical understanding of how we adapt to technology, and absorb it so that it becomes a prosthetic extension to our own bodies, we should perhaps turn to Maurice Merleau- Ponty, who recognizes that any tool can eventually be appropriated as part of our extended body schema, so that we come to experience the word through our tools.
 
Andy Clark and David Chalmers take this idea even further, and argue that a tool can even become a part of our extended mind. We have absorbed our mobile phones and other external devices at a symbolic level, so that they have become part of who we are. They have become prosthetic extensions of our minds.
 
Why limit ourselves to the human mind, when the whole body has been augmented by the increasing introduction of these new technologies? For Hayles, we cannot separate the mind from the body, as though the mind is simply housed in the body. The mind is precisely part of the body. The body has indeed been extended by these new technologies, such that the traditional concept of humanism has now become untenable. We need to go so far as to challenge the traditional liberal concept of humanism and accept that we now operate within a posthuman condition.
 
The term, cyborg, was first coined to refer to the potential of enhancing the human body in order for i t to survive in extraterrestrial environments. It is a cybernetic organism, ‘a hybrid of machine and organism, a creature of social reality as well as a creature of fiction. Moreover, this hybrid condition, should be understood not as static, but as one that is continually adapting and mutating: ‘Already in the few decades that they have existed, they have mutated, in fact and fiction, into second-order entities like genomic and electronic databases and the other denizens of the zone called cyberspaces’. As such, we have now evolved, so that the cyborg has become our predominant disposition: ‘By the late twentieth century, our time, a mythic time, we are all chimeras, theorized and fabricated hybrids of machine and organism; in short, we are cyborgs.’
 
The cyborg is a potent cultural icon of the late twentieth century. It conjures images of human- machine hybrids and the physical merging of flesh and electronic circuitry. My goal is to hijack that image and to reshape it, revealing it as a disguised vision of (oddly) our own biological nature. For what is special about human brains, and what best explains the distinctive features of human intelligence, is precisely their ability to enter into deep and complex relationships with nonbiological constructs, props, and aids.
 
It is precisely our capacity to adapt so effectively to new tools that, for Clark, makes human beings natural born cyborgs’. We are naturally adaptive creatures, because our brains are themselves so adaptive. Neuroscientists tell us that the human brain is ‘plastic.’ The brain has the capacity to adapt constantly to ever changing circumstances, and it is its very 'plasticity' that gives human beings the capacity to appropriate new technologies so that they become part of our extended sense of self: : ‘It is the presence of this unusual plasticity that makes humans (but not dogs, cats, or elephants) natural- born cyborgs: beings primed by Mother Nature to annex wave upon wave of external elements and structures as part and parcel of their own extended minds.’
 
Cyborg culture does not make us posthuman, but rather affirms us as being quintessentially human, in that the innate capacity to adapt, that plays such a significant role in what it is to be human. This, then, allows Clark to challenge Hayles’s claim that we have become posthuman: ‘Such extensions should not be thought of as rendering us in any way post-human; not because they are not deeply transformative but because we humans are naturally designed to be the subjects of just such repeated transformations!
 
It is in the context of this burgeoning ‘cyborg culture’ – where humans have been augmented to become superhuman – that we can explore the full potential of AI as being not an end in itself, but a prosthetic device that can enhance the natural intelligence of the human being. For if the mind – or indeed the embodied self – can be extended through the phone and other devices, could we not also argue that intelligence itself can be enhanced through artificial intelligence?
Increasingly nowadays we are hearing references not to straightforward ‘artificial intelligence’ but to ‘extended intelligence’ [EI]. Ultimately, the most productive strategy is to see the relationship between AI and human intelligence not as one of competition, but rather as a potential synergy between the two, whereby AI operates in tandem with human intelligence, and becomes an extension to human intelligence. As Joi Ito puts it:
Instead of thinking about machine intelligence in terms of humans vs. machines, we should consider the system that integrates humans and machines—not artificial intelligence, but extended intelligence. Instead of trying to control or design or even understand systems, it is more important to design systems that participate as responsible, aware and robust elements of even more complex systems. And we must question and adapt our own purpose and sensibilities as designers and components of the system for a much more humble approach: Humility over Control. We could call it “participant design”—design of systems as and by participants—that is more akin to the increase of a flourishing function, where flourishing is a measure of vigor and health rather than scale or power. We can measure the ability for systems to adapt creatively, as well as their resilience and their ability to use resources in an interesting way
An alternative way of understanding this coupling of human and AI is as a form of ‘intelligence augmentation - IA. AI makes machines autonomous and detached from humans; IA, in on the other hand, puts humans in control and leverages computing power to amplify our capabilities”.’ 46 Whether we call it ‘extended intelligence’ or ‘intelligence augmentation’, the basic concept of coupling human intelligence with AI remains the same. As such, we should not be referring to AI in isolation, so much as a potential extension or augmentation of human intelligence. This extended or augmented intelligence – this synergy between our bodies and machines – will undoubtedly prove enormously productive.
Digital simulations can often help us to understand analogue behaviours. For example, it was not until Craig Reynolds had produced a digital model for the flocking behaviour of birds using ‘boids’, that we were able to understand the behaviour of actual birds. This is not to say that AI would serve as a‘digital model’ of human intelligence – since, for the moment at least, AI cannot track the human brain – but nonetheless the principles that govern AI might offer some clues about the principles that govern human intelligence. Could AI, then, provide us with a mirror in which to understand certain aspects of human intelligence, such as learning, consciousness and even creativity itself?
Take a neural network. Although modelled only loosely on the brain, a neural network might help us  to understand how the brain actually works. Both are black boxes, in that we know that they work, but  do not fully understand how they work. But might they share certain similarities? Might the brain, for example, also perform some form of backpropagation, as Geoffrey Hinton speculates?  After all,  although it had previously been assumed that the synapses in the brain operate in only one direction – feed forward – it has now been argued that they might operate in both directions – both feed forward and deed backwards. And could this principle allow the brain to critique or correct itself, just as backpropagation allows for the neural network to correct errors? Even if the brain does not operate in exactly the same way, it seems to be doing something similar. As Joshua Bengio observes, “Backpropagation works incredibly well, and it suggests that maybe the brain is doing something similar – not exactly the same, but with the same function.
 

Inspirational images of buildings and urban design proposals are now being generated automatically by a process often referred to as ‘hallucination’, a term that also evokes the seemingly hallucinatory nature of the images generated
 



 
Natural-Born Cyborgs: Minds, Technologies, and the Future of Human Intelligence 
ANDY CLARK 
My body is an electronic virgin. I incorporate no silicon chips, no retinal or cochlear implants, no pacemaker. I don’t even wear glasses (though I do wear clothes), but I am slowly becoming more and more a cyborg. So are you. Pretty soon, and still without the need for wires, surgery, or bodily alterations, we shall all be kin to the Terminator, to Eve 8, to Cable . . . just fill in your favorite fictional cyborg. Perhaps we already are. For we shall be cyborgs not in the merely superficial sense of combining flesh and wires but in the more profound sense of being human-technology symbionts: thinking and reasoning systems whose minds and selves are spread across biological brain and nonbiological circuitry. This book is the story of that transition and of its roots in some of the most basic and characteristic facts about human nature. For human beings, I want to convince you, are natural- born cyborgs. 
This may sound like futuristic mumbo-jumbo, and I happily confess that I wrote the preceding paragraph with an eye to catching your atten- tion, even if only by the somewhat dangerous route of courting your imme- diate disapproval! But I do believe that it is the plain and literal truth. I believe, to be clear, that it is above all a SCIENTIFIC truth, a reflection of some deep and important facts about (a whiff of paradox here?) our spe- cial, and distinctively HUMAN, nature. Certainly I don’t think this tendency 
toward cognitive hybridization is a modern development. Rather, it is an aspect of our humanity, which is as basic and ancient as the use of speech and which has been extending its territory ever since. We see some of the “cognitive fossil trail” of the cyborg trait in the historical procession of potent cognitive technologies that begins with speech and counting, morphs first into written text and numerals, then into early printing (without move- able typefaces), on to the revolutions of moveable typefaces and the print- ing press, and most recently to the digital encodings that bring text, sound, and image into a uniform and widely transmissible format. Such technolo- gies, once up and running in the various appliances and institutions that surround us, do far more than merely allow for the external storage and transmission of ideas. They constitute, I want to say, a cascade of “mindware upgrades”: cognitive upheavals in which the effective architecture of the human mind is altered and transformed. 
It was about five years ago that I first realized we were, at least in that specific sense, all cyborgs. At that time I was busy directing a new interdis- ciplinary program in philosophy, neuroscience, and psychology at Wash- ington University in St. Louis. The realization wasn’t painful; it was, oddly, reassuring. A lot of things now seemed to fall into place: why we humans are so deeply different from the other animals, while being, quite demon- strably, not so very different in our neural and bodily resources; why it was so hard to build a decent thinking robot; why the recent loss of my laptop had hit me like a sudden and somewhat vicious type of (hopefully tran- sient) brain damage. 
I’d encountered the idea that we were all cyborgs once or twice before, but usually in writings on gender or in postmodernist (or post postmodernist) studies of text. What struck me in July 1997 was that this kind of story was the literal and scientific truth. The human mind, if it is to be the physical organ of human reason, simply cannot be seen as bound and restricted by the biological skinbag. In fact, it has never been thus restricted and bound, at least not since the first meaningful words were uttered on some ancestral plain. But this ancient seepage has been gathering momentum with the ad- vent of texts, PCs, coevolving software agents, and user-adaptive home and office devices. The mind is just less and less in the head. 
If we do not always see this, or if the idea seems outlandish or absurd, that is because we are in the grip of a simple prejudice: the prejudice that 
whatever matters about my mind must depend solely on what goes on inside my own biological skin-bag, inside the ancient fortress of skin and skull. This fortress has been built to be breached; it is a structure whose virtue lies in part in its capacity to delicately gear its activities in order to collaborate with external, nonbiological sources of order to better solve the problems of survival and reproduction. It is because we are so prone to think that the mental action is all, or nearly all, on the inside, that we have developed sciences and images of the mind that are, in a fundamental sense, inadequate to their self-proclaimed target. So it is actually important to begin to see ourselves aright—it matters for our science, our morals, and our sense of self. 
What, then, is the role of the biological brain, of those few pounds of squishy matter in your skull? The squishy matter is great at some things. It is expert at recognizing patterns, at perception, and at controlling physical actions, but it is not so well designed (as we’ll see) for complex planning and long, intricate, derivations of consequences. It is, to put it bluntly, bad at logic and good at Frisbee. It is both our triumph and our burden, how- ever, to have created a world so smart that it allows brains like ours to go where no animal brains have gone before. The story I want to tell is the story of that triumph, and of what it means for our understanding of our- selves: dumb thinkers in a smart world, or smart thinkers whose bound- aries are simply not those of skin and skull? 
The cyborg is a potent cultural icon of the late twentieth century. It conjures images of human-machine hybrids and the physical merging of flesh and electronic circuitry. My goal is to hijack that image and to reshape it, revealing it as a disguised vision of (oddly) our own biological nature. For what is special about human brains, and what best explains the distinctive features of human intelligence, is precisely their ability to enter into deep and complex relationships with nonbiological constructs, props, and aids. This ability, however, does not depend on physical wire-and-implant merg- ers, so much as on our openness to information-processing mergers. Such mergers may be consummated without the intrusion of silicon and wire into flesh and blood, as anyone who has felt himself thinking via the act of writing already knows. The familiar theme of “man the toolmaker” is thus taken one crucial step farther. Many of our tools are not just external props and aids, but they are deep and integral parts of the problem-solving systems we now identify as human intelligence. Such tools are best conceived as proper parts of the computational apparatus that constitutes our minds. 
The point is best made by the series of extended concrete examples that I develop in this book. Consider, as a truly simplistic cameo, the process of usingpenandpapertomultiplylargenumbers. Thebrainlearnstomake 
the most of its capacity for simple pattern completion (4 ? 4 = 16, 2 ? 7 = 14, etc.) by acting in concert with pen and paper, storing the intermedi- ate results outside the brain, then repeating the simple pattern completion process until the larger problem is solved. The brain thus dovetails its op- eration to the external symbolic resource. The reliable presence of such resources may become so deeply factored in that the biological brain alone is rendered unable to do the larger sums. 
Some educationalists fear this consequence, but I shall celebrate it as the natural upshot of that which makes us such potent problem-solving systems. It is because our brains, more than those of any other animal on the planet, are primed to seek and consummate such intimate relations with nonbiological resources that we end up as bright and as capable of abstract thought as we are. It is because we are natural-born cyborgs, for- ever ready to merge our mental activities with the operations of pen, paper, and electronics, that we are able to understand the world as we do. There has been much written about our imminent “post-human” future, but if I am right, this is a dangerous and mistaken image. The very things that sometimes seem most post-human, the deepest and most profound of our potential biotechnological mergers, will reflect nothing so much as their thoroughly human source. 
My cat Lolo is not a natural-born cyborg. This is so despite the fact that Lolo (unlike myself) actually does incorporate a small silicon chip. The chip is implanted below the skin of his neck and encodes a unique identi- fying bar code. The chip can be read by devices common in veterinarians’ offices and animal shelters; it identifies me as Lolo’s owner so we can be reunited if he is ever lost. The presence of this implanted device makes no difference to the shape of Lolo’s mental life or the range of projects and endeavors he undertakes. Lolo currently shows no signs of cat-machine symbiosis, and for that I am grateful. By contrast it is our special character, as human beings, to be forever driven to create, co-opt, annex, and exploit nonbiological props and scaffoldings. We have been designed, by Mother Nature, to exploit deep neural plasticity in order to become one with our best and most reliable tools. Minds like ours were made for mergers. Tools- R-Us, and always have been. 
New waves of user-sensitive technology will bring this age-old process to a climax, as our minds and identities become ever more deeply enmeshed in a nonbiological matrix of machines, tools, props, codes, and semi-intelligent daily objects. We humans have always been adept at dovetailing our minds and skills to the shape of our current tools and aids. But when those tools and aids start dovetailing back—when our technologies actively, automati- cally, and continually tailor themselves to us just as we do to them—then the line between tool and user becomes flimsy indeed. Such technologies will be less like tools and more like part of the mental apparatus of the per- son. They will remain tools in only the thin and ultimately paradoxical sense in which my own unconsciously operating neural structures (my hippocam- pus, my posterior parietal cortex) are tools. I do not really “use” my brain. There is no user quite so ephemeral. Rather, the operation of the brain makes me who and what I am. So too with these new waves of sensitive, interactive technologies. As our worlds become smarter and get to know us better and better, it becomes harder and harder to say where the world stops and the person begins. 
Mind-expanding technologies come in a surprising variety of forms. They include the best of our old technologies: pen, paper, the pocket watch, the artist’s sketchpad, and the old-time mathematician’s slide rule. They in- clude all the potent, portable machinery linking the user to an increasingly responsive world wide web. Very soon, they will include the gradual smart- ening-up and interconnection of the many everyday objects that populate our homes and offices. 
However, this is not primarily a book about new technology. Rather, it is about us, about our sense of self, and about the nature of the human mind. It targets the complex, conflicted, and remarkably ill-understood relation- ship between biology, nature, culture, and technology. More a work of science-sensitive philosophy than a futurist manifesto, my goal is not to guess at what we might soon become but to better appreciate what we already are: creatures whose minds are special precisely because they are tailor- made for multiple mergers and coalitions. 
All this adds important complexity to recent evolutionary psychological 
2
accounts that emphasize our ancestral environments. We must take very seriously the profound effects of a plastic evolutionary overlay that yields a constantly moving target, an extended cognitive system whose constancy lies mainly in its continual openness to change. Even granting that the biological innovations that got this ball rolling may have consisted only in some small tweaks to an ancestral repertoire, the upshot of this subtle alteration is now a sudden, massive leap in the space of mind design. Our cognitive machinery is now intrinsically geared to self-transformation, arti- fact-based expansion, and a snowballing/bootstrapping process of compu- tational and representational growth. 
The line between biological self and technological world was, in fact, never very firm. Plasticity and multiplicity are our true constants, and new technologies merely dramatize our oldest puzzles (prosthetics and tele- presence are just walking sticks and shouting, cyberspace is just one more place to be). Human intellectual history is, in large part, the tale of this fragile and always unstable frontier. The story I tell overlaps some familiar territory, touching on our skills as language-users, toolmakers, and tool- users. But it ends by challenging much of what we think we know about who we are, what we are, and even where we are. It ought to start, perhaps, somewhere on some dusty ancestral savanna, but join me instead on a contemporary city street, abuzz with the insistent trill of a hundred cell phones. . . . 
Wired 
Brighton main street, hub of a once-sleepy English seaside town lately trans- formed into a hi-tech haven and club-culture capital. This used to be my town, but it has changed. The shops tell a new story. I walk slowly, taking stock. I count one cell phone shop, one Starbucks, another cell phone shop, a hardware store, another cell phone shop, a clothes store, another coffee shop (this one offering full internet access), yet another cell phone shop . . . 
The toll steadily mounts. Brighton, in my ten-year absence in the United States, has converted itself into a town that seems to sell nothing but coffee and cell phones. The center of town is now home to no fewer than fifty shops dedicated entirely to the selling of cell phones and their contracts. Then there are the various superstores that offer these phones alongside a variety 
of other goods. This is quite astonishing. For a relatively small town (around 250,000) this is surely a massive load. Yet business looks good and no wonder: everywhere I turn there are people with phone to ear, or punching in text messages using the fluent two-thumbed touch typing that is the badge of the younger users. Some, with fancier handsets, are using the phone to surf the web. This town is wired. 
Not only is it wired. Half the people aren’t entirely where they seem to be. I spent last Christmas in the company of a young professional whose phone was hardly ever out of his hands. He wasn’t using the phone to speak but was constantly sending or receiving small text messages from his lover. Those thumbs were flying. Here was someone living a divided life: here in the room with us, but with a significant part of him strung out in almost constant, low-bandwidth (but apparently highly satisfying) contact with his distant friend. 
The phone of the flying thumbs was a Nokia. Thanks in large part to Nokia (the firm, based in the Finnish town of the same name) the Finns emerged as early heavy-hitters in the European cell phone league. In 1999, 67 percent of the Finnish population owned and used cell phones compared to 28 percent in the United States. And these are not wimpy devices. Nokia is a pioneer of Wireless Application Protocol (WAP) technology, which sup- ports fluent interfacing between the phone and the internet. Top of the line Finnish phones have for many years opened in the middle to reveal a small keyboard and screen supporting full fax, web, and e-mail capability. But it is not the potency of the technology so much as the pregnancy of the slang that really draws me to Finland. Finnish youngsters have dubbed the cell phone 
3 “kanny,”whichmeansextensionofthehand. Themobileisthusbothsome- 
thing you use (as you use your hands to write) and something that is part of you. It is like a prosthetic limb over which you wield full and flexible control, and on which you eventually come to automatically rely in formulating and carrying out your daily goals and projects. Just as you take for granted your ability to use your vocal cords to speak to someone in the room beside you, you may take for granted your ability to use your thumbs-plus-mobile to send text to a distant lover. The phone really did seem to be part of the man, and the Finnish slang captures the mood. 
I am surprised, but I shouldn’t be. As a working cognitive scientist, the more I have learned about the brain and the mind, the more convinced I have become that the everyday notions of “minds” and “persons” pick out deeply plastic, open-ended systems—systems fully capable of including nonbiological props and aids as quite literally parts of themselves. No won- der the cell phone shops were full. These people were not just investing in new toys; they were buying mindware upgrades, electronic prostheses capable of extending and transforming their personal reach, thought, and vision. 
Upgrades, as we all know, can be mixed blessings. Every new capacity brings new limits and demands. We may, for example, start to spread our- selves too thin, reconfiguring our work and social worlds in new and not necessarily better ways. Certainly, I felt more than a tad jealous of my friend’s constant low-bandwidth info-dribble. It took some of him away from those he was physically beside. Later on, we’ll take a closer look at some of these pros and cons in our cyborg future. 
Brighton main street, then, is just one more sign of the times. As tech- nology becomes portable, pervasive, reliable, flexible, and increasingly per- sonalized, so our tools become more and more a part of who and what we are. With WAP-enhanced cell and access to our own personalized versions of the web in hand we see farther, organize better, know more. The tempo- rary disability caused by a dead battery is unnerving. It seems we just aren’t ourselves today. (The loss of my laptop, as I mentioned earlier, underlined this in a painfully personal way. I was left dazed, confused, and visibly enfeebled—the victim of the cyborg equivalent of a mild stroke.) So I, of all people, really shouldn’t have been surprised. It is our natural proclivity for tool-based extension, and profound and repeated self-transformation, that explains how we humans can be so very special while at the same time being not so very different, biologically speaking, from the other animals with whom we share both the planet and most of our genes. What makes us distinctively human is our capacity to continually restructure and re- build our own mental circuitry, courtesy of an empowering web of culture, education, technology, and artifacts. Minds like ours are complex, messy, contested, permeable, and constantly up for grabs. The neural difference that makes all this possible is probably not very large, but its effects are beyond measure. 
Don’t believe it yet? Or don’t think it matters anyway? Both are fair and proper responses. I began deliberately with a technology—the cell phone— which is at once familiar yet insufficiently fluid and user-responsive to make 
(as yet) the strongest possible kind of case. And I have rehearsed none of the interlocking evidence (some philosophical, some psychological, some neuroscientific), which actually led me to embrace such a strong thesis in the first place. 
Before the day is done, however, I hope to convince you at least of this: that the old puzzle, the mind-body problem, really involves a hidden third party. It is the mind-body-scaffolding problem. It is the problem of under- standing how human thought and reason is born out of looping interac- tions between material brains, material bodies, and complex cultural and technological environments. We create these supportive environments, but they create us too. We exist, as the thinking things we are, only thanks to a baffling dance of brains, bodies, and cultural and technological scaffold- ing. Understanding this evolutionarily novel arrangement is crucial for our science, our morals, and our self-image both as persons and as a species. 
This page intentionally left blank 

Rats in Space 
The year is 1960. The pulse of space travel beats insistently within the temples of research and power, and the journal Astronautics publishes the paper that gavetheterm“cyborg”totheworld. Thepaper,titled“CyborgsandSpace,” was based on a talk, “Drugs, Space and Cybernetics,” presented that May to the Air Force School of Aviation Medicine in San Antonio, Texas. The au- thors were Manfred Clynes and Nathan Kline, both working for the Dynamic Simulation Laboratory (of which Kline was director) at Rockland State Hos- pital, New York. What Clynes and Kline proposed was simply a nice piece of lateral thinking. Instead of trying to provide artificial, earth-like environments for the human exploration of space, why not alter the humans so as to better cope with the new and alien demands? “Space travel,” the authors wrote, “challenges mankind not only technologically, but also spiritually, in that it invites man to take an active part in his own biological evolution.”2 Why not, in short, reengineer the humans to fit the stars? 
In 1960, of course, genetic engineering was just a gleam in science fiction’s prescient eye. And these authors were not dreamers, just creative scientists engaged in matters of national (and international) importance. They were scientists, moreover, working and thinking on the crest of two major waves  and work on cybernetics4—the science of control and communication in of innovative research: work in computing and electronic data-processing, 
animals and machines. The way to go, they suggested, was to combine cy- bernetic and computational approaches so as to create man-machine hy- brids, “artifact-organism systems” in which implanted electronic devices use bodily feedback signals to automatically regulate wakefulness, metabolism, respiration, heart rate, and other physiological functions in ways suited to some alien environment. The paper discussed specific artificial interventions that might enable a human body to bypass lung-based breathing, to com- pensate for the disorientations caused by weightlessness, to alter heart rate and temperature, reduce metabolism and required food intake, and so on. 
It was Manfred Clynes who actually first suggested the term “cyborg.” Clynes was at that time chief research scientist at Rockland State Hospital and an expert on the design and development of physiological measuring equipment. He had already received a prestigious Baker Award for work on the control of heart rate through breathing and would later invent the CAT computer, which is still used in many hospitals today. When Clynes coined the term “cyborg” to describe the kind of hybrid artifact-organism system they were envisaging, Kline remarked that it sounded “like a town in Den- mark.”5 But the term was duly minted, and the languages of fact and fic- tion permanently altered. Here is the passage as it appeared in Astronautics: 
For the exogenously extended organizational complex . . . we propose the term “cyborg.” The Cyborg deliberately incorporates exogenous components extending the self-regulating control function of the organism.
Thus, amid a welter of convoluted prose, was born the cyborg. The acro- nym “cyborg” stood for Cybernetic Organism or Cybernetically Controlled Organism; it was a term of art meant to capture both a notion of human- machine merging and the rather specific nature of the merging envisaged. Cyberneticists were especially interested in “self-regulating systems.” These are systems in which the results of the system’s own activity are “fed back” so as to increase, stop, start, or reduce the activity as conditions dictate. The flush/refill mechanism of a standard toilet is a homey example, as is the thermostat on the domestic furnace. The temperature drops, a circuit is activated, and the furnace comes to life. The temperature rises, a circuit is broken, and the furnace ceases to operate. Even more prosaically, the adapt it to new environments.  toilet is flushed, the ballcock drops, which causes the connected inlet valve to open. Water then flows in until the ballcock, riding on the rising tide, reaches a preset level and thus recloses the valve. Such systems are said to be homeostatically controlled because they respond automatically to de- viations from a baseline (the norm, stasis, equilibrium) in ways that drag them back toward that original setting—the full cistern, the preset ambient temperature, and the like. 
The human autonomic nervous system, it should be clear, is just such a self-regulating homeostatic engine. It works continuously, and without conscious effort on our part, in order to keep key physiological parameters within certain target zones. As effort increases and blood oxygenation falls, we breathe harder and our hearts beat faster, pumping more oxygen into the bloodstream. As effort decreases and blood oxygen levels rise, breath- ing and heart rate damp down, reducing the intake and uptake of oxygen. 
With all this in mind, it is time to meet the first duly-accredited-and- labeled cyborg. Not a fictional monster, not even a human being fitted with a pacemaker (although they are cyborgs of this simple stripe too), but a white laboratory rat trailing an ungainly appendage—an implanted Rose osmotic pump. This rat (see fig 1.1) was introduced in the 1960 paper by Clynes and Kline as “one of the first cyborgs” and the snapshot, as Donna Haraway wonderfully commented “belongs in Man’s family album.”7 
Sadly, the rat has no name, but the osmotic pump does. It is named after its inventor, Dr. Rose, who recently died after a very creative life devoted to 
Fig. 1.1 An early (ca. 1955) classic cyborg: rat with implanted Rose osmotic pump. The pump automatically injects chemicals into the rat to form a biotechnological control loop, which can be adapted to unusual conditions (for example, survival in space). By kind permission of Manfred Clynes. 
the search for a cure for cancer. So let’s respectfully borrow that, calling the whole rat-pump system Rose. Rose incorporates a pressure pump capsule capable of delivering injections at a controlled rate. The idea was to com- bine the implanted pump with an artificial control loop, creating in Rose a new layer of homeostasis. The new layer would operate like the biological ones without the need for any conscious attention or effort and might be geared to help Rose deal with specific extraterrestrial conditions. The au- thors speculate, for example, that the automatic, computerized control loop might monitor systolic blood pressure, compare it to some locally appro- priate reference value, and administer adrenergic or vasodilatory drugs ac- cordingly. 
As cyborgs go, Rose, like the human being with the pacemaker, is prob- ably a bit of a disappointment. To be sure, each incorporates an extra arti- ficial layer of unconsciously regulated homeostatic control. But Rose remains pretty much a rat nonetheless, and one pacemaker doth not a Terminator make. Cyborgs, it seems, remain largely the stuff of science fiction, forty- some years of research and development notwithstanding. 
Implant & Mergers 
Or do they? Consider next the humble cochlear implant. Cochlear implants, which are already widely in use, electronically stimulate the auditory nerve. Such devices enable many profoundly deaf humans to hear again. How- ever, they are currently limited by requiring the presence of a healthy, un- degenerated auditory nerve. A Pasadena-based research group led by Douglas McCreery of Huntington Medical Research Institutes recently addressed this problem by building a new kind of implant (fig 1.2) that bypasses the auditory nerve and connects directly to the brain stem. Earlier versions of such devices have, in fact, been in use for a while, but performance was uninspiring. Uninspiring because these first wave brain stem implants used only an array of surface contacts—flat electrodes laid upon the surface of the brain stem near the ventral cochlear nucleus. The auditory discrimina- tion of frequencies, however, is mediated by stacked layers of neural tissue within the nucleus. To utilize frequency information (to discriminate pitch) you need to feed information differentially into the various layers of this neural structure, where the stimulation of deeper layers results in the audi- 
Fig. 1.2 A new auditory prosthesis that con- nects directly to the brain stem. The implant by- passes the cochlea, penetrating the ventral cochlear nucleus to six different depths. Illustra- tion by Christine Clark. 
tory perception of higher frequencies, and so on. The implant being pio- 
neered by McCreery thus reaches deeper than those older, surface contact 
models, terminating in six iridium microelectrodes each of which penetrates 
the brain stem to a different depth. The overall system comprises an exter- 
nal speech processor with a receiver implanted under the scalp, directly 
wired to six different depths within the ventral cochlear nucleus. A Hun- 
tington Institute cat, according to neuroscientist and science writer Simon 
8
LeVay, is already fitted with the new system and thus joins Rose in our 
Cyborg Hall of Fame.
The roll call would not be complete, however, without a certain maver- 
ick professor. Our next stop is thus the Department of Cybernetics at the University of Reading, in England. It is somewhat of a surprise to find, nowadays, a department of Cybernetics at all. They mostly died out in the early 1960s, to be replaced by departments of Computer Science, Cogni- tive Science, and Artificial Intelligence. But the real surprise is to find, within this Department of Cybernetics, a professor determined to turn himself into a good old-fashioned flesh-and-wires cyborg. The professor’s name is Kevin Warwick, and in his own words:  
Warwick began his personal transformation back in 1998, with the im- plantation of a fairly simple silicon chip, encased in a glass tube, under the skin and on top of the muscle in his left arm. This implant sent radio signals, via antennae placed strategically around the department, to a cen- tral computer that responded by opening doors as he approached, turning lights on and off, and so on. This was, of course, all pretty simple stuff and could have been much more easily achieved by the use of a simple device (a smart-badge or card) strapped to his belt or pinned to his lapel. The point of the experiment, however, was to test the capacity to send and receive signals via such an implant. It worked well, and Warwick reported that even in this simple case he quickly came to feel “like the implant was one with my body,” to feel, indeed, that his biological body was just one aspect of a larger, more powerful and harmoniously operating system. He reported that it was hard to let go of the implant when the time came for its removal. 
The real experiment took place on March 14, 2002, at 8:30 in the morn- ing at the Radcliffe Infirmary, Oxford. There, Warwick received a new and more interesting implant. This consisted of a 100-spike array (see fig. 1.3). 
Fig. 1.3 One-hundred-spike array, implanted into Professor Kevin Warwick, March 14, 2002 (shown against a small coin). By kind permission of Professor Warwick and of icube.co.uk.  
Each of the 100 tips in the array makes direct contact with nerve fibers in the wrist and is linked to wires that tunnel up Professor Warwick’s arm, emerging through a skin puncture where they are linked to a radio trans- mitter/receiver device (fig. 1.4). This allows the median nerve in the arm to be linked by radio contact to a computer. The nerve impulses running between brain and hand can thus be “wiretapped” and the signals copied to the computer. The process also runs in the other direction, allowing the computer to send signals (copies or transforms of the originals) to the im- plant, which in turn feeds them into the nerve bundles running between Warwick’s hand and brain. 
The choice of nerve bundles in the arm as interface point is doubtless a compromise. The surgical risks of direct neural interfacing are still quite high (the kind of brain stem implant described earlier, for example, is performed only on patients already requiring surgery to treat neurofibromatosis type 2). But the nerve bundles running through the arm do carry tremendous quan- tities of information to and from the brain, and they are implicated not just in reaching and grasping but also in the neurophysiology of pain, pleasure, and emotion. Warwick has embarked upon a staged sequence of experi- ments, the simplest of which is to record and identify the signals associ- ated with specific willed hand motions. These signals can then be played back into his nervous system later on. Will his hand then move again? Will he feel as if he is willing it to move? 
The experiment can be repeated with signals wiretapped during episodes of pain or pleasure. Warwick himself is fascinated by the transformative potential of the technology and wonders whether his nervous system, fed with computer-generated signals tracking some humanly undetectable quantity, such as infrared wavelengths, could learn to perceive them, yielding some sensation of seeing or feeling infrared (or ultraviolet, or x-rays, or pair, this kind of thing begins to seem distinctly feasible. Imagine, for ex- ample, being fitted with artificial sensors, tuned to detect frequencies currently beyond our reach, but sending signals deep into the developing ventral cochlear nucleus. Human neural plasticity, as we’ll later see, may well prove great enough to allow our brains to learn to make use of such new kinds of sensory signal. Warwick is certainly enthusiastic. In his own words, “few people have even had their nervous systems linked to a com- puter, so the concept of sensing the world around us using more than our natural abilities is still science fiction. I’m hoping to change that.”
Finally, in a dramatic but perhaps inevitable twist, there is a plan (if all goes well) to subsequently have a matching but surface-level device con- nected to his wife, Irena. The signals accompanying actions, pains, and pleasures could then be copied between the two implants, allowing Irena’s nervous system to be stimulated by Kevin’s and vice versa. The couple also plans to try sending these signals over the internet, perhaps with one part- ner in London while the other is in New York. 
None of this is really science fiction. Indeed, as Warwick is the first to point out, a great deal of closely related work has already been done. Scien- tists at the University of Tokyo have been able to control the movements of a live cockroach by hooking its motor neurons to a microprocessor; elec- tronically mediated control of some muscular function (lost due to damage or disease) has been demonstrated in several laboratories; a paralyzed stroke patient, fitted with a neurally implanted transmitter, has been able to will a ultrasound).
Recalling the work on deep (cochlear nucleus penetrating) auditory recursor to move across a computer screen; and rats with similar implants have learned to depress a reward-generating lever by just thinking about (controlled by a hand-held remote) involving contacts surgically inserted it into specific nerves in the spinal cord. bioelectronic signal exchanges, made possible by various kinds of implant technology, will soon open up new realms of human-computer interaction and facilitate new kinds of human-machine mergers. These technologies, for both moral and practical reasons, will probably remain, in the near future, largely in the province of restorative medicine or military applica- tions (such as the McDonnell-Douglas Advanced Tactical Aircraft Program, which envisages a fighter plane pilot whose neural functions are linked 
ing more and more a part of us every day. To see why, we must reflect some more on what really matters even about the classic (wire-and-implant-domi- nated) cyborg technologies just reviewed. These classic cases all display direct (wire-based) animal-machine interfacing. Much of the thrill, or hor- ror, depends on imagining all those wires, chips, and transmitters grafted 
There is even (fig. 1.5) a female orgasm-generating electronic implant directly into the on-board computer). Despite this, genuinely cyborg technology is all around us and is becom- 
Without much doubt, direct onto pulsing organic matter. But what we should really care about is not the mere fact of deep implantation or flesh-to-wire grafting, but the com- plex and transformative nature of the animal-machine relationships that may or may not ensue. And once we see that, we open our eyes to a whole new world of cyborg technology. 
Recall the case of the cochlear implants, and notice now the particular shape of this technological trajectory. It begins with simple cochlear im- plants connected to the auditory nerve—just one step up, really, from hear- ing aids and ear trumpets. Next, the auditory nerve is bypassed, and signals fed to contacts on the surface of the brain stem itself. Then, finally—classic cyborg heaven—microelectrodes actually penetrate the ventral cochlear nucleus itself at varying depths. Or consider Professor Warwick, whose first implant struck us as little more than a smart badge, worn inside the arm. My sense is that as the bioelectronic interface grows in complexity and moves inward, deeper into the brain and farther from the periphery of skin, bone, and sense organs, we become correlatively less and less resis- tant to the idea that we are trading in genuine cyborg technology. 
But just why do we feel that depth matters here? It is, after all, pretty obvious that the physical depth of an implant, in and of itself, is insignifi- cant. Recall my microchipped cat, Lolo. Lolo is, by all accounts, a disap- pointing cyborg. He incorporates a nonbiological component, conveniently placed within the relatively tamper-proof confines of the biological skin (and fur) bag. But he seems determinedly nontransformed by this unin- vited bar coding. He is far from anyone’s ideal of the cyborg cat. It would make no difference to this intuition, surely, were we to implant the bar code chip as deeply as we like—perhaps right in the center of his brain— humane technology and better bar code readers permitting. What we care about, then, is not depth of implanting per se. Instead, what matters to us is the nature and transformative potential of the bioelectronic coalition that results. 
Still, the idea that truly profound biotechnological mergers must be con- summated deep within the ancient skin-bag runs deep. It is the point source of the undeniable gut appeal of most classic cyborg technologies, whether real or imaginary. Think of adamantium skeletons, skull-guns, cochlear implants, retinal implants, human brains directly “jacked in” to the matrixof cyberspace—the list goes on and on. 
The deeper within the biological skin-bag the bioelectronic interface lies, the happier we are, it seems, to admit that we confront a genuine instance of cyborg technology. 
Intuitions, however, are strange and unstable things. Take the futuristic topless dancer depicted in Warren Ellis’s wonderful and extraordinary 
The dancer (fig. 1.6) displays a fully functional three-inch-high bar code tattooed across both breasts. In some strange way, this merely superficially bar-coded dancer strikes me as a more unnerving, more genuinely cyborg image, than does the bar-coded cat. And this de- spite the fact that it is the latter who in- corporates a genuine “within the skin-bag” implant. The reason for this reaction, I think, is that the image of the bar-coded topless dancer immediately conjures a powerful (and perhaps distressing) sense of a deeply transformed kind of human existence. The image foregrounds our po- tential status as trackable, commercially interesting sexual units, subject to re- peated and perhaps uninvited electronic scrutiny. We resonate with terror, excite- ment, or both to the idea of ever-deeper neural and bodily implants in part because proof, more of a rule-of-thumb) correlation between depth-of-interface and such transformative potential. The deep ventral cochlear nucleus penetrating implants can, after all, upgrade the functionality of certain profoundly deaf patients in a much more dramatic, reliable, and effective fashion than its predeces- sors. What really counts is a kind of double whammy implicit in the classic cyborg image. First, we care about the potential of technology to become integrated so deeply and fluidly with our existing biological capacities and characteristics that we feel no boundary between ourselves and the  nonbiological elements. Second, we care about the potential of such hu- man-machine symbiosis to transform (for better or for worse) our lives, projects, and capacities. A symbiotic relationship is an association of mutual benefit between dif- ferent kinds of entities, such as fungi and trees. Such relationships can be- come so close and important that we tend to think of the result as a single entity. Lichen, for example, are really symbiotic associations between an alga 
The traditional twin factors (of contained integration and profound trans- formation) come together perfectly in the classic cyborg image of the human body deeply penetrated by sensitively interfaced and capacity-enhancing elec- tronics. But in the cognitive case, it is worth considering that what really matters might be just the fluidity of the human-machine integration and the resulting transformation of our capacities, projects, and lifestyles. It is then an empirical question whether the greatest usable bandwidth and potential lies with full implant technologies or with well-designed nonpenetrative 
To see what I mean, let us return to the realms of the concrete and the everyday, scene-shifting to the flight deck of a modern aircraft. The modern flight deck, as the cognitive anthropologist Ed Hutchins has pointed and a fungus. It is often a vexed question how best to think of specific cases. The case of cognitive systems is especially challenging since the requirement— (intuitive enough for noncognitive cases)—of physical cohesion within a clear inner/outer boundary seems less compelling when information flows (rather than the flow of blood or nutrients) are the key concern. 
I believe that the most potent near-future technologies will be those that offer integration and transformation without implants or sur- gery: human-machine mergers that simply bypass, rather than penetrate, the old biological borders of skin and skull. 
With regard to the critical features just mated “fly-by-wire” computer control systems, and various high-level loops in which pilots monitor the computer while the computer monitors the pilots. The shape of these loops is still very much up for grabs. In the the computer pretty much has the final say. The pilot moves the control stick, but the onboard electronics keep the flight devia- tions inside a preset envelope. The plane is not allowed, no matter what the pilots do with the control stick, to bank more than 67 degrees or to point the nose upward at more than 30 degrees. These computer-controlled limits are meant to keep the pilots’ maneuvers from compromising the referred to, in professional training and talk, as “system managers.”)22 Piloting a modern commercial airliner, it seems clear, is a task in which human brains and bodies act as elements in a larger, fluidly integrated, biotechnological problem-solving matrix. But still, you may say, this is state- of-the-art high technology. Perhaps there is a sense in which, at least while flying the plane, the pilots participate in a (temporary) kind of cyborg exist- ence, allowing automated electronic circuits to, in the words of Clynes and Kline “provide an organizational system in which [certain] problems are taken care of automatically.”23 But most of us don’t fly commercial airliners and are not even cyborgs for a day. 
Or are we? Let’s shift the scene again, this time to your morning commute to the office. At 7:30 A.M. you are awoken not by your native biorhythms but by your preset electronic alarm clock. By 8:30 A.M. you are on the road. It is a chilly day and you feel the car begin to skid on a patch of ice. Luckily, you have traction control and the Automatic Braking System (ABS). You simply hit the brakes, and the car takes care of most of the delicate work required. In fact, as we’ll see in later chapters, the human brain is a past master at devolving responsibility in just this kind of way. You may con- sciously decide, for example, to reach for the wine glass. But all the delicate work of generating a sequence of muscle commands enabling precise and appropriate finger motions and gripping is then turned over to a dedicated, unconscious subsystem—a kind of on-board servomechanism not unlike those ABS brakes. 
Arriving at your office, you resume work on the presentation you were preparing for today’s meeting. First, you consult the fat file of papers marked “Designs for Living.” It includes your own previous drafts, and a lot of planes’ structural integrity or initiating a stall. In the Boeing 747-400, 
by contrast, the pilots still have the final say. In each case, however, under normal operating conditions, large amounts of responsibility are devolved to the computer-controlled autosystem. (The high-technology theorist and science writer Kevin Kelly nicely notes that human pilots are increasingly work by others, all of it covered in marginalia. As you reinspect (for the umpteenth time) this nonbiological information store, your onboard wetware (i.e., your brain) kicks in with a few new ideas and comments, which you now add as supermarginalia on top of all the rest. Repressing a sigh you switch on your Mac G4, once again exposing your brain to stored material and coaxing it, once more, to respond with a few fragmentary hints and suggestions. Tired already—and it is only 10 A.M.—you fetch a strong espresso and go about your task with renewed vigor. You now posi- tion your biological brain to respond (piecemeal as ever) to a summarized list of key points culled from all those files. Satisfied with your work you address the meeting, presenting the final plan of action for which (you believe, card-carrying materialist that you are) your biological brain must be responsible. But in fact, and in the most natural way imaginable, your naked biological brain was no more responsible for that final plan of action than it was for avoiding the earlier skid. In each case, the real problem- solving engine was the larger, biotechnological matrix comprising (in the case at hand) the brain, the stacked papers, the previous marginalia, the electronic files, the operations of search provided by the Mac software, and so on, and so on. What the human brain is best at is learning to be a team player in a problem-solving field populated by an incredible variety of nonbiological props, scaffoldings, instruments, and resources. In this way ours are essentially the brains of natural-born cyborgs, ever-eager to dove- tail their activity to the increasingly complex technological envelopes in which they develop, mature, and operate. 
What blinds us to our own increasingly cyborg nature is an ancient west- ern prejudice—the tendency to think of the mind as so deeply special as to be distinct from the rest of the natural order. In these more materialist times, this prejudice does not always take the form of belief in soul or spirit. It emerges instead as the belief that there is something absolutely special about the cognitive machinery that happens to be housed within the primitive bioinsulation (nature’s own duct-tape!) of skin and skull. What goes on in there is so special, we tend to think, that the only way to achieve a true human-machine merger is to consummate it with some brute- physical interfacing performed behind the bedroom doors of skin and skull. 
However, there is nothing quite that special inside. The brain is, to be sure, an especially dense, complex, and important piece of cognitive ma- 
chinery. It is in many ways special, but it is not special in the sense of provid- ing a privileged arena such that certain operations must occur inside that arena, or in directly wired contact with it, on pain of not counting as part of our mental machinery at all. We are, in short, in the grip of a seductive but quite untenable illusion: the illusion that the mechanisms of mind and self can ultimately unfold only on some privileged stage marked out by the good old-fashioned skin-bag. My goal is to dispel this illusion, and to show how a complex matrix of brain, body, and technology can actually constitute the problem-solving machine that we should properly identify as ourselves. Seen in this light, the cell phones of the Introduction were not such a capricious choice of entry-point after all. None of us, to be sure, are yet likely to think of ourselves as born-again cyborgs, even if we invest in the most potent phone on the market and integrate its sweeping functionality deep into our lives. But the cell phone is, indeed, a prime, if entry-level, cyborg technology. It is a technology that may, indeed, turn out to mark a crucial transition point between the first (pen, paper, diagrams, and digital media dominated) and the second waves (marked by more personalized, online, dynamic biotech- nological unions) of natural-born cyborgs. 
Already, plans are afoot to use our cell phones to monitor vital signs (breathing and heart rate) by monitoring the subtle bounceback of the contomatically calls for help if heart troubles are detected. The list goes on. The very designation of the mobile unit as primarily a phone is now in doubt, as more and more manufacturers see it instead as a multifunctional electronic bridge between the bearer and an invisible but potent universe of information, control, and response. At the time of writing, the Nokia 5510 combines phone, MP3 music player, FM radio, messaging machine, and game console, while Handspring’s Trio incorporates a personal digital assistant. Sony Ericsson’s T68i has a digital camera allowing the user to transmit or store color photos. Cell phones with integrated Bluetooth wire- less technology (or similar) microchips will be able to exchange informa- tion automatically with nearby Bluetooth-enabled appliances. So enabled, a quick call home will allow the home computer to turn on or off lights, 
There is a simpler sys- tem, developed by the German company Biotronic, and already under trial in England, that uses an implanted sensor in the chest to monitor heart rate, communicating data to the patient’s cell phone. The phone then auovens, and other appliances.
already as integral to the daily routines of millions as the wristwatch—that little invention that let individuals take real control of their daily schedule, and without which many now feel lost and disoriented. And all this (in most cases) without a single incision or surgical implant. Perhaps, then, it is only our metabolically based obsession with our own skin-bags that has warped the popular image of the cyborg into that of a heavily electronically penetrated human body: a body dramatically transformed by prostheses, by neural implants, enhanced perceptual systems, and the full line of Ter- minator fashion accessories. The mistake—and it is a familiar one—was to assume that the most profound mergers and intimacies must always in- volve literal penetrations of the skin-bag. 
Dovetailing 
Nonpenetrative cyborg technology is all around us and is poised on the very brink of a revolution. By nonpenetrative cyborg technology I mean all the technological tricks and electronic aids that, as hinted earlier, are al- ready transforming our lives, our projects, and our sense of our own ca- pacities. What mattered most, even where dealing with real bioelectronic implants, was the potential for fluid integration and personal transforma- tion. And while direct bioelectronic interfaces may contribute on both scores, there is another, equally compelling and less invasive, route to successful human-machine merger. It is a route upon which we as a society have al- ready embarked, and there is no turning back. Its early manifestations are already part of our daily lives, and its ultimate transformative power is as great as that of its only serious technological predecessor—the printed word. It is closely related to what Mark Weiser, working at XeroxPARC back in 1988, first dubbed “ubiquitous computing” and what Apple’s Alan Kay terms “Third Paradigm” computing. 
More generally, it falls under the category of transparent technologies. Transparent technologies are those tools that become so well fitted to, and integrated with, our own lives and  Weiser, and others insist) pretty much invisible-in-use. These tools or resources are usually no more the object of our conscious thought and reason than is the pen with which we write, the hand that holds it while writing, or the various neural subsystems projects that they are (as Don Norman, that form the grip and guide the fingers. All three items, the pen, the hand, and the unconsciously operating neural mechanisms, are pretty much on a par. And it is this parity that ultimately blurs the line between the intelli- gent system and its best tools for thought and action. Just as drawing a firm line in this sand is unhelpful and misguided when dealing with our basic biological equipment so it is unhelpful and misguided when dealing with transparent technologies. For instance, do I merely use my hands, my hip- pocampus, my ventral cochlear nucleus, or are they part of the system— the “me”—that does the using?) There is no merger so intimate as that which is barely noticed. 
Weiser’s vision, ca. 1991, of ubiquitous computing was a vision in which our home and office environments become progressively more intelligent, courtesy of multiple modestly powerful but amazingly prolific intercom- municating electronic devices. These devices, many of which have since been produced and tested at XeroxPARC and elsewhere, range from tiny tabs to medium size pads to full size boards. The tabs themselves will give you the flavor. The idea of a tab is to “animate objects previously inert.” Each book on your bookshelf, courtesy of its continuously active tab, would know where it is by communicating with sensors and transmitting devices in the building and office, what it is about, and maybe even who has re- cently been using it. Anyone needing the book can simply poll it for its current location and status (in use or not). It might even emit a small beep to help you find it on a crowded shelf! Such tiny, relatively dumb devices would communicate with larger, slightly less dumb ones, also scattered around the office and building. Even very familiar objects, such as the win- dows of a house, may gain new functionality, recording traces and trails of activity around the house. Spaces in the parking lot communicate their presence and location to the car-and-driver system via a small mirror dis- play, and the coffee-maker in your office immediately knows when and where you have parked the car, and can prepare a hot beverage ready for your arrival. 
The idea, then, is to embody and distribute the computation. Instead of focusing on making a richer and richer interface with an even more potent black box on the table, ubiquitous computing aims to make the interfaces multiple, natural, and so simple as to become rapidly invisible to the user. 
The computer is thus drawn into the real world of daily objects and inter- actions where its activities and contributions become part of the unremarked backdrop upon which the biological brain and organism learn to depend. 
This is a powerful and appealing vision. But what has it to do with the individual’s status as a human-machine hybrid? Surely, I hear you saying, a smart world cannot a cyborg make. My answer: it depends just how smart the world is, and more importantly, how responsive it is, over time, to the activities and projects distinctive of an individual person. A smart world, which takes care of many of the functions that might otherwise occupy our conscious attention, is, in fact, already functioning very much like the cy- borg of Clynes and Kline’s original vision. The more closely the smart world becomes tailored to an individual’s specific needs, habits, and preferences, the harder it will become to tell where that person stops and this tailor- made, co-evolving smart world begins. At the very limit, the smart world will function in such intimate harmony with the biological brain that draw- ing the line will serve no legal, moral, or social purpose. It would be as if someone tried to argue that the “real me” excludes all those nonconscious neural activities on which I so constantly depend relegating all this to a mere smart inner environment. The vision of the mind and self that re- mains following this exercise in cognitive amputation is thin indeed! 
In what ways, then, might an electronically infested world come to exhibit the right kinds of boundary-blurring smarts? One kind of example, drawn from the realm of current commercial practice, is the use of increasingly responsive and sophisticated software agents. An example of a software agent would be a program that monitors your online reading and buying habits, and which searches out new items that fit your interests. More sophisticated software agents might monitor online auctions, bidding and selling on your behalf, or buy and sell your stocks and shares. Pattie Maes, who works on software agents at MIT media lab, describes them as software entities . . . that are typically long-lived, continuously running . . . and that can help you keep track of a certain task . . . so it’s as if you were extending your brain or expanding your brain by having software entities out there that are almost part of you. 
Reflect on the possibilities. Imagine that you begin using the web at the age of four. Dedicated software agents track and adapt to your emerging 
interests and random explorations. They then help direct your attention to new ideas, web pages, and products. Over the next seventy-some years you and your software agents are locked in a complex dance of co-evolutionary change and learning, each influencing, and being influenced by, the other. You come to expect and trust the input from the agents much as you ex- pect and trust the input from your own unconscious brain—such as that sudden idea that it would be nice to go for a drive, or to buy a Beatles CD—ideas that seem to us to well up from nowhere but which clearly shape our lives and our sense of self. In such a case and in a very real sense, the software entities look less like part of your problem-solving environ- ment than part of you. The intelligent system that now confronts the wider world is biological-you-plus-the-software-agents. These external bundles of code are contributing as do the various nonconscious cognitive mecha- nisms active in your own brain. They are constantly at work, contributing to your emerging psychological profile. You finally count as “using” the software agents only in the same attenuated and ultimately paradoxical way, for example, that you count as “using” your posterior parietal cortex. 
The biological design innovations that make all this possible include the provision (in us) of an unusual degree of cortical plasticity and the (related) presence of an unusually extended period of development and learning (childhood). 
These dual innovations (intensively studied by the new research program called “neural constructivism”) enable the human brain, more than that of any other creature on the planet, to factor an open-ended set of bio- logically external operations and resources deep into its own basic modes of operation and functioning. It is the presence of this unusual plasticity that makes humans (but not dogs, cats, or elephants) natural-born cyborgs: be- ings primed by Mother Nature to annex wave upon wave of external ele- ments and structures as part and parcel of their own extended minds. 
This gradual interweaving of biological brains with nonbiological re- sources recapitulates, in a larger arena, the kind of sensitive co-develop- ment found within a single brain. A human brain, as we shall later see in more detail, comprises a variety of relatively distinct, but densely inter- communicating subsystems. Posterior parietal subsystems, to take an ex- ample mentioned earlier, operate unconsciously when we reach out to grasp 
The conscious agent seldom bothers herself with these details: she simply an object, adjusting hand orientation and finger placement appropriately. 
decides to reach for the object, and does so, fluently and efficiently. The conscious parts of her brain learned long ago that they could simply count on the posterior parietal structures to kick in and fine-tune the reaching as needed. In just the same way, the conscious and unconscious parts of the brain learn to factor in the operation of various nonbiological tools and resources, creating an extended problem-solving matrix whose degree of fluid integration can sometimes rival that found within the brain itself. 
Let’s return, finally, to the place we started: the cyborg control of as- pects of the autonomic nervous system. The functions of this system (the homeostatic control of heart rate, blood pressure, respiration, etc.) were the targets of Clynes and Kline in the original 1960 proposal. The cyborg, remember, was to be a human agent with some additional, machine-con- trolled, layers of automatic (homeostatic) functioning, allowing her to sur- vive in alien or inhospitable environments. Such cyborgs, in the words of Clynes and Kline, would provide “an organizational system in which such robot-like problems were taken care of automatically, leaving man free to explore, to create, to think and to feel.” Clynes and Kline were adamant that such off-loading of certain control functions to artificial devices would in no way change our nature as human beings. They would simply free the conscious mind to do other work. 
This original vision, pioneering though it was, was also somewhat too narrow. It restricted the imagined cyborg innovations to those serving vari- ous kinds of bodily maintenance. There might be some kind of domino effect on our mental lives, freeing up conscious neural resources for better things, but that would be all. My claim, by contrast, is that various kinds of deep human-machine symbiosis really do expand and alter the shape of the psychological processes that make us who we are. The old technologies of pen and paper have deeply impacted the shape and form of biological reason in mature, literate brains. The presence of such technologies, and their modern and more responsive counterparts, does not merely act as a convenient wrap around for a fixed biological engine of reason. Nor does it merely free up neural resources. It provides instead an array of resources to which biological brains, as they learn and grow, will dovetail their own activities. The moral, for now, is simply that this process of fitting, tailor- ing, and factoring in leads to the creation of extended computational and mental organizations: reasoning and thinking systems distributed across 
brain, body, and world. And it is in the operation of these extended sys- tems that much of our distinctive human intelligence inheres. Such a point is not new, and has been well made by a variety of theorists 
I believe, however, that the idea of human cognition as subsisting in a hybrid, extended architecture (one which includes aspects of the brain and of the cognitive technological envelope in which our brains develop and operate) remains vastly under-appreciated. We cannot understand what is special and distinctively powerful about human thought and reason by simply paying lip service to the importance of the web of surrounding structure. Instead, we need to understand in detail how brains like ours dovetail their problem-solving activities to these additional resources, and how the larger systems thus created operate, change, and evolve. In addition, we need to understand that the very ideas of minds and persons are not limited to the biological skin-bag, and that our sense of self, place, and potential are all malleable constructs ready to expand, change, or contract at surprisingly short notice.
Consider a little more closely the basic biological case. Our brains provide both some kind of substrate for conscious thought, and a vast panoply of thought and action guiding resources that operate quite unconsciously. You do not will the motions of each finger and joint muscle as you reach for the glass or as you return a tennis serve. You do not decide to stumble upon such-and-such a good idea for the business presentation. Instead, the idea just occurs to you, courtesy once again of all those unconsciously operat- ing processes. But it would be absurd, unhelpful, and distortive to suggest that your true nature—the real “you,” the real agent—is somehow defined only by the operation of the conscious resources, resources whose role may indeed be significantly less than we typically imagine. Rather, our na- ture as individual intelligent agents is determined by the full set of con- scious and unconscious tendencies and capacities that together support the set of projects, interests, proclivities, and activities distinctive of a par- ticular person. Just who we are, on that account, may be as much informed by the specific sociotechnological matrix in which the biological organism exists as by those various conscious and unconscious neural events that happen to occur inside the good old biological skin-bag. 
Once we take all this on board, however, it becomes obvious that even the technologically mediated incorporation of additional layers of unconscious functionality must make a difference to our sense of who and what we are; as much of a difference, at times, as do some very large and important chunks of our own biological brain. Well-fitted transparent technologies have the potential to impact what we feel capable of doing, where we feel we are located, and what kinds of problems we find ourselves capable of solving. It is, of course, also possible to imagine bioelectronic manipula- tions, which quite directly affect the contents of conscious awareness. But direct accessibility to individual conscious awareness is not essential for a human-machine merger to have a profound impact on who and what we are. Indeed, as we saw, some of the most far-reaching near-future transfor- mations may be rooted in mergers that make barely a ripple on the thin surface of our conscious awareness. 
That this should be so is really no surprise. We already saw that what we cared about, even in the case of the classic cyborgs, was some combination of seamless integration and overall transformation. But the most seamless of all integrations, and the ones with the greatest potential to transform our lives and projects, are often precisely those that operate deep beneath the level of conscious awareness. New waves of almost invisible, user-sensi- tive, semi-intelligent, knowledge-based electronics and software are per- fectly posed to merge seamlessly with individual biological brains. In so doing they will ultimately blur the boundary between the user and her knowledge-rich, responsive, unconsciously operating electronic environ- ments. More and more parts of our worlds will come to share the moral and psychological status of parts of our brains. We are already primed by nature to dovetail our minds to our worlds. Once the world starts dovetail- ing back in earnest, the last few seams must burst, and we will stand re- vealed: cyborgs without surgery, symbionts without sutures. 
Technologies to Bond With 

Heavy Metal 
Los Alamos National Laboratory, New Mexico, occupies the high ground both physically and technologically. I am here, this hot and sunny day in May 1999, to deliver a talk on the interactions between mind and technol- ogy. Getting in is not easy. There have been security scares, and my perma- nent resident alien card is deemed insufficient proof of identity. After a flurry of panic, my secretary somehow manages to fax them a copy of my UK passport. At last—and just in time for the talk—I am issued with the inevitable plastic photo ID. Soon I find myself deep in the radiation-proof concrete bunkers that currently serve as home to my hosts, the Complex Systems Modeling Team. 
Walking around this eerily silent, windowless, underground laboratory, I am struck by the stark contrast between old technology and new. The massive concrete bunkers and reinforced floors of these old buildings were designed both to resist nuclear attack and to support heavy, in-your-face technology: giant mainframes, immense monoliths of dials, lights, and le- vers. Yet today’s action, in the Complex Systems Laboratory at least, usu- ally requires little more than a few potent laptops and some fiber-optic links to massive databases. The heaviest piece of real, working machinery that I encounter is a somewhat sick old printer whose wheezing vibrations occasionally disturb the tomb-like silence. 
The talk safely delivered, my hosts suggest a meal. Los Alamos’ best restaurant turns out to be Japanese, an irony I decide not to pursue. But my political coyness proves unfounded. In fact, over the course of the meal it is decided that we will next visit one of Los Alamos’ best, if lesser-known, attractions—the Black Hole. 
The Black Hole is the shop-cum-soapbox of peace protester Edward Groshus. To visit it is to step into a retro-technological Aladdin’s Cave. Housed in a rambling, hangar-like complex on the edge of town, the Black Hole is a stunning repository of ex-National Laboratory equipment and scientific junk. The stuff was purchased (by the pound!) direct from the laboratory during its postwar sell-off period. The buyer was the same Ed Groshus, one-time national laboratory employee-turned-peacenik, anti-war campaigner, and retrotechnology entrepreneur. As I walk toward the in- stallation, a billboard on the roadside catches my attention. It reads: 
This is Groshus’s doing. A man with an agenda, to be sure, but one nicely tempered by an enduring sense of fun. This shows, too, in his rela- tionship with the goods in his store. Groshus despises the technologies of warfare, but he clearly sees the beauty as well as the absurdity of all that in- your-face technology. The Black Hole manages, incredibly, to be both shrine and protest. And it is a retro-techno addict’s dream come true. It feels like a vast and ill-organized hardware store. The hardware here is not screws, nails, and duct tape so much as bank upon bank of imponderable valve electronics, heavyweight first-generation calculating machinery, fragments of complex control panels bristling with hundreds of tiny lights and switches, filters, fans, cathode-ray tubes, testers, probes, bomb casings, wires, screens, and dials. My personal favorite was a variety of gray, heavy, metal boxes (rather like office filing cabinets) with enormous single red buttons, la- beled EMERGENCY, slap-bang in the middle, items seemingly straight out of Tom and Jerry, but in fact straight out of Uncle Sam, ca. 1960. 
What we have here is an elephant’s graveyard of Un-transparent, In- Your-Face Technology. Most of this stuff was not built to fade into the background of anyone’s life or work. It made few efforts to configure itself to better suit the user. It was, in many ways, the strict antithesis of Weiser’s vision of ubiquitous computing. Heavy, enormous, almost maximally re- sistant to easy human use, such technologies ran little risk of blurring the boundaries between machine and human, between biological user and tech- nological tool. Naturally, I bought as much of it as I could possibly carry! I came away with a large vacuum tube with shining copper coil and a heavy- weight electromagnet at the base. This was the so-called triggertron, once used to discharge a large bank of capacitors in order to implode trial atomic devices. I also succumbed to the siren call of two black boxes full of inscru- table, but wisely glowing, valve electronics. To complete the order I added a few substantial fragments of complex “Bat-Cave” control panels, featur- ing hundreds upon hundreds of tiny red and green lights and switches. To my eternal regret I could not carry the heavy box of metal with the big red emergency button in the middle. Walking all this through airport security at Albuquerque was surprisingly easy. “What’s that, sir?” “It’s an antique triggering device for an atomic weapon.” “That’s fine—come on through.” 
My own suspicious eroticization of retro-technology aside, the real point of this little reminiscence was just to begin to cement the contrast between two types of technology: “transparent technologies,” and what might con- trariwise be dubbed “opaque technologies.” A transparent technology is a technology that is so well fitted to, and integrated with, our own lives, biological capacities, and projects as to become (as Mark Weiser and Donald Norman have both stressed) almost invisible in use. An opaque technology, by contrast, is one that keeps tripping the user up, requires skills and capacities that do not come naturally to the biological organism, and thus remains the focus of attention even during routine problem-solving activ- ity. Notice that “opaque,” in this technical sense, does not mean “hard to understand” as much as “highly visible in use.” I may not understand how my hippocampus works, but it is a great example of a transparent technol- ogy nonetheless. I may know exactly how my home PC works, but it is opaque (in this special sense) nonetheless, as it keeps crashing and getting in the way of what I want to do. In the case of such opaque technologies, we distinguish sharply and continuously between the user and the tool. The user’s ongoing problem is to successfully deploy and control the tool. By contrast, once a technology is transparent, the conscious agent literally sees through the tool and directly confronts the real problem at hand. The accomplished writer, armed with pen and paper, usually pays no heed to the pen and paper tools while attempting to create an essay or a poem. They have become transparent equipment, tools whose use and function- ing have become so deeply dovetailed to the biological system that there is a very real sense in which—while they are up and running—the problem- solving system just is the composite of the biological system and these nonbiological tools. The artist’s sketch pad and the blind person’s cane can come to function as transparent equipment, as may certain well-used and well-integrated items of higher technology, a teenager’s cell phone per- haps. Sports equipment and musical instruments often fall into the same broad category. 
Often, such integration and ease of use require training and practice. We are not born in command of the skills required. Nonetheless, some tech- nologies may demand only skills that already suit our biological profiles, while others may demand skills that require extended training programs de- signed to bend the biological organism into shape. The processes by which a technology can become transparent thus include both natural fit (it requires only modest training to learn to use a hammer, for example) and the system- atic effects of training. The line between opaque and transparent technolo- gies is thus not always clear-cut; the user contributes as much as the tool. But there is a real and important sense in which some technologies are im- mediately better candidates for ultimate transparency than others. Most of the old heavyweight technology in the Black Hole remained eternally opaque, even to trained operators. While a very few devices are so well suited to the biological user that we either know at once how to use them, or quickly find out by an intuitive process of trial and error. 
Transparent Tools 
Donald Norman—cognitive scientist and contemporary guru of the age of “information appliances”—describes the Rubicon between opaque and transparent technologies in terms of a historical progression from “technology-centered” to “human-centered” products. Human-centered products wear their functionality on their sleeve and exploit the natural strengths of human brains and bodies. These are the kinds of products where, Norman insists, the user almost never needs to open the manual. Yet vanishingly few of our high technology, information-based products are like that to- day. Sadly, a thousand cases of highly opaque, run-daily-to-the-manual products spring all-too-readily to mind. Examples range from VCRs to pho- tocopiers to personal organizers and laptops. 
The trouble with technology-centered products is that, as Norman’s la- bel suggests, they answer only to the need to do things (often, many differ- ent things) that previous products didn’t do, or that they didn’t do to the same degree. What they don’t answer to is the need to enable those things to be done fluently, reliably, and with a minimum of learning and effort on the part of the user. And the reason, as Norman notes, is simple enough. At first, creating a product that can DO THE JOB is hard enough, let alone aiming for products nicely fitted to brains like ours. As time goes by, how- ever, the vendors must seek to extend their market beyond the gung ho early adopters and technophiles. They will need to sell to the average user who simply wants a cheap, reliable, and easy-to-use tool. The technologi- cal product then comes under cultural-evolutionary pressure to increase its fitness by better conforming to the physical and cognitive strengths and weaknesses of biological bodies and brains. In quasi-evolutionary terms, the product is now poised to enter into a kind of symbiotic relationship with its biological users. It requires widespread adoption by users if its technological lineage is to continue, and one good way to achieve this is to provide clear benefits at low cognitive and economic costs. 
There are, of course, many rather less appealing routes to technological (and biological) survival: products, for example, which survive simply be- cause they do the job, however opaquely, or products that depend on the incompatibility of alternatives with some popular platform or protocol. There is also a large gray area (discussed further in chapter 7) in which technolo- gies actively create the very needs (e.g., “more memory”) that they then rush to fulfill. Our immediate task, however, is to get a more concrete sense of some of the complex ways in which technologies simultaneously shape and adapt to the cognitive profiles of biological users. With that in mind, let’s look briefly at a familiar item, one that long-ago passed from the 
realm of opaque technology into that of transparent symbiotic partner— the humble wristwatch. 
We humans didn’t always keep precise, objectively measured time. Be- fore the dawn of the city, the factory, and the organized religious order, human beings used natural cycles to prompt daily activities. The sun rises and farming begins, interrupted only by a brief break when the sun is high in the sky. Darkness signals food and sleep. Today, a great many humans are not like this. We work all hours. We plan to meet friends for coffee at 11:45 A.M. We make a date for supper at 10:00 P.M. and a film at midnight, and so on. The transition from a natural-time society to our present ar- rangements for work and play was mediated by a long thread of techno- logical evolution: a thread that leads from heavy, fixed, unreliable sundials and water clocks, through the development of early oscillating-element- based timekeeping, right up to cheap, accurate, personal quartz crystal wristwatches. But the technological story, though fascinating, pales beside the human-centered story. In a mere five hundred years, the opaque, unre- liable, fixed-location tower clocks of the Middle Ages gave way to the reli- able, cheap, personal timekeepers that we now take so much for granted. Along the way our relationship to time itself was irrevocably changed and transformed. 
Once the average city worker was awakened by the call of the night watch, a living person whose task was to patrol the streets shouting the time. A little later the tolling of a bell, either owned by the town or perhaps by a specific employer, woke the townspeople. These measures instilled a degree of what David Landes nicely calls “time obedience.” But with the availability of personal timepieces, in the form of chamber clocks or (ulti- mately) wristwatches, came the possibility of something new and differ- ent—“time discipline.” The presence of easily accessible, fairly accurate, and consistently available time-telling resources enabled the individual to factor time constantly and accurately into the very heart of her endeavors and aspirations. This made possible ways of thought, and cultural prac- tices and institutions, which were otherwise precluded by our basic bio- logical nature. Landes makes the point well: 
The public clock could be used to open markets and close them, to signal the start of work and its end, to move people around, but it was a limited guide to self-imposed programs. Its dial was not always in view; its bells not always within hearing. Even when heard, hourly bells are at best intermittent reminders. They signal moments. A chamber clock or watch is some- thing very different: an ever-visible, ever-audible companion and monitor . . . a measure of time used, time spent, time wasted, time lost. 
 
Notice that what counts here is not always consciously knowing the time.  None of us, I suppose, looks constantly at his or her watch! Rather, the  crucial factor is the constant and easy availability of the time, should we desire to know it. Therefore, a prime characteristic of transparent technologies is their poise for easy use and deployment as and when required. Daily, unreflective usage bears this out. As you walk down the street, you are accosted by the familiar cry of the temporarily watchless. “Excuse me, sir, do you happen to know the time?” Asked this question on a busy street, prod and key to personal achievement and productivity. most of us will unhesitatingly reply, even before consulting our wristwatches, 6 thatyes,wesurelydo. Graspingtherequesthiddenintheformulaicquestion, many of us will also, and without further request, share our knowledge with the time-challenged supplicant. As we do so, we may find ourselves producing one of the characteristic body motions of the modern world. In the suited male or female, this takes the form of a controlled, punch-like extension of the arm, a clockwise half-rotation of the emerging wrist, and a slight lowering of the gaze. This knowledge-retrieval tropism serves, of course, a single practical function—it permits you to focus your gaze briefly upon the face, dial, or display of your watch, that humble example of cytered a word he does not know. To be concrete, let the word be “clepsydra.” At some appropriate conversational juncture, the question is raised: “Good host, do you know what the word ‘clepsydra’ means?” Perhaps you are like me. I only learned this word a few days before writing this paragraph; until then, it wasn’t part of my working vocabulary at all. But perhaps, like me, you keep a medium-size version of the Oxford English Dictionary some- where in your house. So you know you have the wherewithal to resolve the matter. But what do you say? You surely won’t say “Yes, I know what that word means” and only then proceed to consult the dictionary. Yet this is precisely what usually happens when we are asked the time! 
An easy dismissal of this discrepancy is, of course, to simply lay every- thing at the accommodating feet of convention. When we answer that we know the time, all we mean is that we have the information readily at hand. And to be sure, several cultural variants of the request exist. My wife, a native Spanish speaker, might ask me “Tienes hora?” literally, “Have you got the time?” with the emphasis on possession rather than knowledge. All this notwithstanding, I think the ease with which we accept talk of the watch-bearer as one who actually knows—rather than one who can easily find out—the time is suggestive. For the line between that which is easily and readily accessible and that which should be counted as part of the knowl- edge base of an active intelligent system is slim and unstable indeed. It is so slim and unstable, in fact, that it sometimes makes both social and scien- tific sense to think of your individual knowledge as quite simply whatever body of information and understanding is at your fingertips; whatever body of information and understanding is right there, cheaply and easily available,asandwhenneeded. Accordingtoonediagnosis,then,youaretelling the literal truth when you answer “yes” to the innocent-sounding question “Do you know the time?” For you do know the time. It is just that the “you” that knows the time is no longer the bare biological organism but the hybrid biotechnological system that now includes the wristwatch as a proper part. 
To make this just a little more palatable, consider the parallel case of biological memory. Suppose I ask you whether you know the year of the first walk on the moon. You might answer “Yes, 1969.” In answering “yes,” you do not mean to imply that this date was present to your conscious awareness all along. You do not walk around all day mentally rehearsing “1969,” “1969,” “1969.” Rather, your “yes” signifies that the information was indeed there, poised for easy access and retrieval from your biological memory. The informational poise of the wristwatch (and, as we’ll later see, of the visual scene in front of your own eyes) may sometimes be relevantly similar. Perhaps, then, you may be properly said to know the time even before you actually look at your watch—just as you can be said to know the date of the moon landing even before actually retrieving it from your biological memory. 
If this way of looking at things still strikes you as outlandish, you are in good company. Most people find such a diagnosis strange, unnecessary and (thus) unconvincing. But this reaction is unprincipled. It rests not upon any deep fact about the nature of knowledge or the preset bounds of persons but on a simple prejudice: the contemporary version, as it hap- pens, of the old and discredited idea of the mind as a special kind of spirit- stuff. The idea of “mind as spirit-stuff” is no longer scientifically respectable. Instead, mind is seen as the working of a purely physical device. In identi- fying that physical device solely with the biological brain, we again make a leap of faith, depicting the biological brain itself as the sole and essentially insulated engine of mind and reason. This conception is the old idea of special spirit-stuff in modern dress. A thoroughgoing physicalism should allow mind to determine—by its characteristic actions, capacities, and ef- fects—its own place and location in the natural order. We should not, at any rate, simply assume that it is correct to identify and locate the indi- vidual thinking system by reference to the merely metabolic frontiers of skin and skull. 
We can, in any event, take away two somewhat less contentious lessons from our discussion of modern timekeeping. The first is that transparent (nonopaque, human-centered) technology is by no means a new inven- tion. It is with us already in a wide variety of old technologies, including pen, paper, books, watches, written words, numerical notations, and the multitude of almost-invisible props and aids that scaffold and empower our daily thought and action. The second is that the passage to transpar- 
ency often involves a delicate and temporally extended process of co-evo- lution. Certainly, the technology must change in order to become increasingly easy to use, access, and purchase; but this is only half the story because at the same time, elements of culture, education, and society must change also. In the case at hand, people had to learn to value time discipline as opposed to mere time obedience, and this transition itself, Landes tells us, took over a hundred years to fully accomplish. 
Smart Worlds 
What happened with timekeeping is now happening with the flow of infor- mation itself. Mark Weiser’s vision of ubiquitous computing is finding con- crete expression in attempts to design and market what Norman calls “information appliances.”10 We have met this phrase once or twice already, and it is time to try to pin it down. Information appliances are characterized by three central features: 
1. An information appliance is geared to support a specific activity, and to do so via the storage, reception, processing, and transmission of information. 
2. Information appliances form an intercommunicating web. They can “talk” to each other. 
3. Information appliances are transparent technologies, designed to be easy to use, and to fade into the background. They are poised to be taken for 
Weiser’s vision of the home and workplace as filled with small, inter- communicating, unobtrusive intelligent devices was a vision of a world of such appliances, but Norman offers several rather more restricted, less fu- turistic, examples. He imagines the use of inexpensive, tiny cameras to beam information (the shape of the coffee table, the color of the sweatshirt) directly to family and friends while shopping. This is a natural extension of the current use of cell phone technology. (In fact, as I write the second draft of this text, I note that several cell phone companies offer “picture- messaging” with attendant ability for personal digital input.) Norman goes on to imagine houses with permanent wall-mounted weather displays, con- stantly showing the local forecast and conditions, to imagine (echoing Weiser) applications embedded in walls and furniture, and supermarkets where you simply wheel the laden shopping cart through a sensor, which scans each item and debits your bank account accordingly. He imagines devices embedded in our clothes, eyeglasses capable of comparing a cur- rently presented face to a database and retrieving name and details (again, I lately discover that such glasses now exist and are being marketed as aids for mild Alzheimer’s sufferers). And he imagines—the inevitable final step— similar devices implanted in our own bodies, monitoring the world, com- municating with other such devices, and enabling us to manage, recognize, store, and compare information quite effortlessly as we go about our daily business. 
Such is Norman’s vision: a vision of a world in which “information is more available to all of us, no matter where we are, whenever we need it.” Such technologies, to support the kind of profound integration into hugranted. man life here envisaged, need to be just about maximally nonopaque. They should contribute nothing to the complexity of the tasks they support: “the complexity of the appliance is that of the task, not the tool.”13 That does not mean, of course, that the technology itself needs to be simple. Quite the contrary. It often takes highly complex (but robust and special- purpose) technology to create a device, which can simply be taken for granted by the user in pursuing her goals and projects. What matters is that as far as our conscious awareness is concerned, the tool itself fades into the background, becoming transparent in skilled use. In this respect the technology becomes, to coin a phrase “pseudo-neural.” In childhood we learn how to use our various neural circuits to guide actions (learning to read, to walk, to talk, to write) and, later on, we simply take those ca- pacities for granted as we confront the problems of adult life (preparing the business presentation, going out for groceries, etc.). 
Personal information appliances, functioning robustly, transparently, and constantly, will slowly usher in new social, cultural, educational, and insti- tutional structures. Perhaps we will one day live in a world in which, thanks to some easy-to-access implant or wearable device, your answer to the clepsydra question—like the one about the time—is simply, “Yes, (tiny delay) it means ‘water clock,’” rather than “No, hold on while I go and look it up.” For our sense of self, of what we know and of who and what we are, is surprisingly plastic and reflects not some rigid preset biological bound- ary so much as our ongoing experience of thinking, reasoning, and acting within whatever potent web of technology and cognitive scaffolding we happen currently to inhabit. 
That web is already beginning to include a varied and mutually empow- ering matrix of human-centered technologies. Other elements in the near- future matrix include the development of lightweight, constantly running, personal computing appliances and of new techniques for rendering the informational substantial, thus blurring the boundaries between the vir- tual and the physical (“tangible computing,” more on which below). 
To see why we need this even-richer web of support, reflect that the original vision of Ubiquitous Computing, with its image of a smart world populated by semi-intelligent desks, doors, freezers, and coffeemakers, aims to put all the computational work out of sight. It seems unlikely, however, that we will do away with all need for personal data storage and knowledge access. As a result, we cannot really off-load all the computational work onto some fixed environment. Some of it should be, as Norman realized, linked more directly to a specific user. Wearable Computing, by attaching (quite literally) certain resources directly to the biological agent, offers a nonpenetrative means of catering to just this need. Instead of seeing Wear- able and Ubiquitous Computing as competing approaches, then, it is much more fruitful to consider their large potential for harmonious interaction. A Wearable Computer is an information-processing tool that is, in a deep but noninvasive sense, integral to the user. It is portable, constantly running, and may be used while the agent is in motion or otherwise en- gaged. As such, it should support hands-free use and be capable of pre- senting data unobtrusively to the user whenever it sees fit. Such devices are “designed to be useable at any time with the minimum amount of cost or distraction from the wearer’s primary task [which is] not using the com- puter [but] dealing with the environment.”15 Wearable Computing is thus, in a very broad sense, another instance of what Norman called a human- centered technology; it belongs, or aims to belong, to that species of tech- nology that fades into the background in use, providing support while allowing us to focus not on the technology but on the task at hand. But it achieves this not by embedding the computing into the world but by affixing it to the agent.
An early example is Bradley Rhodes’s “wearable remembrance agent.”This is described as “a continuously running proactive memory aid.” The device comprises a commercially manufactured heads-up display, which presents an 80 ? 25-character screen in the upper visual field via a slightly clumsy “hat-top” mounting.
EyeGlass displays and laser-based retinal displays, could also be used. The Microvision firm has piloted a device that uses safe laser technologies . More discreet technologies, including 
Ultimately, one can imag- ine direct electronic input into V1, the main visual processing gateway to the brain, somewhat along the lines of the cochlear implants described in chapter 1. In Rhodes’s device the heads-up display is combined with a special one-handed keyboard for input, known as a Twiddler keyboard (made by Handykey in New York) and a special software package, the Re- membrance Agent (RA) itself. The software is designed to run constantly, and to respond to inputs by intelligently searching through the agent’s to scan images directly onto the user’s retina. the wearer of a meeting . . . danger is one of loss of control. 
Opaque technologies were, of course, local or distal file spaces for items whose contents match the current probe. Think Google, but imagine the resource roving over your own personal file spaces, looking for the notes you yourself entered last time you encoun- tered such-and-such a person or situation. In principle, search-initiating probes could also originate from eyeglass-mounted cameras linked to face- recognition software and/or from signals continuously broadcast by local devices (here is one potential source of synergy with Ubiquitous Computing approaches). A typical pattern of use, as imagined by the designer, might go like this: 
Say the wearer of the RA system is a student headed to a history class. When she enters the classroom, note files that had previously been entered in that same classroom at the same time of day will start to appear . . . when she starts to take notes on Egyptian hieroglyphics, the text of her notes will trigger suggestions pointing to other readings and note files . . . when she later gets out of class and runs into a fellow student, the identity of the student is either entered explicitly or conveyed through an active badge system or automatic face recognition. The RA starts to bring up suggestions pointing to notes entered while around this person, including an idea for a project proposal that both students were working on. Finally, the internal clock of the wearable gets close to the time of a calendar entry reminding 
The idea is thus to combine the advantages of personal, agent-specific information, storage, and retrieval with input from a variety of fixed, envi- ronmentally distributed resources providing the wearable device with a stream of useful context-fixing information, helping it to guess where the agent is and what she is probably doing. The user is at once a mobile locus of highly personalized resources and a useful interface for local, embedded computational devices. She is also a kind of automatic electronic trail-leaver, whose movements and choices can be tracked—for good or ill—by the devices she passes near (for much more on this, see chapters 6 and 7). Wearable Computing and Ubiquitous Computing are natural allies whose full synergistic potential has yet to be explored. 
There is, however, another problem lurking in the general move toward ever-more-integrated, invisible, automatic, pseudo-neural technologies. The hard to use and control; that’s what made them opaque. But truly invis- ible, seamless, constantly running technologies resist control in a subtler, perhaps even more dangerous, manner. How then can we alter and control that of which we are barely aware? Suppose, for example, I am unhappy with the performance of my biological memory regarding names and dates. There is nothing very direct that I can do about this. I might engage in memory-training exercises; I might augment my biological memory with new resources (Palm Pilot, remembrance agent); I might try some neuro- tropic substances like Gingko biloba. But—considered as a piece of “cogni- tive kit”—my biological memory is pretty hard to get at and reconfigure. It is too far along the spectrum that leads to fully invisible computing. 
Proponents of what has become known as tangible computing take this kind of worry very seriously indeed. In making our technologies truly, per- manently invisible to the user, we may similarly limit our own capacities for creative intervention. The philosopher Heidegger, writing in 1927, dis- tinguished between a tool’s being “ready-to-hand” and its being “present- at-hand.” The hammer, while in use, is ready-to-hand. It is not an object of conscious reflection. We can, in effect, “see right through it,” concentrat- ing only on the task (nailing the picture to the wall). But, if things start to go wrong, we are still able to focus on the hammer, encountering it now as present-at-hand, that is, as an object in its own right. We may inspect it, try using it in a new way, swap it for one with a smaller head, and so on. The effective use of tools thus often involves a kind of flipping between invis- ibility-in-use and availability for thought and inspection. Paul Dourish, a leading proponent of tangible computing, thus reminds us that “the effec- tive use of tools inherently involves a continual process of engagement, separation and re-engagement.”22 Dourish, a one-time colleague of Mark Weiser, invented the term “tangible computing.” Tangible computing main- tains key elements of the invisible computation model but seeks to do so without allowing the tools and technologies to become permanently invis- ible, available solely as ready-at-hand. In common with the work discussed earlier, however, the interactions between the user and the tool are meant to be as natural and easy as possible, and to make the most of our basic skills and knowledge. 
An appealing example of tangible computing is the Marble Answering Machine designed by Durrell Bishop at London’s Royal College of Art. Standard digital answering machines can be hard to use and control; they may have multiple functions hidden in menus and useable only via com- plex sequences of button-pushings and key-holdings. By way of contrast, Bishop’s design, Dourish tells us, works like this: 
[The] answering machine has a stock of marbles. Whenever a caller leaves a message . . . it associates that message with a marble from the stock, and the marble rolls down a track to the bottom, where it sits along with the marbles representing previous messages. When the owner of the machine comes home, a glance at the track shows . . . how many messages are waiting: the number of marbles arrayed at the bottom of the track. To play a message, the owner picks up one of the marbles and drops it in a depression at the top of the answering machine; because each marble is associated with a particular message, it knows which message to play. Once the message has been played, the owner can decide what to do: either return the marble to the common stock for reuse (so deleting the message) or returning it to the  
Now imagine a good spy-movie scenario. You receive a long and vital mes- sage on the home machine, but you have just one minute until your nosy roommate gets home. You don’t want her to see or hear the message, but you cannot delete it yet. What do you do? I am willing to bet that every single reader of this text immediately thought “Take the marble and put it in your pocket, then later, in private, drop it into the machine.” The situa- tion was unusual, yet you at once knew how to proceed—but what would you have done on your usual digital device? 
The point is that the Marble Answering Machine, by giving a familiar kind of physical presence to what is really a digital abstraction (the mes- sage), allows us to use our well-developed intuitions about physical ob- jects to interact with the virtual/informational realm. As Dourish explains, the problem of interacting with the virtual is thus transformed into the more familiar one of interacting with concrete, movable objects. Instead of pushing technology to become totally invisible, the idea is to make it extra- visible: to take digital abstractions and data-flows and make them as solid and manipulable as rocks and stones. In so doing, it is hoped, we provide for the kind of easy flippability (between ready-to-hand and present-at- hand) characteristic of many of our favorite tools. 
Our biologically given neural “tools” typically lack this characteristic flippability, and it may be thought that this is sufficient reason to reject the idea of a potential symmetry between neural structures and any non- biological equipment that is ever encountered as an object in its own right. This cannot be quite right, however, since we can easily imagine encoun- tering some of our own neural mechanisms as objects too. Biofeedback techniques already allow an indirect form of this, as when we learn (by way of audible signals) to induce neural alpha rhythms at will, to lower our blood pressure, and so on. More direct forms of encounter may become commonplace as neuro-imaging techniques allow us to watch our own brains as they process information. In so doing, we surely do not render these aspects of our neural functioning less part of ourselves. The increas- ing visibility of our biological information-processing routines is, however, an interesting counterpoint to the increasing transparency of our best vision of embodied digitality. Their goal is to create a new generation of interfaces that increasingly blur the distinction between the virtual/informational and the tangible/physical. A typical project is the aptly named Sensetable, a tabletop display that uses electromagnetic sensing to determine the position of a variety of physical objects (placed on the tabletop), which the user can then move around so as to amend and alter the infornonbiological props and aids.
The Tangible Media Group at MIT Media Lab is also in pursuit of this 
One example is a chemistry teaching package in which a variety of atoms and molecules are displayed on the desktop screen. On top of the screen (i.e., on the desktop) there is also an array of small, puck- like objects. Each puck, if moved on top of a specific atom or molecule, becomes “bound” to that item (much as a marble became bound to a spe- cific message). The puck is now a physical embodiment of that informa- tion. Moving the puck around on the display, or bringing it into contact with another puck, now causes the system to simulate the effects of bring- ing that atom or molecules into contact with others. Chemical reactions can be investigated, and new molecules built and examined. In addition, modifiers can be attached to the puck via a surface slot, in order to change mation displayed. the charge on that atom or molecule, and so on. Sensetable is a descendent of a system called metaDesk, which used cameras and computer vision techniques .  
Common to all these projects, then, is the use of what the group calls Tangible User Interfaces (TUIs) in which familiar physical ob- jects, instruments, surfaces, and spaces are used to mediate our exchanges with digital information systems. Such interfaces aim, also, to erode the top display. 
When I write on a piece of paper, the input space and the output space are one and the same; the stored item appears exactly where it was input. Standard Graphical User Interfaces (GUIs) pull the spaces apart: you type on a keyboard and the information is stored somewhere else, and displayed on a screen. TUIs use displays, which are themselves “aware” of the user’s activities, and which act as input to the system (the puck-sensitive tabletop screen, for example). One promising idea is to exploit the kinds of interface we find familiar in the noncomputational world to better mediate our contact with digital and informational resources. In this vein, Neil Gershenfeld and his col- leagues produced a bow-using interface to mediate the contact between a world-class cello player (Yo-Yo Ma) and an electronic cello. The bow pro- vides a superbly sensitive, delicately nuanced, feedback-friendly means of continuously controlling the musical ebb and flow. It is an interface that has been tuned and adapted over centuries of use, and to which the hu- man cellist has devoted a lifetime of study. Why throw all that away in favor of a few buttons and a mouse? If synthesized music can sometimes sound cold and lifeless, might that have more to do with the use of such stale interfaces, rather than the potential of the digital medium itself? Re- cently, the bow-based interface was used to great effect by Yo-Yo Ma in a Tokyo performance. The digital media allowed the artist to create new sound combinations beyond the reach of any normal cello, while the familiar in- terface allowed him to explore these new possibilities with all his charac- teristic flair and insight. Ma himself, according to Gershenfeld, is engagingly enthusiastic and unsentimental, treating both his original cello (a Stradi- varius) and the new array as just two technologies: simply the means to his gap between input systems and output systems. musical ends.
Another area in which the notion of the interface is being reinvented is in work on Augmented Reality. In this work, the interface is nothing more than your own view of the world as you look around, but the view is aug- mented using some kind of heads-up or eyeglass style display system. The display might use video systems to mix computer graphics and input from cameras aimed at the scene before you, or take direct optical input and overlay it with computer graphics. The idea, in each case, is to overlay our experience of the physical world with layers of personalized digital infor- mation. This kind of work uses many of the same techniques and tech- nologies as work on Virtual Reality and Wearable Computing. But instead of trying, as with standard Virtual Reality approaches, to re-create a simulacrum of the real physical world entirely inside some computer-gen- erated realm, the goal of Augmented Reality is to add digital information to the everyday scene. Think of it as a kind of digital annotation and enhance- ment regime, with the specific annotations and enhancements being tai- lored to the needs and desires of different users passing through the (real-world) scene. 
For example, combining Global Positioning information with locally poised digital resources makes it possible to associate specific items of information with geographical locations. Such information could be picked up using special eyeglasses, or via handheld or other wearable devices. position and motion information through sensors in your clothing.
To find the library, you simply enter the name “library” in a handheld local guide-box and don a pair of special eyeglasses. As you look around, you see a giant green arrow take shape in the sky, pointing at the roof of the library! Looking down at the path, you see smaller arrows indicating the best route. Hanging in the air around your body you notice a variety of small icons offering you other local services. To use them, you just reach out and “touch” them, sending 
Thus imagine you are lost on a university campus. 
Their idea was to use such sys- tems to help workers install complex wiring harnesses in aircraft. The workers would see the desired positioning superimposed upon the actual physical structure of the plane. In a similar vein, engineers seeking to re- pair broken equipment might soon see the innards of the machine along- side specific repair instructions highlighting the elements to be removed and replaced. Surgeons seeking to repair human brains or bodies could benefit in the same way, seeing ultrasound scans or brain imaging informa- tion projected onto the appropriate areas. Researchers at the University of Central Florida have overlaid a model of a knee-joint on a woman’s leg. 
Using infrared LEDs (Liquid Electronic Displays) to inform the system about current leg position, the Augmented Reality interface allows onlookers to see just how the bones move while the woman walks and bends. The use of overlaid digital resources to enhance our ordinary daily experience of the world and to provide new means of physical-virtual interaction is likely to play a major role in the next decade. Very soon we may expect to see various kinds of personalized electronically overlaid information, from ad- vertising to information about incoming cell phone calls or even about ourselves apparently suspended in the air as we roam about. Such informa- tion might appear attached to the space around an individual, or a shop, or a designated electronic advertising area. Once again, the key innovation is to allow the physical and the informational realms to seamlessly merge and mingle, in ways that unobtrusively support daily activity and that make maximum use of our normal means of embodied, socially embedded activ- ity. It is worth repeating that such work in Augmented Reality, though it uses some of the same technologies as Virtual Reality, is really quite differ- ent at root. The aim here is not to create a richly detailed version of the daily world inside the machine, but to use the machine to add new layers of meaning and functionality to the daily world itself. Some theorists thus speak not of Virtual Reality but of Real Virtuality—a kind of deliberate blurring of the boundaries between physical and informational space. 
This kind of blurring has educational importance too. If we are indeed becoming complex biotechnological hybrids, a major challenge for the future will be to train young minds to think well about a world in which the physical and the informational/digital are densely and continuously interwoven. To that end, researchers are developing forms of so-called mixed 
Versions of such techniques can allow children to engage in “mixed reality play,” in which a coherent play space is created with characters who are able to cross over between the physical and virtual realms. In one of the reality play.
In mixed reality play, the virtual/informational is made tan- gible, the physical made virtual, and the two realms interwoven in single play-based experiences. One of the keys to such experience is the develop- ment of what the team calls “traversable interfaces between real and virtual worlds.” A traversable interface creates the impression of a seamless join between the real and the virtual, and encourages users to frequently and naturally cross over between the two realms. 
last papers written before his untimely death, my colleague and friend Mike Scaife, working as part of a multi-university interdisciplinary research col- laboration, helped design a mixed reality adventure game called “Hunting the Snark.”32 The game targets children in the 6 to 10 age group, and the idea is to locate an elusive entity (the Snark) that can live in both the physi- cal and the digital world, and whose activities seamlessly crisscross the two realms. This game also relies on traversable interfaces that give the illusion of joining the two worlds. For example, physical items bearing electronic tags can be used to trigger events in the digital world, when placed into some key location (e.g., a kind of magic well) in the physical world. Such key locations included the aforementioned well, a wardrobe, a cave, and a mouth. When placing a real object in the well, the children could see the object in their hand disappear in the real world, emerging at once in the digital one, only to be eaten, or rejected, by the digital manifestation of the Snark (figs. 2.1 and 2.2). The use of simple forms of wearable computing also allowed some of the children’s real movements to be coordinated with the movements of the digital Snark. This kind of playful technology pro- vides novel and exciting experiences, which should help young brains learn better how to make the most of a world in which the physical and the digital are ever-more-closely intertwined—worlds in which everyday objects (medicine cabinets, coffeemakers, refrigerators) have informational state, and informational phenomena have much more tangible physical presence. 
Nurtured by such experiences, and living and moving in a world popu- lated with ubiquitous computing devices, augmented reality displays, and various kinds of tangible computing, next-generation human minds will not invest very heavily in the virtual/physical divide. Instead, these minds will focus on activity and engagement, seeing both the virtual and the physi cal as interpenetrating arenas for motion, perception and action.
reality play intends to block the stale opposition between the real and the virtual, or the bodily and the informational, revealing each for what it is: just one more aspect of a larger world in which hybrid selves live, move, work, and play. 


(Facing page, top) Fig. 2.1 The Snark, a being that straddles the digital and physi- cal worlds, displays a happy expression when the children place suitable, electroni- cally tagged objects in the well. Such “mixed reality play” may help young brains learn how to think better about a world in which the digital and the physical are ever-more- closely intertwined. Image courtesy of Eric Harris and Yvonne Rogers. 
Moving On 
Time to take stock. We have now met several visions of the near future. The visions of Invisible Computing, of Tangible Computing, of Wearable Computing, and of Augmented Reality. Of these, Invisible Computing and Tangible Computing at first seem like diametrically opposed research pro- grams, but this is not really the case. The differences are real, but easily overplayed. Is the wristwatch an example of invisible or tangible technol- ogy? Norman and Weiser’s original vision does not require total invisibility so much as invisibility-in-use, and on this, both models converge. The Marble Answering Machine, to take another case, is every bit as good an exemplar of an Information Appliance as it is an instance of Tangible Com- puting. And it is the very same features that make it “tangible” (the way it exploits our ease and familiarity with everyday objects) that allow it to become invisible in daily use. 
The differences between the two visions thus show up only, if at all, at the very extremes, where some Information Appliances will indeed be de- signed to remain firmly out of sight and out of mind. But sometimes, as Norman stresses and Dourish would surely admit, this is the best way. The engine management system of a modern automobile is a case in point, and the intelligent coffeepots and carports imagined by Norman and Weiser may be others. The question we really should not ask may be, Which way is best? That is rather like asking whether our best tools should be more like hands, hammers, or the hippocampus. The question is misguided, because each of these tools is specialized for different purposes and (hence) needs to be accessed, used and/or reconfigured in very different ways. For certain purposes we want tools that we can step back from and think about. For other purposes, we want tools that function continuously and quasi- independently, requiring little or no conscious attention and that resist easy 
Mixed reprogramming (more like the homeostatic control systems that regulate heart rate, breathing, and the like discussed in chapter 1). 
The various kinds of transparent, human-centered technologies that we have so far imagined are, however, typically restricted in one specific way. All the fitting, the adaptation of the technology to the needs and capacities of the biological user, is done by the slow cultural process of design and re- design; the final dovetailing of biology and technology is achieved courtesy of individual human learning. This neglects the important—perhaps ulti- mately transformative—potential of information appliances that, in use, actively work to learn about and better fit the user. I dub such appliances “dynamic appliances.” Not all dynamic appliances are transparent and unobtrusive. Existing speech-to-text software is quite hard to set up and use, but it is dynamic in that it learns about specific users and adapts to their voices and vocabularies. The combination of dynamic appliances and transparent technologies is surely a match made in cyborg heaven. Imagine information appliances that actively learn about the user. First, you take the word-processing function out of the PC and lodge it in a special, dedi- cated writing and composing platform—an information appliance using a nice Direct Manipulation Interface. Next, you allow the machine to monitor its own use. After a while, unused functions can be temporarily disabled, and frequently used functions are given a more efficient implementation. Response times speed up. The appliance has become skilled at doing just what the user requires, no more and no less. This is what we already find in our own neural structures, and pseudo-neural technologies need to aim for the same kind of effect. There is a cost of course: the immediate functional- ity is reduced. But this is a trade-off the biological brain makes all the time. You become skilled at driving a car with a certain configuration of controls. After a while, the cognitive effort needed to drive that kind of car drops, and the capacity to rapidly and effectively avoid collisions increases. The cost of this is felt if driving demands change—say you rent a car with a very alien configuration. But the process then repeats. The biological brain is constantly striving to streamline, chunk, compile, and automate, and it does so by attending to repeated patterns of activity and use. Dynamic information appliances would, when appropriate, do just the same. The combination of brains that learn about technologies, with ubiquitous, in- creasingly transparent technologies that “learn” about individual brains, sets the scene for a cognitive symbiosis whose full potential and implica- tions none of us can yet fully appreciate. 
The technological present, then, is a shifting kaleidoscope of visions of the future. The smart world full of invisible technologies; the world of con- stantly running, easily deployed wearable computers; the world of neuro- electronic implants; the world of tangible computing and real virtuality; the world of dynamic, self-reconfiguring wearables and information appli- ances. Which is it to be? The visions jostle for space, but they are not truly competing. Quite the reverse. These are, as we will see, complementary threads in an emerging biotechnological fabric. Our cyborg future, like our cyborg present and our cyborg past, will depend on a variety of tools, tech- niques, practices, and innovations. What they will increasingly have in com- mon is that deep human-centeredness that Norman so powerfully celebrates. These will be technologies to live with, to work with, and to think through. Such technologies are apt for the most profound and enduring kinds of interweaving into our lives, identities, and projects, and into our constantly constructed sense of place, presence, and self. 
Plastic Brains, Hybrid Minds 

Your own body is a phantom, one that your brain has tempo- rarily constructed purely for convenience. 
—V. S. Ramachandran and S. Blakeslee 
The Negotiable Body 
Here are some playful—but important and illuminating—experiments you can do at home. They were designed by V. S. Ramachandran, who is profes- sor and director of the Center for Brain and Cognition at the University of 
1
California, San Diego. Follow the simple instructions and you will (with 
about 50 percent probability) feel as if your nose is two feet long, feel as if the desktop is part of you and capable of feeling pain, and feel sensation in a dummy (rubber) hand. 
The point of these experiments is to show that our sense of our own bodily limits and bodily presence is not fixed and immovable. Instead, it is an ongoing construct, open to rapid influence by tricks and (as we’ll see in chapters 4 and 5) by new technologies. 
Experiment One: The Extended Nose 
Arrange two chairs in a line, one behind the other. Seat yourself in the rearmost chair and have a friend blindfold you. Get a volunteer to sit in the chair in front of you. Now get your friend to stand beside the two occupied chairs and issue the following instructions, taken from Ramachandran and Blakeslee: 
Take my right hand and guide my index finger to [the seated volunteer’s] nose. Move my hand in a rhythmic manner so that my index finger repeat- edly strokes or taps [the volunteer’s] nose in a random sequence like a Morse code. At the same time, use your left hand to stroke my nose with the same rhythm and timing. The stroking and tapping of my nose and [the 

After less than a minute of this synchronized nose-tapping, about half the subjects report a powerful illusion. It is as if their own noses now extended about two feet in front of them. Here’s why: your brain registers the rhythmic tapping of your finger and knows that your arm is extended out in front of you. It is also receiving signals, perfectly coordinated with this tapping rou- tine, from the end of your own nose (the friend is tapping your nose in synchrony with the tapping of your finger on the volunteer’s nose). To make sense of this close and ongoing match between arm’s-length tapping and end-of-nose sensation, the brain infers that your nose must now extend far enough for the arm’s-length tapping to be causing the feelings. So your nose must be about two feet long. So that’s how it (suddenly) feels to you. But this, as Ramachandran and Blakeslee go on to comment, is really quite ex- traordinary: “Your certain knowledge . . . constructed over a lifetime [can be] negated by just a few seconds of the right kind of sensory stimulation.” 
Experiment Two: A Pain in the . . . Desktop? 
Sitting at your desk, place your left hand underneath the desktop. Get a volunteer to tap the desktop with her right hand while using the left to (in synchrony) tap your hidden hand. Once again, many subjects will feel as if the “being tapped sensation” is located on the desk surface—as if the desk- top were a real, sensitive part of their body. Now have the volunteer hit the desktop with a hammer. Your galvanic skin response jumps as if your own hand had been threatened!3 
Experiment Three: Sensation in a Dummy Hand 
A variant of the last experiment uses a plastic dummy hand. A partition is created so that you see only the dummy hand, and a volunteer again taps both your real hand (hidden behind a screen) and the dummy hand (in your direct view) in perfect synchrony. Subjects experience sensations “in the dummy hand.” 
volunteer’s] nose should be in perfect synchrony. 
PLASTIC BRAINS, HYBRID MINDS 61 
In all these cases (and you can probably now dream up many more), we discover that the body-image supported by a biological brain is quite plas- tic, and highly (and rapidly) responsive to coordinated signals from the environment. The image of the physical body with which we so readily align our pains and pleasures is highly negotiable. It is a mental construct, open to continual renewal and reconfiguration. 
One reason this makes sense, of course, is that our bodies do change during our lifetimes. Limbs grow and develop and sometimes are tragically lost. Ramachandran himself has worked extensively with so-called “phan- tom limb” patients, and it was this work that, in part, led him to devise and carry out the experiments just rehearsed. Phantom limb patients (ampu- tees who continue to feel either motion, movement, or pain in the missing limb), he found, could sometimes be helped by devising ways to fool their brains into thinking the missing limb was in various states. For example, a patient with sensations as of a clenched and painful phantom hand was helped with a box and mirror arrangement allowing an image of the real 
4 remaininghandtobecastintothespace“occupied”bythephantom. The 
patient could then relieve the pain in the phantom limb simply by clench- ing, then unclenching, the real hand. The visual feedback of unclenching occurring in a hand that seemed (courtesy of the mirror box) to be located where the phantom should be, allowed the brain to re-organize its body- image so as to eliminate the sensation of clenching. The trick was thus to create a good enough “virtual reality,” in which the phantom limb was visually available and responsive to the patient’s intentional control and manipulation. This allowed the re-negotiation of a (less painful) orienta- tion for the phantom limb. The key idea, common to both this “mirror box” work and to the experiments rehearsed earlier, is that despite the probable presence of some preset genetic components in our body-images, 
and simultaneously—largescopeforcontinualrevision. The deeper principle underlying all such revisions and re-negotiations now looks reasonably clear-cut. It is that our brains depend on perceived correlations (for instance, the correlation between observed desk-tappings and felt sen- sation) to continuously construct a model of—and hence a sense of—our bodily bounds and locations. We can call this Ramachandran’s principle. In Ramachandran and Blakeslee’s own words, To recap, human brains (and indeed those of many other animals) seem to support highly negotiable body-images. As a result, our brains can quite readily project feeling and sensation beyond the biological shell. In much the same way, the blind person’s cane or the sports star’s racket soon come to feel like genuine extensions of the user’s body and senses. Once again, this is because our continual experience of closely correlated action and feedback routines running via these nonbiological peripheries allows the brain to temporarily generate what is really a new kind of “body-image,” one that includes the nonbiological components. The transformative ef- fects of this run pretty deep. In a recent neuroscientific experiment in which a monkey repeated food-retrieving actions using a rake, experimenters re- ported that the visual RF [receptive fields] of cells in the anterior bank of the intrapari- etal sulcus became elongated along an axis of the tool.
Otherwise put, the monkey’s brain rapidly learned to quite literally treat the rake as an extension of its fingers. It is reasonable to suspect that it is at precisely this point that certain kinds of tools (manually deployed ones) become transparent in use. Here, as elsewhere, the seeds of the most inti- mate organism-artifact unions are sown by the biological brain itself. 
Neural Opportunism 
Several other features of our brains combine to make us humans especially open to processes of deep biotechnological symbiosis. One such feature is nal construct that can be profoundly altered with just a few simple tricks. tool was incorporated into that of the hand. what I’ll call “neural opportunism.” Sit back in your chair and take a look around the room. What did you see? In all likelihood, you had the experi- ence of a succession of rich visual images: images of chairs, books, tables, CDs, audio equipment, whatever. In my own case, I saw a bookshelf stacked rather untidily with too many things I ought to have read, their multicol- ored spines accusingly flaunting clear, crisp, inviting titles. Looking around, I glimpsed an open closet liberally sprinkled with gaudy Hawaiian shirts, stark against the mundane backdrop of darker, workaday clothing. But now let’s ask what turns out to be an especially tricky question. In generat- ing that sequence of visual experiences, what information did my biologi- cal brain actually bother to extract and process? The answer is—significantly less than we might have guessed. 
To understand this, first reflect that the human visual system supports only a small area of high-resolution processing, corresponding to the frac- tion of the visual field that falls into central focus. When we inspect a visual scene, our brains actively move this small high-resolution window (the fovea) around the scene, alighting first on one location, then another. The whole of my bookcase, for example, cannot possibly fit into this small foveal area while I remain seated at my desk. My overall visual field (that area plus the low-resolution peripheries) is, of course, much larger, and a sizable chunk of my bookshelf falls within this coarse-grained view. It has been known since 1967 that the brain makes very intelligent use of its small high-resolution fovea, moving it around the scene (in a sequence of rapid motions known as visual saccades) in ways delicately suited to the specific objects presented with identical pictures, but told to prepare to solve differ- ent kinds of problems (some might be told to “give the sex and ages of the people in the picture,” while others are asked to simply “describe what is going on” and still others to prepare to “recall the objects in the room”), show very different patterns of visual saccade. These saccades, it is also worth commenting, are fast—perhaps three per second—and often repeti- tive, in that they may visit and revisit the very same part of the scene. What are they for? 
One possibility, at this point, is that each saccade is being used to slowly build up a detailed internal representation of the salient aspects of the scene. The visual system would thus be selective, but would still be doing what we intuitively expect. It would be using visual input to slowly build up a detailed neural image of the scene. Subsequent research, however, suggests that the real story is even stranger than that. We can get a sense of this even before looking at the scientific experiments, by thinking about some magic tricks. 
There is an entertaining web site where you can try out the following trick. You are shown, on screen, a display of six playing cards (new ones are generated each time the trick is run). In the time-honored tradition, you are then asked to mentally select and remember one of those cards. You click on an icon and the cards disappear, to be replaced by a brief “distracter” display. Click again and a five-card (one less) array appears. As if by magic, the very card that you picked is the one that has been removed. How can it be? Could the computer have somehow monitored your eye movements? A version of this trick is displayed on pages 65 and 66 of this book. Go to page 65 and immediately pick a card from the display shown in Fig. 3.1. Concentrate on that card. Remember it. Now go to page 66. Did we remove the very card you chose? Amazing isn’t it! I must confess that on first showing (and second, and third) I was quite unable to see how the trick was turned. 
Here’s the secret. The original array will always show six cards of a simi- lar broad type: six face cards, or six assorted low-ranking cards (between about two and six, for example). When the new, five-card array appears, NONE of these cards will be in the set. But the new five-card array will be of the same type: all face cards, low cards, whatever. In this way, the trick capitalizes on the visual brain’s laziness (or efficiency, if you prefer). It seems to the subject exactly as if all that has happened is that one card (the very one he mentally selected!) has disappeared from an otherwise un- changed array. But the impression that the original array is still present is a mistake, rooted in the fact that all the brain had actually encoded was something like “lots of royal cards including my mentally selected king of hearts.” Magic tricks such as these rely on our tendency to overestimate what we actually see in a single glance, and on the manipulation of our attention so as to actively inhibit the extraction of crucial information at certain critical moments. The philosopher Daniel Dennett makes a similar him and fixate on his (Dennett’s) nose. In each outstretched arm he holds point using a different card trick. He invites someone to stand in front of a playing card. He brings his arms in steadily. The question is, at what point will the subject be able to identify the color of the card? Here too, we may be surprised. Color sensitivity, it turns out, is available only in a small and quite central part of the visual field. Yet my conscious experience, clearly, is not of a small central pool of color surrounded by a vague and out-of-focus expanse of halftones. Things look colored all the way out. Once again, it begins to look as if my conscious visual experience is overes- timating the amount and quality of information it makes available. 
Now imagine that you are the subject of another famous experiment. You are seated in front of a computer screen on which is displayed a page of text. Your eye movements are being automatically tracked and moni- tored. Your experience, as you report it, is of a solid, stable page of read- able text. The experimenter then reveals the trick. In fact, the text to the left and right of a moving “window” has been constantly filled with junk characters, not recognizable English text at all. But because that small win- dow of normal, readable text has been marching in step with your central perceptual span, you never noticed anything odd or unusual. For compari- son, this is as if my bookshelf only ever once contained (at the same mo- ment) four or five clearly titled books, and the rest of the titles were all senseless junk. Nonetheless, it would have looked to me as if I were seeing a wide array of clear English titles at all times. In the case of the screen of text, the window of “good stuff” needed to support the illusion is about eighteen characters wide, with the bulk of the characters falling to the right of the point of fixation (probably because English is read left to right). 
Similar experiments have been performed using pictures of a visual scene, such as a house, with a parked car and a garden. As before, the victim sits in front of a computer-generated display. Her eye movements are moni- tored and, while they saccade around the display, changes are clandes- tinely made: the colors of flowers and cars are altered, the structure of the house may be changed; yet these changes, likewise, go undetected. We now begin to understand why the patterns of saccade are not cumulative— why we visit and repeatedly revisit the same locations. It is because our brains just don’t bother to create rich inner models. Why should they? The world itself is still there, a complex and perfect store of all that data, nicely poised for swift retrieval as and when needed by the simple expedient of visual saccade to a selected location. The kind of knowledge that counts, it begins to seem, is not detailed knowledge of what’s out there, so much as a broad idea of what’s out there: one capable of then informing on-the-spot processes of information retrieval and use. 
Simons and Levin took this research into the real world. They set up a kind of slapstick scenario in which an experimenter would pretend to be lost on the Cornell campus, and would approach an unsuspecting passerby to ask for directions. Once the passerby started to reply, two people carrying a large door would (rudely!) walk right between the inquirer and the pass- erby. During the walk through, however, the original inquirer is deftly re- placed (under cover of the door) by a different person. Only 50 percent of the subjects (the direction-givers) noticed the change. Yet the two experi- menters were of different heights, wore different clothes, had very different voices, and so on. The conclusion that Simons and Levin draw is that our failures to detect change are not due to the artificialness of the computerations, consider some recent work by Dan Simons and Dan Levin. 
screen experiments. Instead, they arise because “we lack a precise representation of our visual world from one view to the next” and encode only a kind of rough gist of the current scene—enough to support a broad unde lying sense of what’s going on insofar as it matters to us, and enough to guide further intelligent information-retrieval, via directed saccades, as and  so-called flicker paradigm. Here, you look at a computer-generated image, which flashes on and off, with a masking screen intervening. Between each showing of the image, something changes. Even when these changes are large and significant (for example, one jet engine of an airplane, shown at center screen, repeatedly appears and then disappears), we do not easily spot them. For many of these changes, subjects need to view the rapidly alternating images for nearly a minute before they see the change. Once they have spotted the change they find it hard to believe that they did not see it at once. Normally, motion cues would alert us to the area of the visual scene where a change was occurring. But in these experiments the motion cue is being screened off by the intervening blank screen (the mask). Without that cue, the changes prove very hard to detect. You can try these when needed.
A final demonstration of these startling effects can be obtained using the experiments out at various sites on the web listed in the note.
What all this suggests is that the visual brain may have hit upon a very potent problem-solving strategy, one that we have already encountered in other areas of human thoughts and reason. It is the strategy of preferring meta-knowledge over baseline knowledge. Meta-knowledge is knowledge about how to acquire and exploit information, rather than basic knowl- edge about the world. It is not knowing so much as knowing how to find out. The distinction is real, but the effect is often identical. Having a super- rich, stable inner model of the scene could enable you to answer certain questions rapidly and fluently, but so could knowing how to rapidly re- trieve the very same information as soon as the question is posed. The latter route may at times be preferable since it reduces the load on biologi- cal memory itself. Moreover, our daily talk and practice often blurs the line between the two, as when we (quite properly) expect others to know what is right in front of their eyes. Or when—to recall an example from the previous chapter—we say that we know the time, before looking, simply because we are wearing a watch! 
The visual brain is thus opportunistic, always ready to make do and mend, to get the most from what the world already presents rather than building whole inner cognitive routines from neural cloth. Instead of at- tempting to create, maintain, and update a rich inner representation (inner image or model) of the scene, it deploys a strategy that roboticist Rodney Brooks describes as “letting the world serve as its own best model.”16 Brooks’s idea is that instead of tackling the alarmingly difficult problem of using input from a robot’s sensors to build up a highly detailed, complex inner model of its local surroundings, a good robot should use sensing frugally in order to select and monitor just a few critical aspects of a situa- tion, relying largely upon the persistent physical surroundings themselves to act as a kind of enduring, external data-store: an external “memory” available for sampling as needs dictate. 
Our brains, like those of the mobile robots, try whenever possible to let the world serve as its own best model. In the light of this, some writers have suggested that our daily experience of a rich, highly detailed visual bears quite directly on the larger themes of the present treatment.
To see what I mean, let’s leave the visual case (temporarily) and con- sider a very different example. Imagine you are a devout sports fan, and that you know thousands upon thousands of somewhat arcane facts about the performance statistics of players in U.S. women’s basketball over the last twenty years. One day, as you are seated on your favorite barstool awaiting the start of a game, conversation turns to the Sacramento Mon- archs’ Kedra Holland-Corn. You immediately recall a few useful facts: that in 2000, her three-point field goal percentage was .361, ranking her seven- teenth in the WNBA; that she scored a staggering twenty-three points in 8- of-12 shooting in a recent win over Los Angeles, and so on. While you reel off these facts and figures, you are implicitly aware that you could have done the same for any number of other players in the WNBA. You are not currently thinking about, for example, Jennifer Azzi of the Utah Starzz. But had the need arisen, her field throw percentage of .930 in the 2000 season scene unfolding before the mind’s eye must be something of an illusion. On this view, it only seems to us as if we enjoy rich visual experience, thanks to that rapid capacity to retrieve more detailed information from the world as and when required. I now suspect, however, that this is a rather more delicate call than it at first appears, and the reason is one that would have been as readily available as the data on Holland-Corn. Hence, we have no hesitation in ascribing to you a rich underlying body of basket- ball knowledge. It is not that all that knowledge is currently conscious. You are not, let us imagine, right now experiencing any thoughts about Jennifer Azzi, only about Kedra Holland-Corn, but you do experience yourself as in command of a rich and detailed database in which all that information is stored, organized, and poised for easy recovery and use. Returning to the case of vision, notice that there, too, we find ourselves in command of a rich and detailed visual database in which information about the current scene is stored, organized, and poised for use. It is just that much of the database, in the case of vision, is located outside the head and is accessed by outward-looking sensory apparatus, principally the eyes. In each case, however, it is the fact that you can indeed access all this data swiftly and easily as and when required that bears out our judgments about the rich- ness of our own knowledge and understanding. 
Word Brains 
You can probably see where this is heading and how it fits in with our emerging cyborg theme tune. It just doesn’t matter whether the data are stored somewhere inside the biological organism or stored in the external world. What matters is how information is poised for retrieval and for im- mediate use as and when required. Often, of course, information stored outside the skull is not so efficiently poised for access and use as informa- tion stored in the head. And often, the biological brain is insufficiently aware of exactly what information is stored outside to make maximum use of it; old fashioned encyclopedias suffer from all these defects and several more besides. But the more these drawbacks are overcome, the less it seems to matter (scientifically or philosophically) exactly where various processes and data stores are physically located, and whether they are neurally or technologically realized. The opportunistic biological brain doesn’t care. Nor—for many purposes—should we. 
Consider next the opportunistic infant brain in the ecologically unique environment of spoken and written words. What might the reliable pres- ence of linguistic surroundings do for brains like ours? This is a complex concerns the role of spoken language itself as a kind of triggering cognitive and much-debated issue. But the small thread that I want to pull on here technology. Words, on this account, can be seen as problem-solving artifacts developed early in human history, and as the kind of seed-technology that helped the whole process of designer-environment creation get off the ground. 
Let us bracket the difficult question of what, perhaps relatively small, biological changes and adaptations allowed the process of language-gen- eration and understanding to get going in the first place. To do this is, of the strange and highly structured world into which she is born.
Our question, then, is what occurs when opportunistic infant brains encounter the world of language? One thing that happens is that a variety of cognitive shortcuts become available, allowing brains like ours to ex- plore and understand realms that would otherwise prove intractable or simply invisible. My favorite example of this comes from work not on hu- mans but on a type of chimpanzee, Pan troglodytes. U.S.-based researchers Thompson, Oden, and Boysen trained chimps to associate a simple plastic token (such as a red triangle) with any pair of identical objects (two shoes, say) and a differently shaped plastic token with any pair of different objects . To the contemporary infant brain, public lan- guage is simply encountered during early experience. The words for “car” and “drugstore,” and indeed the practice of labeling cars and drugstores at all, are not things the normal child has to invent, any more than she has to invent parks, playgrounds, or playgroups. They are all simply aspects of course, to bracket a lot. 
The token-trained chimps were subsequently able, without the continued use of the plastic tokens, to solve a more complex, abstract problem that baffled nontoken-trained chimps. The more abstract problem (which even we sometimes find ini- tially difficult!) is to categorize pairs-of-pairs of objects in terms of higher- order sameness or difference. Thus the appropriate judgment for the pair-of-pairs “shoe/shoe and banana/shoe” is “different” because the rela- tions exhibited within each pair are different. In shoe/shoe the (lower or- der) relation is “sameness”; in banana/shoe it is “difference.” Hence the higher-order relation—the relation between the relations—is difference. By contrast, the two pairs “banana/banana and cup/cup” exhibit the higher- order relation “sameness,” since the lower-level relation (sameness) is thesame in each case. (See, I told you this wasn’t easy!)
To recap, the chimps whose learning environments included plastic tokens for sameness and difference were able to solve a version of this rather slippery problem. Of the chimps not so trained, not a single one ever learned to solve the problem. The high-level, intuitively more abstract, domain of relations-between-relations is effectively invisible to their minds. How, then, does the token-training help the lucky (?) chimps whose early designer environments included plastic tokens and token-use training? 
Here’s what the experimenters suggest, and I find compelling. Imagine that the chimps’ brains come to associate the sameness judgments with an inner image or trace of the external token itself. To be concrete, imagine the token was a red plastic triangle and that when they see two items that are the same they now activate an inner image of the red plastic triangle. Then imagine that they associate judgments of difference with another image or trace (an image of a yellow plastic square, say). Such associations reduce the tricky higher-level problems to lower-order ones defined not over the world but over the inner images of the plastic tokens. To see that “banana/ shoe” and “cup/apple” is an instance of higher-order sameness, all the brain now needs to do is recognize that two green triangles exhibit the lower-order relation sameness. The learning made possible through the initial loop into the world of stable, perceptible plastic tokens has allowed the brain to build circuits that reduce the higher-order problem to a lower- order one of a kind their brains are already capable of solving. 
Notice, finally, that all that really matters to generate this effect is the association of the lower-order concepts (sameness and difference) with stable, perceptible items. Instead of plastic tokens, repeatable and distinc- tive sounds would have done just as well: a whistle for sameness, and a hum for difference. What, then, is the spoken language we all encounter as infants if not a rich and varied repository of such stable, repeatable audi- tory items? The human capacity for advanced, abstract reason owes an enormous amount to the way these words and labels act as a new domain of simple objects on which to target our more basic cognitive abilities. And the process is, of course, repeated. Given a simple label for second-order sameness, you and I could go on to make judgments about third-order sameness: sameness of higher-order sameness among pairs-of-pairs-of-pairs, and so on. After a while we might be forced to use pen and paper to keep track, thus falling back on yet another technological resource, but the trick remains the same. The whole imposing edifice of human science itself is testimony, I believe, to the power and scope of this species of cognitive shortcut. The simple act of labeling allows the biological brain to tiptoe into cognitive waters invisible, and hence impassable, to the languageless language gives us not just labels but whole, structured, recursive systems for the encoding, objectification, and communication of thoughts and ideas. In learning such systems, the human brain is subjected to a potent and empowering dose of self-administered transformational medicine. It is not yet clear just how this all works, but the power is evident. Thus consider Joseph, a deaf eleven-year-old who was never taught sign language and had a childhood deprived of all structured linguistic experience. Here is a de- scription from psychologist Oliver Sacks’s 1989 book, Seeing Voices. 
Joseph saw, distinguished, categorized, used; he had no problems with per- ceptual categorization or generalization, but he could not, it seemed, go much beyond this, hold abstract ideas in mind, reflect, play, plan . . . he seemed, like an animal or an infant, to be stuck in the present, to be con- 
Returning to the impact of simple labeling, there is strong evidence that human mathematical abilities likewise seem to depend, in at least one cru- cial aspect, upon our experiences with the stable sound bites corresponding to individual number words. In an elegant series of investigations Stanislas Dehaene and colleagues have provided compelling evidence that precise numerical reasoning, involving numbers greater than three depends upon mind.
Labels, of course, are not the whole story. The cultural tool of public fined to literal and immediate perception. 
approximate numerical sensibility that is probably innate and that we share with infants and other animals. Such a capacity allows us to judge that there are one, two, three, or many items present, and to judge that one array is greater than another. But the capacity to know that 25 + 376 is precisely 401 depends, Dehaene et al. argue, upon the operation of distinct, culturally inculcated, and language-specific abilities. 
The evidence for this is threefold. First, there is suggestive data from brain-damaged patients. Some of these patients (with left parietal damage) lose their general sense of number, and cannot decide, for example, whether 9 is closer to 10 or to 5. Yet they can still perform rote arithmetic: they know 
There is, to be sure, a kind of that 7 ? 7 is 49, and so on. Other patients (with left frontal damage) may present the opposite profile and are unable to decide whether 2 + 2 is 3 or 4, but they know that it is closer to 5 than to 9. This already suggests a certain disassociation between capacities for exact and approximate calcu- lation, as if each facility depends on distinct neural resources. 
The second source of evidence is experiments on bilinguals. Trained in one language to do specific sums, some requiring approximation and some requiring exact reason, subjects were then tested on the same problems in their other language. For the exact problems, this switch caused increased response times. For the approximate sums, there was no such cost in re- sponse time. Again, it looks as if the approximate sums are solved and stored using a language-independent resource, whereas the exact ones de- pend on some language-specific encoding. 
Third, and to my mind most convincing, the researchers used advanced brain-imaging techniques to observe neural activity in volunteers perform- ing a variety of exact and approximate calculations. The exact calculations evoked increased activity in areas of the left frontal lobe known to be speech and language related, whereas the approximate calculations evoked activ- ity in bilateral areas of the parietal lobes, regions known to be important for visuo-spatial reasoning. The kind of mathematical reasoning unique to our species appears to depend, in part, upon neural representations of number-words. It depends, therefore, upon a learning cycle that essentially involves experience with one of the most basic and ubiquitous species of cognitive technology: the spoken words of our public language. 
Another way to see the importance of this language-specific loop is to reflect on the interesting case of short-term memory (STM) for lists of num- bers. At one time, human STM was considered to be bound by “the magic number seven.” Based on experiments involving the capacity of subjects to recall presented lists of numbers (e.g., 4, 9, 7, 1, 2, 7, 6, 9, 4) after a brief delay (on the order of twenty seconds), a short-term memory space of about seven items was calculated. Conduct these same experiments in China, however, and the number goes up. Conduct them in a certain Cantonese dialect and it can reach ten or more items! The reason is that such lists are typically recalled as lists of number words, held in a kind of phonetic loop, and Chinese number words are much briefer than English ones. The briefer the sounds, the more number words can be held in the temporary buffer. 
Certain aspects of human mathematical intelligence and recall are thus seen to depend on the individual nature of the specific number words with number words but also the production and re-perception of actual, persisting numerical inscriptions: sums and equations written out on pa- per or blackboard. For most of us, even the task of calculating 796547 ? 4179645 requires the use of some kind of external tool: pen and paper or a calculator perhaps, or both, if you are especially cautious! And it isn’t hard to see why. For most of us (idiot savants and highly skilled mathema- ticians excepted) know, thanks to some early rote learning, that 7 ? 2 is 14. We do not know that 47 ? 42 is 1,974 (Is it? How will you check for yourself?). Hence, when confronted with the larger-scale problem, most of us need to carve it up into bite-size chunks (7 ? 2, then 7 ? 4, and on) according to a specific routine, as taught in school. Moreover, this routine, when the numbers are fairly large, cries out for the use of pen and paper to store various intermediate results. Our biological short-term memory (STM) is not large enough to store and sequence all the intermediate results. Some of us, like the memory-masters at carnivals and fairgrounds, become adept at using clever mnemonic ploys to squeeze items together, thus packing more into biological STM, while others are simply in command of an un- usually large store of prelearned mathematical facts and relations. Many professional mathematicians fall into this category, but most of us still resort to pen and paper to solve the larger and more complex problems. 
One quite general way to see the contribution of tools such as pen and paper is thus in terms of a deep complementarity between what the bio- logical brain is naturally good at, and what the tool provides. Biological brains do not seem to function like logic machines or like digital comput- ers. Brains—unlike computers—are not good at storing and recalling long arbitrary sequences such as a 200-digit number. Brains—again unlike com- puters—are not good at recalling long arbitrary lists of instructions. That’s why a lot of multifunction technology, like current PCs, and a lot of old technology, like first-generation VCRs, can be so hard to use. These tech- nologies require the biological brain to perform a role for which it is inher- ently unsuited: recalling and executing long, essentially arbitrary lists of instructions. On the other hand, brains, unlike standard digital computers, are good at pattern matching and at simple associations (as when the sight of the cat’s tail activates your memory of the whole cat, or when the smell of a certain perfume activates a sudden whirl of thoughts and memo- ries). Our brains are also good at perceptual processing, at using sensory input to control bodily movements, at reasoning about location and move- ment in space, and the like. Our overall profile is indeed “Good at Frisbee, Bad at Logic.”2
Scaffolded Thinking 
For biological brains the question is how to profit from their pattern-asso- ciating strengths while minimizing their weaknesses? One excellent strat- egy is (you guessed it) to combine the biological pattern associating systems with various environmental props, aids, and scaffoldings. Pictures and spo- ken words, then written words and diagrams, and most recently the full firepower of interchangeable digital media rank high among the tools by which we press maximum problem-solving power from brains like ours. 
Seen in this light, one small story told in chapter 1 takes on a new dimen- sion. It was the story of the presentation preparation where, confronted at last with the shiny finished product, the human being (especially if she is a card-carrying physicalist always seeking the basic scientific explanation of everything) may find herself congratulating her brain on its good work. But this, we argued, is misleading. It is misleading because the structure, form, and flow of the final product often depends heavily on the complex ways the brain cooperates with, and depends on, various special features of the media and technologies with which it continually interacts. We tend to think of our biological brains as the point source of the whole final content, but if we look a little more closely what we often find is that the biological brain partici- pated in some potent and iterated loops through the cognitive technological environment. These loops can now be seen to consist, in many cases, in the use of the stable external environment as a source of complementary capaci- ties to those provided by the biological brain. We began, perhaps, by looking over some old notes, then turned to some original sources. As we read, our brain generated a few fragmentary, on-the-spot responses, which were duly stored as marks on the page or in the margins. This cycle then repeats, paus- ing to loop back to the original plans and sketches, amending them in the same fragmentary, on-the-spot fashion. The whole process of critiquing, rearranging, streamlining, and linking is deeply informed by specific prop- erties of the external media, which allow the sequence of simple, pattern- associative reactions to become steadily organized and to grow (hopefully) into something like an argument or presentation. The brain’s role is crucial and special, but it is not the whole story. In fact, the true power and beauty of the brain’s role was that it acted as a mediating factor in a wide variety of complex and iterated processes, which continually looped between brain, body, and technological environment, and it is this larger system that solved the problem. 
Consider now a superficially very different kind of case, the role of sketch- ing in certain processes of artistic creation. Van Leeuwen, Verstijnen, and Hekkert offer a careful account of the creation of certain forms of abstract art, depicting such creation as heavily dependent upon “an interactive pro- cess of imagining, sketching and evaluating [then re-sketching, re-evaluat- ing, etc.]”27 The question the authors pursue is, Why the need to sketch? Why not simply imagine the final artwork “in the mind’s eye” and then execute it directly on the canvas? The answer they develop, in great detail and using multiple real case studies, is that human thought is constrained, in mental imagery, in some very specific ways in which it is not constrained during online perception. In particular, our mental images seem to be more interpretatively fixed, and less able to reveal novel forms and components. Suggestive evidence for such constraints includes the intriguing demonstration that it is much harder to dis- cover (for the first time) the second interpretation of an ambiguous fig- ure (such as the duck/rabbit in fig 3.3) in recall and imagination than when confronted with a real drawing. Good imagers, who proved unable to discover a second interpretation in the mind’s eye, were able nonethe- less to first draw what they had seen from memory and then, by perceptually inspecting their own memory-based drawing, find the second interpretation! Given the evident constraints on our ability to find new interpretations using mental imagery alone, it is not surprising that the discovery of such multiple interpretable forms turns out to depend heavily on a kind of looping process. In this looping process the artist first sketches and then perceptually, not merely imaginatively, re- encounters visual forms, which she can then inspect, tweak, and re-sketch so as to create a final product that supports a densely multilayered set of structural interpretations. The fossil trail of this process remains visible in the sequence of sketches themselves. This description of artistic creativity is strikingly similar, it seems to me, to the story about the presentation. The sketch pad is not just a convenience for the artist, nor simply a kind of external memory or durable medium for the storage of fully formed ideas. Instead, the iterated process of externalizing and re-perceiving turns out to ferent structural interpretations. be integral to the process of artistic cognition itself. To dramatize the point, imagine that we encounter a colony of Martian artists. The brains of these artists are very much like ours, but by some freak of evolution, the Martians also possess a kind of biological scratch-pad memory, which allows them to do in their heads what we do using the sketch pad. We would have no hesitation in treating this internal resource as an aspect of the Martian mind. But why, then (aside from the prejudice that all real thinking and cognition must go on inside the ancient biological skin- bag) should we not treat the human artist, armed with her trusty sketch pad, as a unified, extended cognitive system in just the same way? We must never underestimate the extent to which our own abilities as artists, poets, math- ematicians, and the like can be informed by our use of external props and media. Such “mind-tools” (I borrow the term from Daniel Dennett, who in turn cites the work of psychologist Richard Gregory) effectively transform complex problems into ones that the biological brain is better equipped to solve. In the words of cognitive anthropologist Ed Hutchins such tools would come relatively naturally. Yet the tool itself provides means of en- coding, storing, manipulating, and transforming data that the biological brain would find hard, time consuming, or even impossible. 
In sum, one large jump or discontinuity in human cognitive evolution seems to involve the distinctive way human brains repeatedly create and exploit various species of cognitive technology so as to expand and re- shape the space of human reason. We—more than any other creature on the planet—deploy nonbiological elements (instruments, media, notations) to complement our basic biological modes of processing, creating extended cognitive systems whose computational and problem-solving profiles are quite different from those of the naked brain. 
Our discussion of human mathematical competence displays this process in a kind of microcosm. Our distinctive mathematical prowess depends on a complex web of biological, cultural, and technological contributions. First, the biological brain commands an approximate sense of simple numerosity. Second, specific cultures have coined and passed on specific number words and labels, including key innovations such as words for zero and infinity. Third, the cultural practice of enforcing simple rote-learning regimes (math- ematical tables and so forth) added another element to the matrix. Finally, mix in the novel resource of pen and paper, and PRESTO! Our culturally en- hanced biological brains can begin to tackle and solve ever-more-complex problems, eventually scaling mathematical heights that unaided biological brains (of our stripe) could never have hoped to conquer. 
In all this we discern two distinct, but deeply interanimated, ways in which biological cognition leans on cultural and environmental structures. One way involves a developmental loop, in which exposure to external symbols adds something to the brain’s own inner toolkit. The other in- volves a persisting loop, in which ongoing neural activity becomes geared to the presence of specific external tools and media. 
The deepest contribution of speech and language to human thought, however, may be something so large and fundamental that it is sometimes hard to see it at all! For it is our linguistic capacities, I have long suspected, that allow us to think and reason about our own thinking and reasoning. And it is this capacity, in turn, that may have been the crucial foot-in-the- door for the culturally transmitted process of designer-environment con- struction: the process of deliberately building better worlds to think in. 
How so? The reason is straightforward. When we freeze a thought or idea in words, we create a new object upon which to direct our critical attention. Instead of just having thoughts about the world, we can then make those very thoughts (and thought processes) the targets of more think- ing. This opens up the space of what I call “second-order cognitive dynam- ics.” By this I mean that it opens up the possibility of thinking about how to think well, and allows us explicitly to ask things like this: 
What is my reason for believing that?
Is it a good reason?
How sound is the evidence?
How could I gather better evidence?
Under what circumstances do I think best, and how can I bring them about? How can I build a better world in which to think and reason? 
The list could be continued but the pattern is clear. In all these cases we are effectively thinking about our own cognitive profile or about specific 
Donald Merlin, in his excellent exploratory text The Making of the Mod- ern Mind, usefully distinguishes two ways of using speech and language. They are the mythic, and the theoretic. Mythic uses focus on storytelling and narrative. The Greeks, however, are said to have begun the process of using the written word for a new and more transformative purpose. They began to use writing to record ongoing processes of thought and theory- building. Instead of just recording and passing on whole theories and cosmologies, text began to be used to record half-finished arguments and as a means of soliciting new evidence for and against emerging ideas. Ideas could then be refined, completed, or rejected by the work of many hands separated in space and time. What was thus created, Donald argues, was thoughts. once a resource such as language allows us to make our own thought pro- cesses into objects for further scrutiny—only when, as Daniel Dennett puts it, we command “a representation of the reason [which may be] composed, designed, edited, revised, manipulated, endorsed.”32 
Second-order cognitive dynamics, I suggest, are possible only cess of externally encoded cognitive change and discovery. 
THE EARLY ADOPTER’S DREAM TECHNOLOGY 
It was hard to believe. A fully portable, shareable resource, which would radically alter the way we think, work, and live. The early adopt- ers, indeed, would be so vastly empowered that there were great fears in the land concerning fairness, access, and equality. Subject to local protocol matches, groups of users could cheaply share information and coordinate activities across vast disconnections in space and time. Totally human-centered, delicately matched to the strengths and weak- nesses of our biological brains, able to evolve and alter to become easier to learn and deploy, the new piece of kit was, in fact, so simple that even a child could use it! Yet it would allow us to learn quicker, to grasp concepts otherwise beyond our reach. And—wonder of won- ders—it would allow us to begin actively to think about our own thoughts and problem-solving strategies. As a result, it would invite us to systematically and repeatedly build better worlds to think in. 
Many feared the new resource. They felt it was sure to encour- age great laziness and to stop people thinking for themselves. If you could just ask someone for the answer, who would bother to learn anything? In the presence of such potent resources, wouldn’t our “real” memories simply wither away? Where would it all lead? Might we not turn into a race of lazy, desensitized “post-humans”—hybrids who had traded flesh and spirit for artifice, abstraction, and power? 
You be the judge. For the technology was (of course) language, and indeed, it changed us beyond recognition. It brought into being the kinds of explicit thought and reflection upon which this whole scenario depends. That’s why the scenes just imagined could never have occurred. Public language was the spark that lit the hybrid fire. 
The process of which Donald speaks is the public, collective version of the kind of scaffolded thinking and reasoning described earlier. Just as I might use pen and paper to freeze my own half-baked thoughts, turning them into stable objects for further thought and reflection, so we (as a society) learned to use the written word to power a process of collective thinking and critical reason. The tools of text (and to some extent speech) thus allow us, at mul- tiple scales, to create new stable objects for critical activity. 
With speech, text, and the tradition of using them as critical tools under our belts, humankind entered the first phase of its cyborg existence. What we had succeeded in doing was to discover and harness a new kind of cognitive resource: a kind of magic trick by which to go beyond the bounds of our animal natures. One image that I find useful in thinking about this is the image of the Mangrove Swamp. 
Picture yourself in the humid swamplands of the area known as Ten Thousand Islands—a maze of black mangroves extending from Key West to the Everglades. You are stunned by the distribution and density of these unusual trees, some of which reach heights of more than eighty feet. Yet often, these large trees stand neatly, one per island, on their own small beds of land. How did this neat arrangement arise? The answer is unexpected, for the trees did not seed upon the islands. Instead, the islands that sends complex vertical roots through the water, searching for shallow mud flats. The first result looks like a tree on stilts in the water, a bit like those famous swamp houses seen in many a Hollywood movie. But quite soon, the raised roots collect dirt and debris carried through the water, and a small island begins to form. Sometimes several such islands merge creat- ing a new shoreline. In these swamplands, our standard expectations (that trees need land to grow on) are upset. Most of the visible land is built by the trees. The tree comes first, the island second. 
In much the same way, I suggest, we tend to think of words and lan- guage as simply built upon the preexisting islands of our intelligence and thought. But sometimes, perhaps, the cycle of influence runs the other way. Our words and inscriptions are the floating roots that actively capture the cognitive debris from which we build new thoughts and ideas. Instead of seeing our words and texts as simply the outward manifestations of our biological reason, we may find whole edifices of thought and reason accreting only courtesy of the stable structures provided by words and texts. We saw something of this process in the chimps’ use of the concrete labels for sameness and difference. We see something of this in the manager’s con- struction of a presentation, the artist’s use of a sketch pad, and the math- ematician’s use of number words and external encodings. Over time we become sensitized to the relation between good cognitive products and the processes that gave rise to them. We then begin to actively structure our worlds (from our schools, to our offices, to our peer review systems) in ways that help promote better thinking. Soon, we inhabit a world not sim- ply adapted to our bodily needs (with heating, clothes, and cooking) but to our cognitive strengths and weaknesses. All of art, science, education, and culture, I shamelessly speculate, is testimony to this runaway process. Human cognition is now a moving target. The biological organism is just one part of the chameleon circuitry of thought and reason, much of which now runs and flows outside the head and through our social, technologi- cal, and cultural scaffoldings. 
Laying all this at the foot of the door of language may seem to many to go too far. We are already pretty special, after all, in being able to use and understand human language at all. Maybe it isn’t the language that makes us smart so much as the smartness that lets us learn and use language. As so often, the truth surely lies in between. Something, clearly, allows us to that defeat a fixed-architecture learner. There is similar work using netlearn the kind of open-ended structured language that sets us apart from most other animals, but it is only because we command such a distinctive resource that we become able to treat our own thoughts and ideas as ob- jects. It is this process—of using words to turn thoughts and ideas into new stable objects for further thinking and reasoning—that starts the real cognitive snowball rolling. 
Building Better Brains 
To all this, we now add a final neurobiological ingredient. There is growing evidence that the human brain, more than that of any other animal on the planet, benefits from what has become known as constructive learning. A constructive learning system, broadly speaking, is one whose own basic com- putational and representational resources alter and expand (or contract) as the system learns. To get the idea, consider the contrast between a system with a fixed short-term memory (STM), and a system in which the capacity of the STM gradually increases over some developmental period. Or a system with fixed speed and processing power, versus one in which speed and pro- cessing power can be increased. Or a system with a fixed stock of representa- tional resources (like a fixed dictionary of words), versus one capable of adding brand new items to its dictionary/vocabulary as required. 
Constructive learning systems use early learning to build new basic struc- tures upon which to base later learning. There is a powerful body of com- putational work using artificial neural networks, which has begun to show in concrete detail how such increases in problem-solving capacity can be systematically achieved. There is work that shows, for example, that an artificial neural network whose STM grows as it learns can solve problems works, which add basic processing units and connections during learning.
build an internal representational and computational environment that is itself a partial response to the early training environment. The structure of the encountered problem domain thus determines, to some extent, the architecture (number of units, layers, connections) of the network. 
More precisely, their claim is that similar mechanisms of neural growth allow the human cortex to function as an “organ of plasticity,” which is shaped and sculpted by the problems, resources, and opportuni- 
The evolutionary emergence of the mammalian neocortex is generally ac- cepted as the key neural innovation underlying advanced reason. Cortical evolution, if the neural constructivists are correct, is not simply a story about the addition of new, special-purpose brain structures. Rather, it is a story about the addition of a plastic resource geared to allowing the encountered environment to build dedicated, delicately fitted neural substructures “on- the-hoof.” The human neocortex and prefrontal cortex, along with the ex- tended developmental period of human childhood, allows the contemporary environment an opportunity to partially redesign aspects of our basic neural hardware itself. The designer environments discussed in the previous chap- ters are thus matched, step-by-step, by dedicated designer brains, with each side of the co-adaptive equation growing, changing, and evolving to better fit—and maximally exploit—the other. It is in this way that the human learner becomes “dovetailed” to the set of reliable external problem-solving resources that she encounters during early learning. 
The neural constructivist vision thus depicts neural and especially cortical growth as experience-dependent and involving the actual construction of new neural circuitry (synapses, axons, dendrites) rather than just the fine-tuning of circuitry whose basic shape and function is already deter- mined. The learning device itself changes as a result of organism-environ- mental interactions; learning does not just alter the knowledge base for a fixed computational engine, it alters the internal computational architectally deaf children, whose brains are thus never exposed to the complex and distinctly structured inputs that the auditory world provides, fail to develop the complex web of inner connectivity that supports normal hear- ing. If such stimulation is artificially provided, using the kind of cochlear implant described in chapter 1, recovery is rapid. The neural bases of this recovery are increasingly well understood and involve complex changes in ties encountered during postnatal and lifetime learning.
the environments in which our brains grow and develop may actually help structure the brain in quite deep and profound ways. As a concrete example, consider the development of hearing. Congeni- 
This means that the connectivity and response characteristics of auditory cortex. Visual cor- tex, likewise, requires extensive, experience-dependent rewiring to support seeing. Newborn human infants have very bad vision; it is highly restricted in scope, and the resolution is forty times worse than adult vision. Depth ap- preciation is pretty well nonexistent. It takes about a year of “cortical train- ing” for the visual system to become normal, a process that can be blocked by cataracts or other impairments, which deprive the visual cortex of the experience it needs. Remove the cataracts and replace the affected lens with a clear artificial one, and improvement is again dramatically fast. According to one researcher, this kind of result “demonstrates the amazing plasticity of the young brain and underscores the importance of complex, balanced, early sensory input for guiding subsequent brain development.”39 
So great, in fact, is the plasticity of immature cortex (and especially that of prefrontal cortex, according to Quartz and Sejnowski) that O’Leary dubs it “protocortex.” The whole sensory, linguistic, and technological environment in which the human brain grows and develops is thus poised to function as one of the anchor points around which such flexible neural resources adapt and fit. Such neural plasticity is, of course, not restricted to the human spe- cies; in fact, some of the early work on cortical transplants was performed on rats. But our brains do appear to be far and away the most plastic of them all. Combined with this plasticity, however, we benefit from a unique kind of developmental space: the unusually protracted human childhood. 
In a recent evolutionary account, Griffiths and Stotz argue that the long human childhood provides a unique window of opportunity in which “cul- tural scaffolding [can] change the dynamics of the cognitive system in a way that opens up new cognitive possibilities.” These authors argue against what they describe as the “dualist account of human biology and human culture” according to which biological evolution must first create the “ana- tomically modern human” before being followed by the long and ongoing process of cultural evolution. Such a picture, they suggest, invites us to believe in something like a basic biological human nature, gradually co- opted and obscured by the trappings and effects of culture and society. This vision (which is perhaps not so far removed from that found in some of the more excessive versions of evolutionary psychology) is akin, they argue, to looking for the true nature of the ant by “removing the distorting influence of the nest.”
A more realistic vision depicts us humans as, by nature, products of a complex and heterogeneous developmental matrix in which culture, tech- nology, and biology are pretty well inextricably intermingled. It is a mistake to posit a biologically fixed “human nature” with a simple wrap-around of tools and culture; the tools and culture are indeed as much determiners of our nature as products of it. Ours are (by nature) unusually plastic and op- portunistic brains whose biological proper functioning has always involved the recruitment and exploitation of nonbiological props and scaffolds. From this neurologically and ecologically unique whirlpool, we humans emerge. We are beings factory-tweaked and primed in order to be ready to participate in hybrid cognitive and computational regimes, able to think and learn in ways that take us, bit-by-bit, far beyond the scope and limits of our basic biological endowments. To be sure, there is a basic profile of biological strengths and weaknesses, one that, as Hutchins and Norman both suggested, must act as a kind of reference point for our technologies. We should not underestimate the capacity of human brains in general— young human brains in particular—to simultaneously alter and grow so they can better exploit the problem-solving opportunities our technologies provide.
We see this in the physical domain every day. A recent Warwick University study showed that young people’s thumbs have overtaken fingers as the most muscled and dextrous digits among the under-twenty-fives, sim- ply as a result of their extensive use of handheld electronic game control- lers and text messaging on cell phones. New generations of phones will be designed around this greater agility, leading to even further changes in manual dexterity and the like, in a golden loop. The same kind of user- technology co-adaptation can occur at the deepest levels of neural process- ing. Such developmentally open brains are not just opportunistic, but explosively opportunistic. They are ready to change themselves to make the most of the structures, media, and opportunities encountered during learning. 
Such explosive opportunism has implications for social policy and edu- cational practice. The goal of early education (and perhaps of all educa- tion) should not be seen as simply that of training brains whose basic potential is already determined. Rather, the goal is to provide rich environ- ments in which to grow better brains. The more seriously we take the notion of the brain-environment engagement as crucial, the less sense it makes to wonder about the relative size of each of the two contributions. What really matters is the complex reciprocal dance in which the brain tailors its activity to a technological and sociocultural environment, which—in concert with other brains—it simultaneously alters and amends. 
Our brief foray inward is now at an end. We have glimpsed just a few of the biological innovations that help make us so culturally and technologi- cally open. One is our capacity to re-create our own body image on the hoof. This capacity is especially important in allowing us to imaginatively relocate ourselves courtesy of new techniques such as telepresence and telerobotics. Our brain is highly opportunistic, ready and willing to allow reliable external structures to do duty both as memory store and as process- ing arena. The worlds of speech and text here play a special role. Sometimes we internalize strategies that originally involved the actual manipulation of external symbols and objects. At other times we learn strategies that will require the continued presence of various external scaffoldings and sup- port (pens, paper, and so forth). While all this goes on, if the neural constructivists are correct, we remain open to quite profound kinds of neural (cortical) growth and rewiring. In all these ways we are transformed by the almost unimaginable effects of our own primary transition technologies. The biggest transformation of all, however, was the one that occurred when our thoughts and ideas became objects of our own critical attention. By making our own thoughts into stable objects for our own and others’ un- hurried scrutiny, our skills with language opened the floodgates of self- reflective reason. We began to think about our own thoughts and about how to build better tools for thinking. Revamped and enhanced, all bets were off. Human cognition was poised to go indefinitely beyond its animal origins. 
Stretch to Fit 
“Distance,” a philosopher-friend once commented, “is what there is no action at.”1 There is considerable wisdom in this. Next time you are on a crowded train or in a subway station, look at all the people around you talking on their cell phones. Where are they? Well, clearly, they are with you in the station or on the train, but often, they are not much engaged with these local surroundings. They are, temporarily at least, jacked into a web of personal and business communications, which deliberately disre- spects current physical location. Draw the lines of proximity and distance according to the criterion of effective action, and a virtual neighborhood emerges; one in which the speakers are more proximal to their colleagues or loved ones than to the strangers on the platform. 
There is nothing especially new or surprising in this. Our sense of our own location, like our sense of our own bodily limits as discussed in chap- ter 3, is the fruit of an ongoing project. It too is a construct: this time, one formed by our implicit awareness of our current set of potentials for action, social engagement, and intervention. Imagine yourself confined to a hospi- tal bed. You cannot walk, but you can move your arms and hands. Your world seems to shrink to the radius of action and control. Add a buzzer to summon a nurse and you feel a tad more liberated. Add a phone link to your stockbroker and/or your family, and the claustrophobia recedes even more. But action, of the kind that seems most important for our sense of our own location, is a complex thing. The mere provision of telecommuni- cation links, though it goes some way toward freeing us from the bonds of physical space and proximity, is not really enough to alter our bedrock sense of where we are. In this chapter, I want to explore the potential of richer technologies to impact, for better and worse, this fundamental sense of location. In the next chapter, I explore the potential of these technolo- gies to alter our fundamental sense of self. 
In my opinion, the single best piece of philosophical fiction ever written must be the short story “Where Am I,” which Daniel Dennett (professor of philosophy at Tufts University) published in 1981. Dennett tells the story of an American citizen who agrees to participate in a secret experiment. The citizen is Dennett himself, and in the experiment Dennett’s brain is removed, kept alive in a tank of nutrients, and equipped with a multitude of radio links by means of which to execute all its normal bodily control functions. Dennett’s body is equipped with receivers and transmitters, so that it can use its built-in sensors (eyes, ears, etc.) to relay information back to Dennett’s brain. As the technicians in the story put it: 
Think of it . . . as a mere stretching of the nerves. If your brain were just moved over an inch in your skull, that would not alter or impair your mind. We’re simply going to make the nerves indefinitely elastic by splicing radio 
His brain safely excised and relocated, and the radio links established, Dennett awakes. He sees the nurse, who leads him to the room where his brain is being kept. The experience that ensues is puzzling. There is Dennett, standing up, staring at his own brain. Or is he? Perhaps, he muses, the proper thought is that “here I am, suspended in a bubbly fluid, being stared at by my own eyes.” But try as he may, Dennett cannot seem to place himself in the tank. It continues to seem as if he is outside the tank, looking in. Dennett’s point of view, as he moves, seems securely fixed outside the tank. The feeling shifts, however, when Dennett’s body is subsequently trapped by a rockslide, entombed far beneath the earth’s surface. At first, Dennett feels trapped beneath the surface, but then the radio links them- selves begin to give way, rendering him blind, deaf, and incapable of feel- ing. The shift in point of view was immediate. 
Whereas an instant before I had been buried alive in Oklahoma, now I was disembodied in Houston. . . . as the last radio signal between Tulsa and Houston died away, had I not changed location from Tulsa to Houston at the speed of light?3 
Where would you place our hero? Is Dennett really in the tank of nutrients, really trapped beneath the soil, or really no place at all (or both places at once)? Such questions need have no clear-cut answers. But what does seem clear is that our sense of location is not simply a function of our beliefs about the location of our body. Dennett, after all, continues to believe that his body is buried in Oklahoma, but his point of view is more labile. It is, I want to say, a construct grounded in the brain’s experiences of control, communication, and feedback. And as such, it is open to rapid and radical reconfiguration by new technologies. 
Dennett’s story was pure fiction, but science is never far behind. Consider the work of Miguel Nicolelis of Duke University in North Carolina. Nicolelis and his team studied the way signals from the cerebral cortex control the motions of a monkey’s limbs. An owl monkey had ninety-six wires implanted into its frontal cortex, feeding signals into a computer. As the monkey’s brain sent signals to move the monkey’s limbs, this “neural wiretap” was used to gather data about the correlations between patterns of neural signal and specific motions. The correlations were not simple and turned out to involve ensembles of neurons in multiple cortical areas, but the patterns, though buried, were there in the signals. Once these map- pings were known, the computer could predict the intended movements directly from the neural activity. The computer could then use the neural signal to specify movements, which could be carried out by a robot arm. In experiments conducted with the MIT Touch Lab, signals from the owl monkey’s brain in North Carolina were used to directly control an electro- mechanical prosthesis in an MIT laboratory six hundred miles distant. 
The results were impressive. The neural commands were rapidly and accurately translated into actual motions of the remote robot arm, which mimicked the full range of motions of its biological template. Dr. Mandayam sight to see the robot in our lab move, knowing it was being driven by signals from a monkey brain at Duke. It was as if the monkey had a 600 mile-long virtual arm.”6 The robot thus controlled was a haptic interface, part of a multisensory virtual reality system used to touch, feel, and ma- nipulate computer-generated objects. Pursuing the theme, Dr. Srinivasan speculates that “if we extended the capabilities of the arm by engineering different types of feedback to the monkey—such as visual images, auditory stimuli and forces associated with feeling textures and manipulating ob- jects—such closed-loop control might result in the remote arm’s being incorporated into the body’s representation in the brain.” In short, there may be all kinds of ways in which we can one day augment our bodies in virtual space, extending and altering our own body image and representa- tion into the bargain. 
There is, of course, a whole swathe of technologies supporting so-called telepresence, literally, remote presence. The good old-fashioned telephone affords a thin, narrow bandwidth kind of aural telepresence. Typically, the term implies state-of-the-art equipment supporting a more realistic, multidimensional effect. Taken to the limit, the effect would be very much as in 
Dennett’s thought experiment, except that instead of going to all the trouble of removing the brain and setting it up to control and communicate with the distant body, the technologies of telepresence leave brain and body joined and intact but wrap the body in a kind of additional sensory cocoon. This cocoon is fed with information gathered—perhaps using a mobile robot body—at some distant site. That information is used to power (via the cocoon) a local sensory barrage corresponding to the distally de- tected inputs. The term “telepresence” was introduced into the literature in 1980 by the computer scientist and A.I. (Artificial Intelligence) pioneer Marvin Minsky. Minsky’sinspirationwasthekindoftele-operationsysteminwhich a worker might handle radioactive materials by wearing a pair of special lenses, along with gloves and sleeves that transmit her arm and hand mo- tions to a robotic device. The device in turn transmits visual and tactile feedback to the operator. In this way, it is as if the operator were actually present in the hazardous environment. In such cases operators report that they rapidly and effectively come to feel the shift in point of view so vividly described by Dennett, flipping back and forth between the local and the distant locations as needed. 
True telepresence, insofar as it is achievable, would seem to require a high bandwidth multisensory bath of information with local sensory stimu- lation: in effect, the full virtual reality bodysuit, with feedout and feedback connections for sight, sound, hearing, touch, and smell, as well as heat and resistance sensing. Also—perhaps crucially—the user needs the ability not just to passively perceive but to act upon the distant environment and to command the distant sensors to scan intelligently around the scene. 
It is noteworthy, however, that temporary shifts in point of view can be achieved using much more limited and ordinary resources. If you visit the Virtual Artists’ VA Robocam Site (http://www.robocam.va.con.au/) you can interact with a motorized camera mounted on a tall building, sweeping the areaasyoudesire. AsimilarprojectwasdescribedbyMinskyintheorigi- nal Omni paper like this: 
A Philco engineer named Steve Moulton made a nice telepresence eye. He mounted a TV camera atop a building and wore a helmet so that when he moved his head, the camera on top of the building moved, and so did a viewing screen attached to the helmet. Wearing this helmet you have the feeling of being on top of the building and looking around Philadelphia. If you “lean over” it’s kind of creepy. But the most sensational thing Moulton did was to put a two-to-one ratio on the neck, so that when you turn your head 30 degrees, the mounted eye turns 60 degrees: you feel as if you had a 
This description highlights two important points. The first is that even quite basic but interactive technologies can generate a sense of real telepresence. Comparing the VA robocam experience (visit the telepresence hub at http://mitpress.mit.edu/telepistemology) with an experience of purely passive viewing (e.g., the wonderful web camera that looks at the African landscape: www.africam.com) is instructive. The passive experience leaves the observer clearly at home; it is no more like telepresence than looking at the photos in National Geographic (though it is sometimes more exciting 
Yet as soon as a distant camera responds to your controls, and especially if the mode of control is either natural (the helmet rig) or highly practical (a gamester with a joystick), you begin to feel relocated, as if you are in the distant scene. 
Given our discussion in chapter 3, this should come as no surprise. There we saw that normal human vision involves a complex process of intelligent search and just-in-time information retrieval. In normal vision we leave most of the information out in the world, secure that we can, with a flick of the eye, retrieve what we need to know as and when required. When faced with input from a fixed camera much of that flexibility is lost. As a result, the scene presents itself as a source of visual information, but not really as a context for fluent, embodied action. Our sense of personal location has more to do with this sense of an action-space than with any- thing else. 
The links between our capacities for action and our perceptual experi- ences are extraordinarily deep and potent. In a famous series of psycho- logical experiments, human subjects were fitted with special glasses whose lenses turned the visual input upside down. At first, as you would expect, the subjects saw an upside-down world, but after a period of sustained use, the visual world began to “flip back over.” After a few days, the sub- jects reported that their visual experience was back to normal. Remove the glasses, however, and the world now looks upside down (for a while, until re-adaptation occurs). Most interesting of all, these kinds of perceptual adaptations are highly action-dependent. They are primarily driven by the combination of the visual inputs and the subject’s experiences of trying to move and act in the world (and hence, crucially, by feedback coming through various motor and locomotion systems). Thus a subject fitted with the lenses, but simply pushed around in a wheelchair, does not show the ad- 
So fluent are our perceptual systems at making these motor-loop-dependent adaptations that it is even possible to adapt to both the presence and the absence of the lenses. By wearing the goggles intermittently, while acting in the world, you can train your visual system to cope with both kinds of input (right way up and upside down). This coping is, remarkably, quite seamless. The instant you don or remove the goggles, your visual system flips into one of the two “settings.” The scene before your eyes looks unchanged to you, aptation, while one who walks along a complex trail does. nothing seems to flip or alter; ask an untrained friend to try it, and she will immediately flounder in the face of the upside-down world! Interestingly, such adaptation need not be global. Instead, there can be adaptation for certain well-practiced motor routines and not for others. Subjects fitted with sideways shifting lenses (those that shift the image a little way to the left or right) who played repeated games of darts displayed adaptation only while using their normal dart throw. If they were then asked to throw underhand, or with their left hand (if they were right-handed), the between perceptual inputs and our own deliberate motions and actions. The notion that our perceptual experience is determined by the passive receipt of information, though seductive, is deeply misleading. Our brains are not at all like radio or television receivers, which simply take incoming signals and turn them into some kind of visual or auditory display. Who would there be to look at the display anyway? The whole business of seeing and perceiving our world is bound up with the business of acting upon, and intervening in, our worlds. And where action and intervention goes, our sense of bodily presence and location swiftly follows. The extent to which current efforts at telepresence support appropriate kinds of fluent action and intervention is, however, rather limited. Here are some fairly representative examples:
The first documented internet-based telebot was set up in 1994 at the Uni- versity of Southern California. The remote user could control a digital cam- era and airjet, mounted on a robot arm so as to “dig for artifacts” in a In 1996, Eduardo Kac, an artist, writer, and media theorist, set up an “inter- active networked telepresence” installation at the Nexus Contemporary Art Center in Atlanta. There, you saw a large (real) aviary stocked with thirty 
These re- sults further underline Ramachandran’s principle, as described in chapter 3. The principle, remember, was that the “mechanisms of perception are mainly involved in extracting statistical correlations from the world to cre- ate a model that is temporarily useful.”14 The most important of these cor- relations—as the nose-tapping experiments already suggested—are those compensatory effects of the practice immediately disappeared. sandbox in the USC lab. 
flying birds and one large robot bird. In front of the aviary was a virtual reality headset. Wearing the headset, the viewer was able to perceive the aviary from within the robot bird. The bird’s eyes were twin digital cameras, and the viewer’s head movements moved the head of the bird. The viewers could even watch themselves, standing outside the cage and wearing the headgear, in this way! The upshot, according to Kac, was that “the local participant [was] both vicariously inside and physically outside the cage . . . a metaphor that revealed how new communications technology enables the effacement of boundaries at the same time as it reaffirms them [and ad- dresses] issues of identity and alterity.”17 
GARDENER’S WORLDS 
The telegarden (Goldberg et al., 1994) is a telerobotic, web-accessed, yet totally real garden. Visitors use CCD cameras and a robot arm to plant seeds or plants, water them, pull them up, and so forth. The idea is to “invite participation” and encourage return visits and monitoring. You can read about the garden at http://telegarden.aec.at. 
TELEBOTIC TILLIE 
Go to www.lynnhershman.com/tillie and visit Lynn Hershman’s San Fran- cisco gallery through the eyes of Tillie, a telerobotic female doll (see fig 4.1). Click on her eye and you will see what is currently visible to the camera in Tillie’s eye as she sits in the gallery. You can turn her head, look around, and so on. You can also view the gallery “objectively,” seeing Tillie herself watching you, perhaps with your own eyes! 
The world of industrial telerobotics is, not surprisingly, somewhat more advanced. By “industrial robotics” I mean both what is more properly termed “teleoperation” and genuine “telerobotics.”18 Teleoperator systems are ones in which the human “master” directly guides a distant robotic “slave.” The slave is meant to merely echo, at a distance, the actions actually being performed by the human master. Early versions of such teleoperator sys- tems (called “telemanipulators”) would have been standard fodder at Los Alamos during the time of the Manhattan project, and some, for all I know, might even have found their way into Ed Groshus’s Black Hole (mentioned in chapter 2). They were originally developed to allow workers to manipu- late toxic or radioactive materials, and they represented a modest advance on the use of simple tong-like appendages. By 1950 there were quite ad- vanced mechanical systems, which translated the movements of an opera- tor on one side of a one-meter-thick quartz window into subtle and precise movements of an identical mechanism on the other side. The first true teleoperation system, however, was invented by Ray Goertz in 1952.
like the simple telemanipulators, these systems incorporated advanced elec- tronics and computer control; like the earlier systems, they involved a master and a slave manipulator. Motors, sensors, and calculating devices were also brought into play. Slave-side motors allowed the system to apply sig- nificant forces over much larger distances, while master-side motors sup- ported force feedback so that the operator could feel the resistance and compliance of the distant materials. 
The 1970s saw the widespread introduction of coordinate transforming teleoperators. These were systems in which the master motions were not simply replicated but were instead systematically transformed (scaled up, scaled down, made to fit within a confined space, etc.). Now, for the first time, the kinematics of the master and slave could diverge. 
By the 1980s, true bidirectional teleoperators existed, with the full six degrees of freedom (six independent motion axes) required for the fluent manipulation of rigid 3D objects. The first such device was designed by feel the force exerted on the distant slave.
In the late 1980s and throughout the 1990s, telerobotics came into their own. A telerobotic device is one in which the fine details of action control are left (at least in part, and at times) to the robot itself. 
Thus a telerobot on the moon might be told, by a human operator watch- ing a video image in Houston, to acquire the large rock appearing in the top right-hand corner of the operator’s dis- play. The robot would then locally compute the kind of walk, reach, and grip needed to carry out the task. In the same way, a house- hold robot, controlled over the internet, might be told to go into the living room and transmit pictures of the sleeping cat. The iRobot-LE (fig. 4.2) made by Rodney Brooks’s company has carried out similar tasks in a Boston apartment, while under supervisory control from Brooks in Tokyo. 
The practical reasons for moving toward telerobotics are obvious. It is easier for the operator to issue only high-level commands, and this may be essential when time-delays are critical and bandwidth limited. As a tech- nology of genuine telepresence, however, telerobotics may at first seem less promis- ing than teleoperation. The best teleoperator systems, after all, provide rich capacities of finely directed action and intervention, and a wide spectrum of sensory feedback (e.g., force feedback coordinated with visual feed back). This rich two-way energetic exchange is surely just the kind of link that might allow the distant equipment to become transparent in use, whereas issuing high-level commands, with merely visual feedback, to a distant robot seems less likely to generate any real shift in perspective. 
Advanced Telerobotics 
Such a diagnosis is, however, still a little too hasty. To see why, we need to revisit our old friend the biological brain. As a first step, however, recall the example (chapter 1) of the car driver who relies on an ABS (Automatic Braking System). Once drivers are accustomed to ABS, they cease to feel as if the braking is in any way “out of their control.” Yet the machinery medi- ating between the action of the foot and the actual braking is now much more intelligent than before, able to adjust and pulse the braking action as required. The presence of such modestly intelligent intermediaries, how- ever, need in no way compromise our sense of direct engagement and con- trol. Such semi-intelligent technologies can become as transparent in use as any others. In fact, we are all intimately familiar with this kind of case, since much of our daily bodily activity (and, indeed, our daily decision making) falls into the same category. 
Take the simple (or not-so-simple) act of walking to the store. The last time I walked to the store, the sum total of my conscious, deliberate neural activity amounted to something like this: “Oh dear, we’ve run out of Guinness. I’ll just pop out and pick some up. Hope they’ve got some in the chiller.” The high-level decision thus made, a great deal of subsequent ac- tivity was left to the good devices of, well, my good devices. It was left to various neural and biomechanical subsystems operating way beneath the levels of my conscious awareness. I never decided, for example, just how far to swing back my right leg while walking to achieve a steady gait (though I suppose I might have, had we drunk a great deal more Guinness before- hand). I never decided how to move around my head and eyes to spot looming obstacles, or how precisely to time and control the trajectory of my hand and arm while reaching for the beer. In fact, on reflection, most of what I did I seemed to have very little to do with. Even the decision to actually go out and get the beer, although it, at least, was conscious, did not seem to arise from any set of previous conscious thoughts. It was just suddenly there, in my head, at the forefront of my thoughts (most deci- sions, as they say, are born, not made). The conscious self, it quickly ap- pears, is but the tip of the “I” berg; the vast bulk of neural activity leading both to, and away from, this tip is unconscious. 
Recent experiments confirm and dramatically extend this general diag- nosis. Take a look at the two pairs of center (large) circles displayed in figure 4.3. Which circle strikes you as the largest? In the top case, both center circles are the same size; in the bottom case, the one on the right is larger—but they probably didn’t look that way to you. Your conscious perception is misled, it seems, by scaling effects caused by the other smaller circles surrounding the targets. This visual illusion is known as the “Titchener Circles” or Ebbinghaus illusion. In 1995 Aglioti and his colleagues published a suggestive follow-up experiment.  In this experiment, thin poker chips were used as the center circles; subjects were asked to physically pick up the chip on the left if the two appeared equal in size, and to pick up the one on the right if they appeared unequal. Subjects used the same hand for each task, and sensitive opto-elec- tronic recording equipment was used to record the precise size of their finger-thumb grip aperture (measured just before actual con- tact with the disk). As expected, the subjects’ choice of chip was influ- enced by the illusory scaling effect, so they would choose in ways de termined by the apparent, not the real, sizes of the disks. But—and here’s the punch line—their fine-tuned finger-thumb grip aperture 
Milner and Melvin Goodale, is that the human visual system is already a hybrid, a cooperating mixture of two distinct elements. One element is an evolutionarily ancient system for controlling motor actions using visual information. The other, more evolutionarily recent system, takes the same visual inputs but processes them very differently. It extracts information about what the object is (is it a cat, or a cup?), and it makes contact with memory systems (is it an especially heavy cup?) and with reasoning sys- tems (is it covered in oil, and hence slippery?). In extracting this kind of information, Milner and Goodale believe, this system must discard much of the fine detail (precise location in space relative to current arm location, etc.) required to actually act on the object. Nature’s solution, they argue, is a kind of biological division of labor. One set of neural circuits (the ventral stream), leading from V1 (early vision) to IT (interotemporal cortex), is de- voted to recognition, classification, and reasoning. Another set of circuits. (the dorsal stream), leading from V1 but proceeding to PP (posterior pari- etal cortex), is devoted to the fine control of ongoing action (movement). It is this latter system, they claim, which is most directly implicated in conscious seeing and verbal report. a kind of limited interaction. 
In the case of the Tichener circles it is the conscious (illusory) percep- tion of one circle as larger than the other that causes the autopilot-like subsystem to reach for a specific disk. Our conscious high-level decisions thus serve as the impetus for the other systems to do their stuff, while still devolving substantial subproblems (like the calculation of exact grip and reaching trajectory) to other internal agencies. The conscious self in these cases is exercising a form of what Thomas Sheridan originally dubbed “Su- pervisory Control”: a “type of control in which goals and high level com- mands are communicated to the slave robot.”25 
Consider the moonrock-gathering telerobot once again. The human op- erator spots a rock in the top right-hand corner and tells the robot to ac- quire it. The robot then plans the walk and calibrates the reach. Just how different is this, from the case in which the conscious “I” decides to reach for an object (one of the disks or a can of beer) and nonconscious neural systems kick in to compute arm trajectory, determine grasp size, and so forth? In a recent piece, Mel Goodale suggests that the interplay between the neural systems generating our conscious perceptions and those respon- sible for the remaining details is thus “reminiscent of the interaction be- tween the human operator and a semi-autonomous robot in what engineers call teleassistance.”26 Ramachandran, the neuroscientist we met in chapter 3, likewise speaks of “the Zombie in the brain,” meaning the mass of auto- matic subsystems, which contribute so profoundly to our thoughts, ac- tions, capacities, and skills. The neuropsychologist Michael Gazzaniga devotes the bulk of his 1998 book The Mind’s Past to showing that “even though our sense of purpose and centrality of will are foremost, there dwells within us an automatic and highly specialized machine.”
The original cyborg vision, as we saw in chapter 1, was precisely the vision of external, nonbiological elements taking over various automatic functions of the nervous system. At that time, however, attention was largely focused on systems that controlled basic bodily functions such as heart rate and respiration. The goal was to allow the electronically augmented 
human body to survive in otherwise inhospitable conditions. The full range of tasks that the brain carries out automatically is, however, now known to be much, much larger, and to include many of the operations involved in complex problem solving and even decision making. Knowing this, the range of possible cyborg-like extensions of the human mind expands dra- matically. Not just basic physiological homeostasis, but limb control, tra- jectory planning, and major components of the reasoning process itself may themselves be farmed out. There is no special magic associated with direct physically wired links between components. The differences between links forged by nerves and tendons, by fiber-optic cables, and by radio waves are relevant only insofar as they affect the timing, flow, and density of informational exchange. These latter factors are relevant, in turn, because they affect the nature of our relationship with the various kinds of tools, equipment, and subsystems. If the links are sufficiently rich, fluid, bidirec- tional, fast, and reliable, then the interface between the conscious user and the tool is liable to become transparent, allowing the tool to function more like a proper part of the user. The move thus from teleoperator systems to telerobotics systems relying on high-level commands need not result in the alienation of the tool from the conscious user—no more so than the fact that the conscious self merely deciding to go fetch some Guinness results in the alienation of my arms, legs, locomotion, and grasp control systems from the “real me”! In practice, however, teleoperated systems seem to induce the feeling of actual telepresence much more effectively than do existing telerobotic ones. It is time to examine why this is so. 
Imagine that you are the human operator of an original Manhattan project telemanipulator. Deep in your B-movie concrete bunker, you handle toxic materials from behind the safety of a thick quartz window. These first-gen- eration devices were clumsy and primitive by today’s standards, but despite this “the one-to-one connection between the two sides creates a compelling sensation reproducing the actual sensations of manipulation.”28 The user, in this case, feels as if he or she is actually touching and manipulating the (modestly) distant materials. The blind person whose cane feels like a sensi- tive extension of her arm is the obvious classic case. You may have had the same experience using chopsticks to select the tastier morsels from the com- munal platter. Or, when driving your car, you may have had the experience of feeling the road through the system of racks, pinions, axles, and tires. 

What seems to matter in these cases is the presence of some kind of local, circular process in which neural commands, motor actions, and sen- sory feedback are closely and continuously correlated. This, of course, is ex- actly what Ramachandran’s principle (which depicts the body image as a temporary construct based on ongoing sensory correlations) would lead us to expect. Remember the compelling demonstrations in chapter 3 where the subjects came to feel as if the desktop or dummy hand were the source of tactile signals being fed to their biological brain?29 When the dummy or desktop was then hit with a hammer, these subjects showed a galvanic skin response consistent with the expectation of damage to their biological body. They had, at the very deepest level (and after only a few minutes of training), come to identify themselves with the nonbiological “extensions.” As Ramachandran put it: 
It was as though the table had now become coupled to the students own limbic system and been assimilated into his body image, so much so that pain and threat to the dummy are felt as threats to his own body, as shown I don’t think the authors are being entirely facetious when they add that “if this argument is correct, then perhaps it’s not all that silly to ask whether you identify with your car. Just punch it to see whether your GSR changes.”31 
In general, then, the sense of extension, alteration, and distal presence arises as a result of close, ongoing correlations between neural commands, motor actions, and (usually multisensory) inputs. Simple telemanipulation and teleloperator systems afford this kind of dense, real-time correlation. The payoff is a compelling sense of bodily augmentation and extension, a sense of genuine, (if modest) telepresence. The intimate web of closely cor related signals and responses necessary for such rarified reinvention of the body is, however, quite fragile and easily disrupted. The most important kind of disruption is temporal: if there is a noticeable time lag between issuing the command and receiving the sensory feedback, or (worse still) if the time lag is variable due to the traffic on phone lines, for instance, the illusion is shattered. This is what happens, then, as applications grow in complexity, and distance increases. Before continuing, I’d like to pause and take back something I just said. I just wrote that when the web of real-time signaling is disrupted, “the illusion is shattered,” but this is dangerously misleading. For it is the bur- den of this text to argue that in a very significant sense, the feeling of telepresence is not an illusion at all, or to be more precise that either the basic feeling of presence is always some kind of illusion, even in the normal everyday case, or if you don’t want to count that feeling as illusory, the case of feeling the cup with my hand and feeling it with the telemanipulator are really, in the deepest sense, potentially on a par. I am arguing here for a kind of parity. Our sense of bodily presence is always constructed on the basis of the brain’s ongoing registration of correlations. If the correlations are reliable, persistent, and supported by a robust, reliable causal chain, then the body image that is constructed on that basis is well grounded. It is well grounded regardless of whether the intervening circuitry is wholly bio- logical or includes nonbiological components. 



Slugs, Ants, and Amazon.com 
A mild winter’s morning in Norbury, South London. The sun is freshly risen, and there is coffee steaming in the pot. I look into my mother’s backyard and it is awash with glistening, sticky signatures. The walls and paving stones gleam and sparkle with narrow, silvery undulating trails: unmistakable evidence of the nocturnal passage of common garden slugs. 
There was a time, now dimly recalled, when I found the mucal scribblings of these small creatures less than entrancing. The trails were, I felt, merely the unsightly by-products of an eccentric mode of locomotion. Today I am enthralled. These glimmering trails are not mere ambulatory by-products but active elements in a distributed, multifunctional, activity-enhancing grid—key players in a smart world for slugs. These trails record, reveal, and simultaneously help structure slug activity. Our own electronic trails, laid down as we access data, buy online, and move physically through a world of intercommunicating information appliances, will likewise play multiple important roles in shaping our collective cyborg destiny. By ex- ploiting these trails we will automatically generate new knowledge as we read, buy, and act. It’s a global electronic free lunch, and the appetizers are already on the table. But first, the slugs. 
These ants, while seeking food, lay down a distinctive pheromone trail. Now imagine that there are two food sources, one nearer the nest than the other, and that randomly ex- ploring ants discover each source. The ants returning from the closer food source follow their own trail, which now becomes marked with twice the concentration of pheromone. The same applies, of course, to the ants re- turning from the more distant source. But the ants whose total route out and in is the shortest arrive back first, and the pheromone concentration on that trail is therefore temporarily greater. So new ants set out on that trail and, on return, again increase the amount of pheromone, causing even more ants to choose that trail. This process of “positive feedback” (in which successful foraging leading to increased pheromone concentrations, which leads to still more successful foraging, leading to yet another in- crease in pheromone concentration . . . ) allows the colony to rapidly self- organize in order to discover and exploit the best, meaning shortest) routes before gradually moving on—once the nearby food is exhausted—to the next closest source, and so on. 
But what, you may well ask, does all that have to with us. A few perfumes and pheromones aside, we humans seem noticeably lacking in native trail- laying skills. Here the contemporary cyborg has a distinct edge, for she is already an electronically tagged agent, swimming in an unremarked sea of intercommunicating information appliances. As we move in physical space and act in commercial and intellectual space, we can automatically lay elec- tronic trails, which can be tracked, analyzed, and agglomerated with those laid by others. Already, trails laid down during web-based search, purchas- ing, and communications can and are used to inform and personalize the relations between buyers and vendors. Take the mundane business of buy- ing a CD from a firm such as amazon.com. Suppose, to be concrete, you are about to purchase the latest Nick Cave CD. You are told, on-screen, that “people who bought this CD also bought . . . ” You are then presented with a list of other CDs purchased by other purchasers of works by Nick Cave. This apparently pedestrian little trick is, in fact, astoundingly potent. Many times I have been led, via the purchasing paths laid down by others who share some aspect of my tastes or interest, to find new and wonderful things, well suited to my somewhat peculiar tastes. To appreciate the full value and potential scope of such techniques we must first understand a little more about how they work, placing them in the larger context of what are some- times called “self-organizing knowledge structures.” 
The CD-suggesting technique used by Amazon depends upon a technique known as “collaborative filtering.” I first learned about this while visiting the Complex Systems Modeling Team at Los Alamos National Laboratory in 
New Mexico. Luis Rocha, a member of the team there, introduced me to a way of thinking about such techniques as exploiting the basic principles of “swarm intelligence.” In swarm intelligence, relatively dumb individual agents (ants, bees) create beautiful, complex, and life-enhancing structures (foraging trails, honeycombs, hives) by following a few simple rules and by automatically pooling their knowledge courtesy of chemical traces and struc- tural alterations laid down by their own activity. 
Collaborative filtering, as Norman Johnson (head of the aptly named Symbiotic Intelligence Project at Los Alamos) notes, exploits very similar  principles to those underlying pheromone-based self-organization. Each episode of use or access by a consumer lays down a trace, and after a sufficient amount of consumer activity, exploitable patterns emerge. Sup- pose, then, each purchaser of the Nick Cave CD also knew of, and pur- chased, two other CDs. One is a gift for a friend whose tastes are very different indeed; the other is a CD the purchaser thinks she might like for herself. The chances of substantial overlap in consumer choices concern- ing the nongift CD are much greater than in the case of the gift. So the trails that get doubly and triply marked by this self-selected group (buyers of Nick Cave’s latest CD) are indeed more likely to lead to products that will appeal to the rest of the group (the Cave-lovers). The simple tactic of allow- ing consumer activity to lay down cumulative trails thus supports a kind of automatic pooling of knowledge and expertise. 
One reason this kind of procedure is so potent is because it allows pat- terns of consumer action to speak for themselves and to lay down tracks and trails in consumer space as a by-product of the primary activity, which is online shopping. Those collective tracks and trails have the great advan- tage of sidestepping all the simplistic categories that we human beings use to classify our own choices. For example, instead of classifying a Nick Cave fan as belonging to this or that category and then offering suggestions based on that act of pigeonholing, the “category” is created on-the-hoof by the consumer activity of many Nick Cave fans. If many Nick Cave fans were also listening to The Handsome Family or Peaches, then despite the lack of any obvious common category, these will indeed be duly suggested. Notice how deeply and genuinely different this is from a traditional system that simply assigns each CD to a category (i.e., Patsy Cline = C&W) and then offers you top sellers from that category. “Categorization” by cumulative trail laying is unplanned, emergent, and as flexible as consumer choice itself. Later in this chapter we will see how the same kinds of consideration can be applied to the development of internet search engines, so as to sidestep the rigidity of a the typical keyword-based approach. 
Providing the electronic environments that best support flexible, un- planned, collectively self-organized modes of information extraction, re- trieval, and organization is immensely important if we are to press maximal benefit from the burgeoning web of human knowledge. I recently purchased a copy of Neil Gershenfeld’s excellent treatment of the near-technological future, Things That Think. I was appalled to see on the back cover the simple categorization “Non-Fiction: Computing.” Such a classification leaves out at least half and probably much more of the ideal readership, which includes artists, designers, engineers, philosophers, and cognitive anthro- pologists. My initial reaction, however, was surely inappropriate because we increasingly live in a world in which the rather arbitrary labeling deci- sions—made by well-intentioned vendors—can be trumped by emerging and self-reinforcing patterns of consumer choice. If just a few artists and designers discover and purchase the book, their purchasing trails (which may combine the purchase with others more traditionally suited to their disciplines) will bring the book to the attention of others in their group. All this will happen automatically, as a result of the trails laid during con- sumer activity. The vendors need never know. The traditional labels need never alter. Now, instead of being consigned to one dusty corner of a physi- cally organized bookshop, Gershenfeld’s book lives at the complex inter- section of multiple purchasing trails and is equally and simultaneously “present” in multiple viewing locations. Moreover, these locations will shift and alter over time in response to an open-ended set of continuously con- structed purchasing trails. The books themselves are, in a sense, actively tracking their best contemporary audiences! 
We are merely scratching at the surface. Returning to the original CD- buying scenario, imagine a slightly more sophisticated system, which still exploits past combinations of consumer choice. You buy Nick Cave and Patsy Cline, and the system offers you suggestions based on the buying habits of those other folks who are fans of both artists. Given a substantial history of consumer trail laying, such a system should automatically track the buried commonality that binds Cline and Cave into a coherent whole. 



Finally, imagine a system that retains a trace not just of what different individuals purchased but of the temporal sequence in which they did so. Such a system might fluidly track common patterns of taste-evolution, and thus offer useful hints about what to try next. 
Uncongested links, allowing the rapid free flow of multiple calls, will quickly accumulate an attractive “scent.” Now suppose that the traces evapo- rate over time. The scent deposited in a blocked or slow and congested area will soon disappear, and the link will become unattractive. Some pre- set degree of random exploration can allow “dead” links to become gradu- ally open again as a few calls pass successfully through. Potent variants include having each call adjust its “scent deposit” according to how long it took to pass through the link, and so on. Such a system continuously self- organizes into an efficient overall message-passing configuration, without any central authority or global monitoring system. Grassroots computation at its finest. 
Better Living Through Search 
This general idea, of strengthening and weakening connections and trails as an automatic result of ongoing patterns of use, may one day turn the world wide web itself into a kind of swarm intelligence. Another Los Alamos- based group, the Distributed Knowledge Systems Project, has pioneered a kind of self-organizing web server called the Principia Cybernetica Web. The key feature of the server is its ability to create, enhance, and disable links between pages as an automatic result of use. More popular links be- come increasingly prominently displayed, instigating the kind of positive feedback process described earlier, while little used links wither and fade away. The server can also create new links using a technique that one of the system’s originators, Francis Heyligher of the Free University of Brus- sels, calls “transitivity.” Roughly, if many users move from a site A to a site 
B and then on to C, it will instigate a direct link from A to C as a kind of shortcut. Returning full circle to the theme of individual human-machine mergers, servers may one day do all this on something more like a user-by- user basis. When you log on, you will be recognized and the hyperlink structure partially adapted to suit you. 
To begin to grasp just how very different this would be, consider what a similar degree of user-sensitivity would look like were it (impossibly) real- ized in the physical world of roads and interstates. Our roads, too, are nonbiological structures, which alter and transform our needs and projects; they are largely fixed and static structures, slow to alter and respond to changing needs and pretty well impervious to the quirks of individual road users. Imagine then a world in which the roads and interstates automati- cally adapt and change, re-routing themselves in response to patterns of use. Little used routes become smaller and then fade away, busy routes automatically expand, varying according to the time of day, and there is automatic re-routing around congested areas. Most spectacularly of all, the whole road network slightly reconfigures itself in response to your per- sonal tastes each time you step into your car. A Traffica Cybernetica would be something like that! 
The best of the new generation of internet search engines, although they do not actively restructure hyperlink space, nonetheless work by exploit- ing the collectively created knowledge implicit in the links between web pages. They mine the knowledge implicit in the multiple trails (in this case, the hyperlinks between web pages) that structure the collectively created web. First generation search engines such as AltaVista and Infoseek relied heavily on fairly simple forms of first-order heuristic search, ranking pages according to the number of times the query items appeared, or how early in the text they did so. Such engines often retrieved, even when used prop- erly, voluminous junk and had a regrettable tendency to miss the good stuff altogether. There is a sense in which this is not surprising, for the problem they confront is formidable. There are often literally millions of pages whose contents look superficially relevant, especially given that the usual test for relevance is dumb syntactic matching: the search engine seeks pages that either contain, or are indexed as containing, tokens of the spe- cific string or strings entered by the user. The situation is worsened by the unplanned, anarchic nature of the web itself: there is little deliberate global organization of the kind that might be useful in streamlining search. Sec- ond-generation search engines, such as Google, have found an interesting way around this problem. The key trick, it seems, is to focus attention not (ultimately) on the content of the pages so much as on the structure of links between pages. The hyperlink structure itself—the way different pages link to and from each other—turns out to be a treasure house of communally generated implicit knowledge concerning which pages are most central and authoritative regarding a given topic. In 2001 the Cornell University com- puter scientist Jon Kleinberg received the National Academy of Science award for Initiatives in Research for his work on such methods. Kleinberg devised a set of algorithms or formal methods to extract and utilize some of  the knowledge implicit in the burgeoning web of connectivity. 
Here are a few other examples: to search for “search engines” is espe- cially tough because many of the most authoritative pages (Yahoo, Excite, AltaVista) do not use that term on their home pages; to search for very broad topics, such as “censorship,” tends to return a hodgepodge of largely nonauthoritative sources. Standard searches thus tend to be both ineffi- cient (returning too much) and insufficiently intelligent (despite returning too much, they often miss—or return way down in the list—the most rel- evant and authoritative sites). 

We live, after all, in a society where a great deal of behavior, which is neither illegal nor harmful, might if made public impact negatively upon our lives and careers. Spare a thought for the grade school teacher who likes to cross-dress in the privacy of his own home but buys the clothes off the web, and visits other sites and chat rooms to share discoveries and experiences. Or the peace campaigner with a taste for violent literature and a one-click account with Amazon. Or the Catholic priest with a nuanced love of women’s lingerie. Or, to end with a real-life example from the Brit- ish press, the gay police chief with a soft spot for anarchy and cannabis. I leave the reader to fill in her own special interests. The list is endless, the shades of gray innumerable. 
And then there is the e-romance. Anyone foolish enough to attempt to conduct an extramarital, or otherwise unauthorized, affair using electronic media will quickly find cause to regret those tracks, trails, and incomplete deletions. They spell doom for your dreams of a private corner of cyberspace, complete with white picket fence and a full range of modern domestic appliances. 
Then there are drugs. In an age when large numbers of well-informed, intelligent adults occasionally partake of recreational drugs (other than the taxed, time-honored, and often quite lethal alcohol), it might be hoped that they will do so in as careful a manner as possible. To that end, they might visit a useful site such as Dancesafe, which offers balanced information con- cerning doses, effects, addictiveness, and relative toxicity. Yet if such visits are perceived as a two-edged sword, perhaps helping users avoid the worst kinds of abuse but simultaneously adding their names and details to some law enforcer’s list, can we really hope for such care and caution? 
All this is disturbing since it again hints at the creation of a new elite: this time, the elite subset of internet users who stand any chance of achieving even a modicum of privacy. For every one who deploys advanced security tools such as Kremlin and public key encryption, there will be a thousand who trust to the goodwill of commerce and government. In any case, it seems unlikely that many citizens, be they ever so technologically aware, will ever win an “arms race” between users and government/employers. Encryption and firewalling is probably not the ultimate answer. 
Another possibility, which I have grudgingly come to favor, involves a kind of leap of faith, or democratic optimism. As our governments, em- ployers, colleagues, and lovers learn more and more about the typical be- havior of a wide range of valued, productive, and caring citizens, it should become clearer and clearer in what ways the goalposts of “good behavior” must be moved. Such movement need not signal decay and decline, for only hypocrisy and more solid public/private firewalls ever kept them in place! As the lives of the populace become more visible, our work-a-day morals and expectations need to change and shift. It is time for the real world to play catch-up with our private lives, loves, and choices. As the realm of the truly private contracts, as I think it must, the public space in any truly democratic country needs to become more liberal and open- hearted. This attitude, which I am calling democratic optimism, may seem naively idealistic, but it is surely preferable to an escalation of cyber wars that the average citizen simply cannot hope to win. 
Uncontrollability 
Some suggest that we should actively limit our reliance on technological props and aids, not just to protect our privacy but to control our own desti- nies and preserve our essential humanity. Here, the title of this book gives me away. Human-machine symbiosis, I believe, is simply what comes natu- rally. It lies on a direct continuum with clothes, cooking (“external, artificial digestion”), bricklaying, and writing. The capacity to creatively distribute labor between biology and the designed environment is the very signature of our species, and it implies no real loss of control on our part. For who we are is in large part a function of the webs of surrounding structure in which the conscious mind exercises at best a kind of gentle, indirect control. 
Of course, just because nature is pushing us doesn’t mean we have to go. There are times, to be sure, when the intelligence of the infrastructures does seem to threaten our own autonomy and to cede too much, too soon, to the worlds we build. In the novel Super-Cannes, J. G. Ballard depicts a highly engineered environment (“Eden-Olympia—the first intelligent city”) in which there are no more moral decisions than there are on a new superhighway. Unless you own a Ferrari, pressing the accelerator is not a moral decision. Ford and Fiat and Toyota have engineered in a sensible response curve. We can rely on their judgment, and that leaves us free to get on with the rest of Chilling stuff. The more so since this vision of machines bearing more and more of the load once borne by biological intelligence is precisely the one with which Clynes and Kline launched our cyborg odyssey, back in 1960 and back in chapter 1. 
We are torn, it seems, between two ways of viewing our own relations to the technologies we create and which surround us. One way fears retreat and diminishment, as our scope for choice and control is progressively eroded. The other anticipates expansion and growth, as we find our ca- pacities to achieve our goals and projects amplified and enhanced in new and unexpected ways. Which vision will prove most accurate depends, to some extent, on the technologies themselves, but it depends also—and crucially—upon a sensitive appreciation of our own nature. 
Many feel, for example, that increased human-machine symbiosis di- rectly implies decreasing control. In an age of Ubiquitous Computing must we be slaves to the whims of the machines that surround us? In an age of global swarming, should we fear that even the machines don’t have a leader? Have we cast ourselves as King Lear, but with whole legions of ungrateful daughters? 
As we saw in chapter 5, the kind of control that we, both as individuals and as society, look likely to retain is precisely the kind we always had: no more, no less. Effective control is often a matter of well-placed tweaks and nudges, of gentle forces applied to systems with their own rich intrinsic capabilities and dynamics. The fear of “loss of control,” as we cede more and more to a supporting web of technological innovations is simply misplaced. 





Degradation 
Close cousin to these worries about deceit is a worry about lack of quality control. In the wired (and soon to be wireless) world, where anyone can publish thoughts and insinuate e-mails into thousands upon thousands of inboxes, how are we to separate the wheat from the chaff? The problem is especially pressing given the very real problem of overload, mentioned ear- lier. Time is a precious resource, and we cannot afford to read everything everyone has to offer us in order to decide—even assuming we could tell— what is most authoritative or important. 
Sometimes, of course, an item might arrive in our inbox with the valida- tion of a close and trusted friend. In the case of the Ansary letter just de- scribed, my first copy arrived early in the chain, and with the endorsement of just such a friend. But what happens when materials arrive via a public bulletin board or an unpoliced newsgroup? The alternatives at first seem stark: either we regress to some kind of good old-fashioned central author- ity (such as reading only the online Times), or we confront an unsorted, unfiltered barrage of information, misinformation, and innocent but time- consuming spam. Hope, however, springs eternal, and our choices may not be so stark after all. 
To start with, the board worked well. The friends shared many interests (Star Wars, video games—you get the picture) and posted only things that most of the group wanted to see. But as time went on, traffic increased, much of it from far away. The board’s originator, Rob Malda, was unable to filter the postings. His first response was to appoint some lieutenants—people he trusted to help sieve the spam. Apart from locking out the truly offensive or totally irrelevant, these lieutenants had an added power: the power to rate the re- maining contributions on a scale of 1 to 5 (5 being the best). Users of the board could then choose what quality level they wanted to inspect. 
What could be done? Rather than try the normal remedies, Malda made the entire group collectively responsible for its own quality control. The system worked like this. After a few visits as a Slashdot user, you might find a message telling you that you had been temporarily assigned the role of moderator. At any given time, a shifting subset of users would have this status (rather like being called to jury duty) and would be asked to rank other users’ contributions on the 1 to 5 scale. Each moderator is allowed only so many points, and once they are awarded, the moderator ceases her role. On top of this, however, Malda introduced a system he called Karma. A specific user would accrue Karma according to how many of the person’s past postings had achieved high ratings. Those with “good karma” got spe- cial rewards! New postings from these users would begin with a higher default rating than the others, and the users would more likely be chosen as moderators. The moderation process thus collectively helps choose the moderators themselves. As a result, those whose postings were most highly ranked by the group tended to become the key figures in guiding the group ahead. Best of all, the new system works. A new user can just set the qual- ity control to 4 or 5 and find thousands of recent postings reduced to a few dozen high-quality items, while the more adventurous, or time-liberal, user can still explore the peripheral spaces in search of missed gems. 
This broad approach, in which users rate the activities (including espe- cially the rating or reviewing activities) of other users, offers the best cur- rent hope for a kind of collective, flexible, grassroots approach to the tricky questions of policing, filtering, and regulating. At its best it preserves most of the delicious freedom and anarchy of the web, while allowing individual users to reduce their cognitive loads and home in on reliable sources more or less at will. Amazon, eBay, and other large web-based concerns have all implemented their own versions of these so-called meta-feedback systems (ones using feedback about the usefulness and quality of feedback) in the last few years. 
Finally, as Steven Johnson notes, there is no need to fear that such sys- tems must tend toward narrowing and conservatism (for example, favoring postings that are liked by the average user). Instead, the underlying algo- rithm could be altered to favor moderators whose choices have sparked the most feedback, or whose own postings have generated large numbers of re- sponses both pro and con, and so on. These moderators would still hunt the spam, but with the overall system thus tweaked, the level 5 filter would now favor not the safe median but the stuff most likely to generate intense debate and feedback. In fact, a single system could easily offer both, allowing users to choose which kind of filter (median or controversial) they prefer. The possi- bilities thus exist for an open-ended variety of new and potent forms of swarm intelligence, with meta-feedback reconfiguring our filtering routines to suit the different types, or moods, of users. 
Disembodiment 
I have a special stake in this one, as I have long championed the impor- tance of the body in the sciences of the mind. One of my books even bears the subtitle “Putting Brain, Body and World Together Again.” Imagine my horror, then, to find myself suspected, in writing enthusiastically of tech- nologies of telepresence and digital communication, of having changed sides, of now believing that the body didn’t matter and the mind was some- thing ethereal and distinct. 
Far from having changed sides, however, the present work flows directly from this stress on the importance of body and world. What we have learned is that human biological brains are, in a very fundamental sense, incom- plete cognitive systems. They are naturally geared to dovetail themselves, again and again, to a shifting web of surrounding structures, in the body and increasingly in the world. Minds like ours solve problems not by intel- lectual force majeure but by cooperating with all these other elements in a spaghetti-like matrix. Just about everything in the present treatment speaks in favor of that image, from the use of pen and paper to do complex sums to the ease with which Stelarc now deploys his “third hand,” to the daily babble of cell phones and text messages with which we now coordinate our social lives, all the way to the use of mind-controlled cursors, swarm- based data-mining, and telepresence guided house-minding devices. More- over, as we saw in chapter 2, the intimacy of brain and body is evidenced in the very plasticity of the body-image itself. Our brains care so much about the fine details of our embodiment that they are ready and willing to recalibrate those details on the spot, again and again, to accommodate changes (limb growth, limb loss) and extensions (prosthesis, implants, even sports equipment). It is this tendency that allows them sometimes to be fooled by certain tricks, and it is because of this that the physical feeling of remote presence—and even of remote embodiment—is sometimes quite easy to achieve. The brain, in all these cases, is just one player on a crowded field. Our experience of what it is to be human, and our sense of our own capacities for action and problem solving, flows from the profile of the whole team. 
Whence, then, the fears about “disembodiment”? One root of the worry is the popular image of the lonely keyboard-tapping adolescent, who pre- fers video games to human company, takes no interest in sports or direct- contact sex, and who identifies more closely with his or her own electronic avatar or avatars than with his or her biological body. Isolated, discon- nected, disembodied, desexed. Virtues, perhaps, in a politician, but hardly what we would wish for any child of our own. 
is hard to imagine such a group achieving this kind of critical mass. There is, however, a new danger that accompanies the creation of more and more specific (often gated) electronic communities. It is one that is espe- cially marked in the case of communities held together by shared but un- usual sexual preferences or tendencies. The danger is of a new kind of marginalization. By relying upon an electronic community in which it is easy to speak of unusual needs and passions, people with special interests may find it easier to live out the rest of their lives without revealing or admitting this aspect of their identity. This could be dangerous insofar as it artificially relieves the wider society of its usual obligations of understanding and sup- port, creating a new kind of ghetto that once again hides the group from the eyes—and protective social policies—of mainstream society.
It is a delicate matter, then, to balance this danger against the compet- ing vision (explored a few specters back) of new media allowing us slowly and safely to explore multiple aspects of our personal and sexual identities. Once again, the most we can do is to be aware, as individuals and as public servants, of this danger, and to make active efforts to take account of even these relatively invisible minorities in lawmaking and social policy.
A less familiar version of the more general worry about “disembodiment” takes the idea quite literally. With so much emphasis on information trans- mission and digital media, the physical body itself can begin to seem some- what unnecessary. Respected scientists such as Hans Moravec speak enthusiastically of a future world in which our mental structures are some- how preserved as potentially immortal patterns of information capable of being copied from one electronic storage medium to another. In the redu?ing heat of such a vision, the human body (in fact, any body, biological or 
To be fair, Moravec himself repeatedly stresses the symbiotic nature of good forms of human-machine relationship. His vision of the self as a kind of persisting higher-order pattern is, ultimately, much more subtle and interesting than his critics allow. But what I seek to engage here is not the true vision but the popular caricature: the idea that the body and its capa- bilities are fundamentally irrelevant to the mind and hence the self. Noth- ing, absolutely nothing, in the account I have developed lends support to such a vision of essential disembodiment. In depicting the intelligent agent as a joint function of the biological brain, the rest of the human body and the tangled webs of technological support, I roundly reject the vision of the self as a kind of ethereal, information-based construct. There is no informationally constituted user relative to whom all the rest is just tools. It is, as we argued in chapter 5, tools all the way down. We are just the com- plex, shifting agglomerations of “our own” inner and outer tools for thought. We are our own best artifacts, and always have been. 
Some of these tools, to be sure, help constitute our conscious minds, while many operate below or beneath or otherwise outside of that domain. As we have repeatedly seen, it would be crazy to identify the physical basis of oneself solely with the machinery of the conscious goings-on. As we saw, just about everything we do and think arises from a complex interplay between the contents of conscious awareness and reflection and the more subterra- nean processing that throws up ideas, and supports fluent real-world action. If there is any truth at all, then, in the image of the self as a kind of higher- level pattern, it is a pattern determined by the activities of multiple conscious and nonconscious elements spread across brain, body, and world. 
Fine words indeed. But no consolation, one supposes, to our isolated friend, tapping away at the keyboard late at night, fearful of human con- tact and aroused only by the occasional warbling of “it’s not my fault” emanating from the speakers as the machine crashes for the tenth time that day. While this lifestyle may have more good in it than many critics believe, it is (I submit) a vision of the past. The agenda of human-centered technology differs in just about every respect. In particular, such technolo- gies hold out the promise of more mobility, richer interfaces, and richer interactive support. Far from being stuck in an isolated corner, our hero may find herself engulfed in a mobile, varied, and physically demanding social whirl. 
First and foremost, human-centered technology aims to free the user from that whole “box on a tabletop” regime: the regime of sitting, looking at a screen, and interfacing with the digital world using the narrow and demanding channels of keyboard and mouse. Wearable computers, aug- mented reality displays, and richer interface technologies transform this image beyond recognition. Mobile access to the web will soon be as com- mon as mobile access to a phone line. Keyboard interfaces, of all kinds, will be augmented, and sometimes replaced, by the kinds of rich, analogue interface described in chapter 2. Instead of touching tiny and elusive keys to pull up a menu to select a favorite web site, you might just move a finger to touch an icon that only you can see, hanging in the air about three inches above your eyeline. At first, such augmentations may rely on clumsy spectacle-based displays—but in the end, all the new functionality may be engineered into our eyes themselves. 
As a simple taste of this kind of freedom, imagine the probable end point of the cell phone revolution. The receiver will be surgically implanted in order to make fairly direct contact with the auditory nerve or perhaps even the ventral cochlear nucleus. Alerted to an incoming call by a characteristic tingling in the fingers, you can take the call without anyone else hearing; your replies need not be spoken aloud as long as you gently simulate the correct muscle movements in your throat and larynx. Such a technology would look, to us today, like to some kind of “telepathy.” There are pros and cons to such a scenario, without a doubt, but there would certainly be no feeling of being trapped, bound, or isolated courtesy of such mobile, easy-to- use, communication-extending enhancements. 
The point about mobility is probably crucial. Wearable computing and ubiquitous computing are each, in different but complementary ways, geared to freeing the user from the desktop or laptop. They are geared to matching the technology to a mobile, socially interactive, physically engaged human life form. The development of new and richer interfaces goes hand in hand with this. The ubiquitous devices will be more self-sufficient—more likely to monitor us than to receive deliberate commands and inputs. We will still need to communicate data and requests at times, and here the use of a variety of physical embodiment-exploiting interfaces will be crucial. The violinist Yo-Yo Ma’s communications with his instrument via the bow are, we saw in chapter 2, amazingly rich and nuanced. One day soon we will see expert web-surfers and designers able to manipulate data streams and virtual objects with all the skill and subtlety of a Yo-Yo Ma. Almost cer- tainly, they will not be using a keyboard and mouse to do so. 
Where some fear disembodiment and social isolation, I anticipate mul- tiple embodiment and social complexity. An individual may identify himself as a member of a wide variety of social groups, and may (in part courtesy of the new technologies of telepresence and telerobotics) explore in each of those contexts, a variety of forms of embodiment, contact, and sexuality. The feeling of disembodiment arises only when we are digitally immersed but lack the full spectrum of rich, real-time feedback that body and world provide. As feedback links become richer and more varied, our experience will rather become one of multiple ways of being embodied; akin, perhaps, to the way a skilled athlete feels when she exchanges tennis racket for wetsuit and flippers. In these new worlds, Katherine Hayles notes, it is “not a ques- tion of leaving the body behind but rather of extending embodied aware- ness in highly specific local and material ways that would be impossible without electronic prostheses.”39 
In a strange way, we may even come to better appreciate the value and significance of our normal bodily presence by exploring such alternatives. Not disembodiment, then, so much as a deeper understanding of why the body matters and of the space of possible bodies and perspectives. Not isolation so much as a wider and less geocentric kind of community. Not handcuffed to a desktop device in a dusty corner, but walking and running out in the real world. Not mediated via the narrow and distressing bottle- necks of keyboard and screen, but richly coupled via new interfaces that make the most of our biological senses and native bodily skills. 
But let’s not fool ourselves. The problems all too briefly scouted above are real and pressing, and the solutions I have gestured at are at best partial and often visibly inadequate. Still, there is no turning back. The drive to- ward biotechnological merger is deep within us—it is the direct expression of much of what is most characteristic of the human species. The task is to merge gracefully, to merge in ways that are virtuous, that bring us closer to one another, make us more tolerant, enhance understanding, celebrate embodiment, and encourage mutual respect. If we are to succeed in this important task, we must first understand ourselves and our complex rela- 
tions with the technologies that surround us. We must recognize that, in a very deep sense, we were always hybrid beings, joint products of our biologi- cal nature and multilayered linguistic, cultural, and technological webs. Only then can we confront, without fear or prejudice, the specific demons in our cyborg closets. Only then can we actively structure the kinds of world, tech- nology, and culture that will build the kinds of people we choose to be. 









Recursivity and Contingency 
Yuk Hui 
The current work is primarily a treatise on cybernetics. It aims to understand the evolution of systems in general, and the emergence of technical systems in particular, by interrogating the concept of the organic, a concept that marks a rupture with the dominating mechani- cal worldview of early modernity. We will attempt to study the genesis of systems according to two guiding concepts: recursivity and contin- gency. Both conceptually and materially, these concepts lead to the emergence and constant improvement of technical systems. This read- ing of technical systems will be accompanied by an evolving concept of nature, one that traces a trajectory from a Romantic first nature to a second nature in which nature is considered as “standing-reserve” in the sense of Heidegger, and probably also a third nature in which nature is understood neither as Romantic nature nor as standing-reserve, but is rather inscribed in the concept I call cosmotechnics.1 
With recursivity and contingency we hope to sketch a historical- critical exposition regarding the theorization of the organic in philoso- phy, which gives rise to two major lines of twentieth-century thought: organicism (ecology and cybernetics) and organology. We attempt to demonstrate that it is necessary to take up again the concept of the organic and to shed new light on the condition of philosophizing in view of the “becoming organic” of digital machines on a planetary scale, or what Pierre Teilhard de Chardin called the “omega point” or “final point of Noospheric reflection.”2 This new condition will oblige us to reflect anew on the tension between system and freedom taken up by the philosophers of German idealism. While most of them largely neglected the Industrial Revolution, as we know, this leads to the economical materialist critique of Karl Marx. We will reflect on the increasing determination of technical systems realized in the new wave of industrialization, fueled by artificial intelligence, machine learning, and all sorts of surveillance technologies endowed with a transhuman- ist ideology that wants to overcome the limit of the human and politics. In order to unfold this hypothesis, we will have to go through a long journey or a theatre of reason. 
§1. ADVENTURE OF REASON 
First of all, I would like to put forward a central claim: namely, that since the publication of Kant’s third Critique in 1790, the concept of the organic has been the new condition of philosophizing. It is a reopen- ing of philosophy after the epoch of mechanism and is later developed in other directions: vitalism, organicism, systems theory, cybernetics, and organology, among others. Research in natural science, especially among the naturalists, has introduced the organic to philosophy both as a new metaphysical object3 and as the antidote to the mechanical view of life. The mechanistic reduction associated with the metaphor of the clock has lost its lure, and the blurred distinction between animal and machine in René Descartes’s mechanism is interrogated under a bright light, ending with an expression of amazement: How could something like an animal body even be possible?4 
What is the meaning of “the organic”? Instead of following conven- tional discourses in biology, this book proposes to analyze it accord- ing to the two key concepts of recursivity and contingency. This is done in order to firmly comprehend the history and dynamic of the notion. The organic mode of thinking also opens up the question of the generatio aequivoca5—that is, the gradual growth from inorganic to organic, from preformed to self-organized, and from heteronomy to autonomy. As Kant has famously claimed in the Critique of Judg- ment, an “organized product of nature is one in which everything is a purpose and reciprocally also a means.”6 We may even be able to say that the organic serves as the model of a system of metaphysics,7 but also as a resolution to the antinomy between mechanical laws and freedom that Kant proposed in the Critique of Pure Reason. It progressed from idealists such as Johann Gottlieb Fichte, F. W. J. von Schelling, and G. W. F. Hegel to later thinkers such as Alfred North Whitehead, and on to the “organicist movement” and cybernetics (which we refer to as a mechanical organicism), which underlies a theory of individuation qua ontogenesis. 
Retrospectively we may say that Kant’s notion of teleological judg- ment gives rise to four interpretations. The first two are included in what historian Timothy Lenoir calls a teleo-mechanist program.8 The third and fourth correspond to what we want to claim as an organol- ogy: first, organic mechanism, in which the organism is considered as a nonlinear algorithm producing complexity beyond the grasp of the understanding; second, suspicious vitalism, which is due to Kant’s use of the concept of Bildungstrieb (formation drive) from biology; third, organicism, which emphasizes the whole (community) and the exchange between the parts (reciprocity); fourth, organology, which concerns the intimate relationship between biology and technology. These interpretations are nevertheless interlinked. Recursivity is not mere mechanical repetition; it is characterized by the looping move- ment of returning to itself in order to determine itself, while every movement is open to contingency, which in turn determines its singu- larity. We can imagine a spiral form, in its every circular movement, which determines its becoming partially from the past circular move- ments, which still extend their effects as ideas and impressions. This image corresponds to the soul. What is called the soul is the capacity of coming back to itself in order to know itself and determine itself. Every time it departs from itself, it actualizes its own reflection in traces, which we call memory. It is this extra in the form of difference that witnesses the movement of time, while at the same time modify- ing the being that is itself time, so that it consequently constitutes the dynamic of the whole. Every difference is a differing, deferring in time and a being differed in space, a new creation. Every reflective movement leaves a trace like a road mark; every trace presents a ques- tioning, to which the answer can be addressed only by the movement in its totality. This questioning is a test, in the sense that it may fall or it may continue with intensification, like the movement of a curve. What determines the falling or intensifying is the contingent encounter between internal and external ends. 
Recursion is both structural and operational, through which the opposition between being and becoming is sublated. Sublation preserves the oppositional theses (thesis and antithesis) and it also elevates them to comprise a third (synthesis). Being is preserved as a dynamic struc- ture whose operation is open to the incoming of contingency: namely, becoming. The opposition between the Eleatic rationality of being and the Ionian physiology of becoming is resolved by this living form, which implies at the same time movement and identity. It is in the Timaeus that Plato first resolves the opposition by constructing the soul in the form of a circle. The soul constantly comes back to itself as a necessity of its being. Aristotle was not able to comprehend the ques- tion of recursivity, this being the reason he criticized Plato, maintaining that since we have pauses in thinking, the soul therefore cannot be a circular movement.9 Aristotle didn’t see that the soul is both structure and operation. 
The opposition between the body and the mind, as well as the concepts of evolution and development in biology, also involve a failure to understand structure and operation, since they all attempt to substantialize. The opposition can be resolved in two ways: first, via a monism that conceives the mind and the body as different functions of a single substance, as we find in Baruch Spinoza; second, via the notion that the body and the mind cannot be separated since they con- stitute a recursivity that can only be either complete or nothing—this recursivity is often referred to as reentry or self-reference in contem- porary biology and systems theory. Gottfried Leibniz’s monadology is a composition of both methods: a monism of monads and a recur- sivity of mirrors. The recursive movement comprises one of the main characteristics of Leibniz’s concept of the organism: “[E]ach portion of matter may be conceived as a garden full of plants, and as a pond full of fish. But each branch of a plant, each limb of an animal, each drop of its humours, is also such a garden or such a pond.”10 Yet in fact this recursive structure is not necessarily limited to organic beings. Leibniz’s notion of individual substance implies such a possibility of movement in all beings, which, as we know, was rejected by the Cartesians precisely because it implies the omnipresence of the soul in nonorganic beings, such as a stone. The Cartesians cannot accept this, since in their eyes the individual substance is a confusion of two irre- ducible substances, res extensa and res cogitans. Leibniz’s monad, the simple substance, takes up the complete notion of individual substance he proposed in Discours de Metaphysique by giving a new apparatus to the substance: the mirror. The mirror reflects; it reflects into itself what is outside of it according to its point of view. Why didn’t Leib- niz grant a window to the monad, and instead endow it with a mirror? A window may allow connection like extension, but a mirror allows recursion—it reflects what other mirrors reflect. 
Every reflection is from a point of view, but such a partiality embeds a totality; it is not the fragment of the universe but the universe seen from a point of view,11 like the finite inscribing the infinite in baroque art,12 which Gilles Deleuze characterizes as “the fold.” However, can an infinite be reached at all? If it is reachable, can one still call it infinite? We can say that it is a becoming infinite (or, like Kant says, “as if”): unity in multiplicity, being in becoming, constancy in change.13 This becoming infinite constitutes the internal dynamics of the monad’s world. Every being, like the sound of a wave, is composed of infini- tesimal parts: les petites perceptions. Recursion presents a form in which the infinite is inscribed in the finite; such an infinite is always an approximation, since in the world of the infinite there is no longer difference in quantity but only in quality. The monad is therefore self- sufficient and complete, since what can one demand further when one already possesses the infinite? 
If such a recursivity characterizes being in becoming—difference in terms of structure and identity in terms of operation—a question arises: How shall we account for the differences among individu- als? Why do the sand grains on the beach look different from each other? Why are all twins, in spite of their resemblances, singular on their own? The singularity of every being is constituted by the play of recursivity and contingency. What is contingent can sometimes be information, which primordially means giving form. Information triggers the process of individuation, like an unexpected event that circumscribes other probable events. It is an ontogenesis in the sense that movement is no longer the threat of annihilating being into non- being, but rather that which conditions its own becoming. Knowing this formation or morphogenesis means that the mind sets itself into the same movement, because if the mind is able to set itself into such movement, there is an identity between mind and nature. The opposition between mind and nature, ideal and real, seems resolved in this respect. The equivalence between mind and nature is a slogan attributed to all idealists and their disciples, as well as to those who write books with titles such as Mind and Nature. Where does recursion begin? The search for the beginning is a search for the first cause. While in a circular loop, the beginning is only tem- poral, but not necessarily a cause.14 The cause is the totality of the loop. The prime mover or unmoved mover does not intervene from without, but rather the cause is immanent. It is probably in this way that we can interpret what Spinoza says about Deus sive Natura. Or, on the subject of knowing, it is the Ich denke that is itself the loop that is analogical to practical reason and aesthetic judgment. Therefore, the fact is that there is no beginning, just as there is no ground, since every original ground (Urgrund) is a groundless ground (Ungrund) or abyss (Abgr- und), and every beginning is the end of another beginning. The prime mover doesn’t do manual winding-up, as one does with mechanical clocks. This image of the prime mover is the fantasy of the epoch of mechanism, because mechanism presupposes a linear causality. Tracing the first cause will therefore necessarily lead to the figure of the divine: The beginning already includes the end. However, the nuance that we have to register here is that, in the mechanical view, God ceases to be both efficient and final cause, as it is interpreted after Aristotle; rather, he is only an efficient cause.15 
In Giordano Bruno’s philosophy of nature,16 as well as Spinozism or the doctrine attributed to it (pantheism), God is immanent, since it is no longer conceived of as a deity who is somewhere outside of the earth, but rather as the internal principle of motion. Faith is no longer only in transcendence but also in immanence. Recall the teaching of Zarathustra to the villagers after he had descended from the mountain: “[R]emain faithful to the earth and do not believe those who speak to you of extra-terrestrial hopes! They are mixers of poisons whether they know it or not.”17 The announcement of the death of God is also the announcement of the death of the extraterrestrial prime mover and an intention to look for the immanence of the will as power. Recursivity is a notion of immanence. Recursivity defends a nonmechanical existence by emphasizing the formal and the final cause, the telos. It defends against the theory of design while at the same time affirming the final cause as the cause of all causes, which constitutes a natural theology since the end remains ungraspable but effective. This is also the depart- ing point of Kant’s notion of teleological judgment. 
Kant, in the Critique of Judgment, provides a new condition for phi- losophizing after rationalism and empiricism: the reflective judgment based on the concept of the organism. Retrospectively we may want to think that the aesthetic judgment presented in the first book on the beautiful and the sublime is analogic to the model of teleological judg- ment in the second book. In the first book Kant presents a heuristic approach that is not determined by any a priori rules but rather arrives at the end by dynamically constituting its own rules. The end, should it be beautiful or sublime, driven by the imagination, is determined by a purposiveness devoid of purpose. The negations that define the beauti- ful, such as purposiveness without purpose and pleasure without inter- est, are indefinite in the sense that it is not immediately given and may never be reached through empirical induction. The aesthetic judgment is analogous to the teleological judgment in the sense that the natural end is an ideal of self-organization that conditions the process without pre-given rules, meaning that it is open to contingency.18 In the aesthetic judgment the mode of negative affirmation is to grant the a priori to the natural end, which serves another end: the moral. In the organism we find a new set of relations (part-whole) and new form of movement (recursion), which not only surpasses mechanism but also reveals the “hidden plan” of nature, a finality that is the “ideal.” 
Under this condition, thinking and acting have to become organic, since simple mechanical relations of cause and effect are no longer sufficient to serve as the ground of explanation, either in science or in philosophy. Every act is recursive in the sense that it constantly refers to itself and evaluates itself. There is only determinative judgment in mechanism, since determinative judgment only informs linear causal- ity. We know that there is in fact no judgment in this activity: in other words, it cannot be called a judgment; it is only a command. An act presupposes a reflective judgment, which is beyond the mere act since it involves a judgment of value; it is rather an approximation of the whole through reflection. The categorical imperative is ethical because it is recursive in order to arrive at the “universal,” which we call the moral end. 
But what is this immanent force sustaining such a recursivity? To know the natural end is to know the end of human spirit. This can only be done through an identification between Geist and Natur. It is in the sys- temic thinking of Fichte, Schelling, and Hegel that we find elaborations of the recursion form (see Figure I.1). The ich is the point of departure in which every confrontation with the nicht-ich, which Fichte calls a check (Anstoße), forces the ich to return to itself, and it is in this recursive and repetitive movement that reality is revealed as such. The movement between the ich and the nicht-ich is the fundamental principle (Grund- satz) of the philosophical system. 
In Schelling we see a similar form, but Schelling doesn’t take the opposition between nature and mind as the condition of such an identification. Rather, he takes the mind and nature as two analogical structures and operations, as in his famous claims that mind is invisible nature and nature invisible mind.19 This analogy reposes on a circular- ity, like the ancient world soul described by Plato. This structure is the circular movement that represents the infinite productivity of nature and spirit. It encounters obstacles such as contingent events, for example, inhibition (Hemmung), whereby this productive force is actualized into products, from natura naturans to natura naturata. The oppositions between nature and spirit, subject and object are obliterated since nature becomes subject—an identity between nature and mind is established. In Hegel, we find another logic of reflection characterized by a double negation (known as dialectics), in which spirit recognizes nature as the other of the self in order to absorb it into the whole. 
Contingency in the mechanical model is a rupture in a set of large- scale industrial machineries. It can be catastrophic, since it interrupts the system, while the latter does not know how to respond due to a lack of sophisticated feedback mechanisms. Contingency has to be rearticu- lated as possibility so that the mechanism will be able to handle hard- coded rules that anticipate contingencies as probability: the possibility of an incoming signal or an input is limited to several options, which are given meanings and values. If one goes to a concert, the chance of being killed by a falling stone on the street is one of the possibilities that is rarely taken into consideration. It can happen, of course, or it may not happen. Contingency is something that is beyond the obvious possibili- ties, but it is also a possibility: indeed, a strange possibility, since it is a threat to the necessity of laws, including the law of chance in the sense of Henri Poincaré.20 In the recursive model, contingency is expected as necessity since without it there is no exteriority and no external finality. Here the finality is no longer one that is assured by mechanisms like interferences of linear causal propositions, but rather attempts to arrive at such an end by recursively turning back to itself to determine itself. The form it determines is accomplished by combatting contingency, not to eliminate it but rather to integrate it as necessity. 
Romanticism endows nonorganic beings with this recursivity, since it is the nature or general organism (Allgemeiner Organismus)—accord- ing to Schelling (as well as Plato and Plotinus before him, and James Hutton and James Lovelock later)—in which we dwell. Ernst Bloch has rightly noticed that, with the Romantics, every stone is a living being.21 It is a living being not only because it has spirit, but also because it possesses infinite force: It is the infinite force inscribed in the finitude of individual beings. Imagine that a stone is not a static being, but rather that its appearance as a stone is the result of a recursive process toward the infinite. The recursive movement resides not only in nature but also in history. History in the Hegelian sense is the realization of the Absolute through dialectical movements. In this case, contingency is necessary, not only because contingency is omnipresent in nature but also because it is a test that reason has to pass—that is to say, to overcome and to progress. In Hegel we find the death of nature (Tod des Näturlichen) as fate, the sacrifice of the affirmation of logic, and it is this attempt to overcome contingency via contingency that we arrive at the question of systemization. 
§3. CONTINGENCY AND FINALITY 
The English word accident has a double meaning that we can also find in Aristotle’s Categories, where he distinguishes substance (??π??????????) from accidents or contingent attributes (???????????): accidents, for example, color, being the nonessential in the sense that their arrival or disappearance will not affect the identity of the subject.22 For example, this apple is an apple, regardless of the fact that it was green last week and is red this week. In the Metaphysics, therefore, Aristotle says that “accident is close to non-being.”23 The accidents are also the sensibles, for example, color, quality, quantity, and so on. We will keep in mind this double sense of the accident, as both ines- sential attribute and as accident, and come back to the conflation of these senses frequently in what follows. By distinguishing accidents as predicates that are subject to change from substance that remains identical to itself, Aristotle resolves the dispute between being and becoming. 
The modal logic in Kant’s Critique of Pure Reason deals with pos- sibility, existence, and necessity; what is necessary is universal or that which guarantees the universal. As Kant says, “[T]hat whose coher- ence with the actual is determined according to universal conditions of experience is necessary.”24 If we understand contingency as a mere category of modality, it is opposed to necessity and is only one of the many possibilities. It is our intention to enlarge the concept of contin- gency beyond modal logic. 
In March 2014, Malaysia Airlines Flight 370 from Kuala Lumpur to Beijing disappeared; 239 people completely vanished. As of the time of writing, nothing has yet been confirmed regarding what caused its disappearance. It is a contingent event: It isn’t expected, but it happens. It can happen, though it may not happen. In every event there are theo- retically indefinite possibilities. Even when I walk out of my house this morning, as is my routine, I will go to the épicerie next to my house for a coffee before starting writing, or I may see a friend who owes me money from a long time ago and suggest he return his debt. Aristotle calls the first case automaton, often rendered automatic; the second case tyche, meaning chance or luck. There are, in theory, infinite possibilities when an event is actualized, but there is a metastability that is promised by highly probable events, for example, going to the café or meeting a neighbor. However, an event like the disappearance of the Malaysian airplane was not expected and does not belong to the set of highly probable events. Had they known in advance, no passenger would have taken the flight. 
If we understand contingency as the merely probable then it is pos- sible to understand it from the point of view of probability and statistics. For example, for Pierre-Simon de Laplace determination is opposed to randomness and implies predictability. We know that this opposition is false from various fields, including mathematics (Poincaré), phys- ics (Werner Heisenberg), and biology (Jacques Monod). However, we want to suggest that contingency is not only the merely probable, but rather that which exists in all movements, its meaning and functionality being relative to the nature of such movements. What is a movement? If I say that one moves from one place to another, it means that an event takes place in time and space. I make such a movement in order to grasp, say, a chair. It means that there is a finality in such a movement, and the finality is the ultimate cause: nihil est sine ratione. Contingency exists in all movements and it has different meanings for different types of movement. We can list three of them: 
1. Linear mechanical movement with predefined finality linearly chained to its causalities (A?B?C?D), for which contingency means error, since contingency is not expected: for example, when the gear of a mechanical engine is worn out, one of the linear cau- salities will not take place, D cannot be produced; 
2. Nonlinear movement with predefined finality (A?B?A’?B’?C): for example, a recursive algorithm that exists in every Turing machine. The operation will have to stop at one point since otherwise it will exhaust all resources; a recursive function is limited in this sense. Whether the program stops or not is determined by a check determining whether or not the goal is reached; contingency is demanded in order to improve the performance of the system (for example, the introduction of noise into the operation, as in machine learning). 
3. Nonlinear movement with auto-finality (A?B?C?A), mean- ing that a goal is not predefined. It changes direction according to contingent events, for example the evolution of an organism. Auto- finality means precisely that the result is not yet completely defined; even the finality itself is situational. It will have the same fate as (2), but its exhaustion is due not to error but rather to the limits of negentropy. 
One tends to relate contingency to modal logic instead of seeing 
it as a functional necessity. So long as we don’t take it to designate mere possibility, it is no longer an abstract concept, but concrete and functional. Hegel understands this well, since first of all dialectics is a nonlinear movement, and in order to advance toward the Absolute, contingency is necessary to affirm freedom and to avoid becoming merely formal (formal in contrast to content [Inhalt]). Contingency becomes necessity regarding the system. Contingency stands out as a concept fundamental to rationality and creativity, something that can clearly be seen in Iannis Xenakis’s stochastic music, about which he writes: 
Since antiquity the concepts of chance (tyche), disorder (ataxia), and dis-organization were considered as the opposite and negation of reason (logos), order (taxis), and organization (systasis). It is only recently that knowledge has been able to penetrate chance and has discovered how to separate its degrees—in other words to rationalize it progressively, without, however, succeeding in a definitive and total explanation of the problem of “pure chance.”25 
Contingency will not yield a system unless this contingency becomes necessary; as Xenakis claims, “pure chance and pure determinism are only two facets of one entity.”26 What is such movement, which is contingent and necessary at the same time, or, in other words, it is while it is not? We may want to follow Augustin Cournot and con- sider it as objective contingency, for it is caused by “the combination or the encounter of events which belong to independent series.”27 Cournot’s concept of contingency is not a violation of causality but rather a particular case of causality, in which two things come together like the technique of collage used by the surrealists whereby two objects or two distant realities are brought together in order to provoke something new or unexpected.28 Recursion is the movement that tirelessly integrates contingency into its own functioning to real- ize its telos. In so doing it generates an impenetrable complexity in the course of time. Organisms exhibit a complexity of relations between parts and whole inside the body and with its environment (e.g., structural coupling) in its functioning. Life also exhibits such complexity, since it expects the unexpected, and in every encounter it attempts to turn the unexpected into an event that can contribute to its singularity. Failures come when the recursive form cannot generate its consistency. It is then only a pure becoming, meaning that it is neither actual nor potential, like the natura naturans without natura naturata, meaning the not-being. 
§4. BEYOND MECHANISM AND VITALISM 
We want to show that the continuation of this organic condition of philosophizing gave us organicism and organology in the twentieth century.29 We will see the emergence of a biological organicism and its double, a mechanical organism, also known as cybernetics. One of the aims of this book is to put the two into conversation and reveal the intimacy and tension between them. First of all, organicism is what stands between mechanism and vitalism. The historical drama between Wilhelm Roux and his student Hans Driesch is a classic example dem- onstrating the opposition between mechanism and vitalism. Roux took two developing cells from frog eggs and killed one with a hot needle. As expected, the remaining cell continued to develop but formed only half a tadpole. In 1891 Driesch repeated the experiment with urchin cells but teased the two cells apart rather than killing one of them. This time he observed that each of the cells grew into a full sea urchin. In contrast to Roux’s mechanism, according to which a half will only develop into a half, Driesch found that a half can also develop into a whole: There is a vital force in the organism that cannot be exhausted by mechani- cal explanation. This contingent event—contingent in the sense that Driesch wanted to prove that Roux was right, and didn’t expect such a result—characterizes the opposition between Roux’s Entwicklungs- mechanismus and Driesch’s vitalism. The first has been discredited as reductionism and the second attacked as having relied on a mysterious force such as élan vital (Henri Bergson) or entelechy (Driesch). In the first half of the twentieth century, between the vitalists and the mecha- nists, there emerged a third group. Since they saw the organism as a form of organization other than either mechanical laws or a mysterious vital force, they are called organicists. This view is expressed by the philosopher Joseph Woodger when he claims that “cell is not a name for a thing but for a type of organisation,”30 a type of organization to be understood in terms of embryology and biological chemistry. Both the organicists and the cyberneticians believe that the opposition between mechanism and vitalism has already been overcome.31 
The organicists and cyberneticians both embrace the general systems theory of Ludwig von Bertalanffy. The cyberneticians take another approach to overcoming this opposition. They are fascinated by Nor- bert Wiener’s concept of feedback, George Spencer-Brown’s concept of reentry, and Heinz von Foester’s concept of recursion, since these introduce a new epistemology as well as a new machinic operation. In his 1948 Cybernetics, Wiener challenged the mechanism versus vital- ism opposition, represented by Newtonian mechanical-reversible time versus Bergsonian biological-irreversible time, since for him cybernetic machines understood as feedback systems have already overcome such an opposition (along with Maxwell-Boltzmann-Gibbs statistical mechanics, which resolves the discrepancy between macro- and micro- states in physics). In this book we understand feedback, together with self-reference, as another name for recursion. Wiener’s new cognitive scheme is highly valued by Gilbert Simondon as a new epistemology, replacing that of Cartesian mechanism. We find a similar evaluation in Hans Jonas, who also sees cybernetics as “an overcoming of the dual- ism which classical materials had left in possession by default: for the first time since Aristotelianism we would have a unified doctrine. . . . for the representation of reality.”32 Unlike Jonas, however, who sees the poverty of the cybernetic teleology, Simondon suggests that the feedback system possesses “an active adaptation to a spontaneous finality.”33 
If this spontaneous finality is possible, it is because the machine is able to respond to contingency and thus contingency acquires meaning in these operations. For an amplifier that amplifies an incoming signal, regardless of whether it is noise or a melody, contingency doesn’t have meaning. When the amplifier is constructed in the way that noise will be filtered, noise here stands for contingency and, against meaning, acquires a negative meaning. When a machine learning algorithm is added to the amplifier it is possible that this algorithm will make good use of noise; in other words, contingency acquires a positive meaning, which we may want to follow Deleuze and the Stoics in calling a quasi cause.34 
Cybernetic machines employ a new concept that allows the operation to be evaluated and controlled. This concept is information. Information for Wiener is a measure of the level of organization. Organization here means the capacity to recursively integrate contingencies. We therefore see in this sense that there is not really a contradiction between the notion of information as understood by Wiener, on the one hand, and that of Claude Shannon on the other. Note that for Shannon informa- tion means surprise: An incoming signal has more information if it is less expected. At first glance this seems opposite to Wiener’s notion of information, which measures the degree of organization. Information is a new category that the ancients didn’t anticipate. As Wiener says, information is neither matter nor energy. Indeed, information, matter, and energy become the fundamental elements of a new theory of indi- viduation. We may want to raise a question: Is it possible to identify information with one of the four Aristotelian causes: formal, material, efficient, and final? If yes, how can we articulate it? For sure, informa- tion contributes to the efficient and final causes, and it may be derived from the formal and material causes, but it cannot be reduced to any one of them. This is also the great difference between an information machine and a thermodynamic machine, since an information machine allows a finer order of magnitude and precision of control than energy. 
Simondon took up the concept of information and turned it into a much broader concept, beyond statistics. Simondon’s innovation is that he sees the importance of the new category, which cannot be reduced to either matter or energy. Simondon’s approach to information is to turn it into a more general concept, which means signification. When an incoming signal produces signification to the system, it carries informa- tion. In this sense, Simondon’s concept of information is closer to that of Gregory Bateson, for whom information is “the difference which makes a difference.”35 Bateson is a thinker of recursion; the later Bate- son speaks of a “recursive epistemology” or “ecological epistemology.” Information, as “difference which makes a difference,” is operational and self-referential. Information is that which triggers and facilitates an operation in a system, which Simondon calls individuation. Not all information, and not information alone, will lead to individuation, since information is only one of the conditions, along with matter and energy. However, it also means that energy and matter alone cannot account for individuation. We can push it further by saying that, without informa- tion, a contemporary theory of individuation is impossible. For sure, theories of individuation were proposed according to the dominant epis- temology of their epochs; for example, the key concept would be force in the time of Isaac Newton and Leibniz, and later also in Kant and Schelling. It is the new epistemological condition that offers a different way of looking at individuation. Information discovered by cybernetics is instrumental to Simondon’s thinking of individuation. Signification in Simondon’s theory of individuation is submitted to contingency: Given a system, signal A may produce a different effect than signal B; the former may be regarded by the system as noise, and therefore ignored, but the latter may have meaning to the system according to its structure of meaning. 
§5. THE GREAT COMPLETION 
Heidegger famously proclaimed that cybernetics is the end of meta- physics—or, as we would like to call this event, the great completion. This completion has to be understood as the concretization of systems. Heidegger’s statement needs to be qualified, and it is one of the tasks of this book to do so—but also to reopen it as a question. Heidegger not only saw that the main feature of cybernetics is to turn every procedure into a calculable and steering (Steuerung) process; he also understood it as the victory (Sieg) of a new method over science. This method is the conceptualization of feedback: “[T]he reciprocal (hin- und herlaufende) regulation of processes in their interrelation thus takes place in a circu- lar motion. . . . the basic feature of the cybernetic world is the control loop.”36 Feedback is used not only to comprehend being as such—see- ing being as looping processes—but also to grasp being as a whole—a whole in the sense of the Greek to panta. This trajectory from being as such to being as a whole in cybernetics also characterizes the pas- sage from first-order cybernetics to second-order cybernetics, of which Niklas Luhmann’s systems theory is one of the highest achievements. 
Luhmann famously claimed that “the system emerges et si no dare- tur Deus (even if God doesn’t exist),” and this system emerges from the base of what he and Talcott Parsons call “double contingency,” the Grundsatz of the social system. The concept of double contingency can be simply demonstrated in the interaction between two individuals who have no prior knowledge of one another. Since the ego has no suf- ficient knowledge of the other’s action, and the other may not anticipate the ego’s action, there is thus a situation of double contingency. The double contingency is posed as a problem to be resolved. For Parsons it is resolved by establishing consensus, or sharing the same symbolic system, while for Luhmann the double contingency is the drive of the emergence of the social system. Since every action is contingency reducing, and it is by constantly attempting to reduce contingency that social orders and norms are established, in this sense Luhmann is able to claim that “every error is productive.”37 
The system absorbs contingency by turning it into something prob- able—that is to say, that which is expected. This absorption of contin- gency is a process of systematization in which contingency becomes something probable through which the system distances itself from mere mechanism. Jacques Ellul, in his 1977 Le système technicien, has added the notion of the technical system to what Simondon outlined as the lineage of technical objects from element to individual to ensemble. Ellul sharply pointed out that the capability of computers to manipulate large amounts of data was the key to the realization of the technical sys- tem, which is becoming more and more totalizing. The technical system is formed through a seemingly contradictory movement: specializa- tion and totalization. On the one hand, technologies become more and more specialized; on the other hand, there is a totalizing tendency that is effectuating. The specialization and diversification of technologies blinds humankind from seeing the unification and totalization of tech- nology: namely, the technical system. He writes: 
But man has not yet grown aware of this relation between his striving for unity and the constitution of technology as a unitary system. He does not yet know, does not yet see, that this system exists as a system.38 
Instead of Luhmann, Ellul choses Edgar Morin as his targeted systems theorist, and considers the latter’s 1973 book Le paradigme perdu: La nature humaine39 to be one of the most dangerous books ever written, since Morin “forges the theory of the de facto technological totalization.”40 This does not necessarily mean that Morin admires the totalization of the technical system, but rather that, in trying to explain the historical progress, he (consciously or unconsciously) unfolds the mechanism of such totalization. A totalized system is one that has its own rules of growth and its own rhythms of development, which Ellul calls “self-argumentation.” As Ellul has emphasized, this does not mean that humankind cannot intervene in the system, but rather that he is “caught in a milieu and in a process, which causes all his activi- ties, even those apparently having no voluntary direction, to contribute to technological growth, whether or not he thinks about it, whether or not he wishes it.”41 The technical system has become a superorganism in the sense of Teilhard de Chardin’s noosphere,42 or technosphere in the sense of the geologist Peter Haff.43 Retrospectively, Jean-François Lyotard was right when he said that modernity should be understood not as an epoch but rather as the capacity to tolerate contingency,44 which is called resilience today. This assertion may become more evi- dent when we think of notions such as systems theory, which Lyotard used to characterize postmodern society. 
Neither mechanism nor vitalism will be the best solution to under- standing the coevolution between human and machine. The organicists stand out as a new paradigm for thinking about life and system. Among the organicists we would like to highlight Joseph Needham, who started out as a mechanist (author of the book Man a Machine),45 and later turned into an organicist, becoming a core member of the Theoretical Biology Club along with Joseph Woodger, Conrad Waddington, and others. More dramatically still, he also became one of the most impor- tant sinologists, having documented the history of Chinese science and technology and, more importantly, framing Chinese philosophy as an organicist philosophy; and lastly, he was president of the Teilhard Cen- ter for the Future of Man (London), a center dedicated to the work of Teilhard de Chardin. It is surprising that, in his turn from an organicist to a historian of Chinese science and technology, Needham did not develop it into an organology, but rather seems to affirm an organicist thinking in Chinese technology. Allow me to quote a significant obser- vation from the second volume of his Science and Civilisation in China: 
Here it is not possible to do more than mention the great movement of our time towards a rectification of the mechanical Newtonian uni- verse by a better understanding of the meaning of natural organisation. 
Philosophically the greatest representative of this trend is undoubtedly Whitehead, but in its various ways, with varying acceptability of state- ment, it runs through all modem investigations in the methodology and the world picture of the natural sciences—the numerous and remarkable developments of field physics, the biological formulations which have put an end to the sterile strife between mechanism and vitalism while avoiding the obscurantism of the earlier “Ganzheit” schools, the Gestalt psychology of Kohler; then on the philosophical level the emergent evo- lutionism of Lloyd Morgan and S. Alexander, the holism of Smuts, the realism of Sellars, and last but by no means least the dialectical material- ism (with its levels of organization) of Engels, Marx and their successors. Now if this thread is traced backwards, it leads through Hegel, Lotze, Schelling and Herder to Leibniz (as Whitehead constantly recognized), and then it seems to disappear. But is that not perhaps in part because Leibniz had studied the doctrines of the Neo-Confucian school of Chu Hsi, as they were transmitted to him through the Jesuit translations and dispatches? And would it not be worth examining whether something of that originality which enabled him to make contributions radically new to European thought was Chinese in inspiration?46 
This history of the philosophy of organism that Needham hinted at and the figures that he named here will be partially pursued in this book. Needham’s affirmation of the presence of an organicist thought in Chi- nese science and technology seems to suggest an intimacy between Chi- nese thought and cybernetics. Needham’s historical analysis of Chinese science and technology has been instrumental for my own develop- ment of the concept of cosmotechnics, though I still have reservations regarding the perfect history and comparison recounted by Needham. Leibniz’s organicism will be dealt with in chapter 2, with regard to Wiener’s Leibnizianism, but Needham’s claim that Chinese thought is a philosophy of the organism47 and that Leibniz took this organicism from neo-Confucian thinking is a delicate issue, one that remains to be examined with more careful historical studies.48 The unbridgeable difference between Leibniz and neo-Confucian thought is that Leibniz developed his monadology from a logical-mathematical point of view, while the neo-Confucians developed it from the perspective of a moral cosmology centered on the notions of heaven (tian), energy (ch’i), heart (xin) and reason (li).49 The main task here is to address the ambivalent relation between organicist thinking and modern technology. For if we follow Needham and others in saying that the organicism of Whitehead and neo-Confucianism is the antidote to mechanism, and cybernetics is that which surpasses mechanism in the West, will cybernetics be the superior war machine and what will be its appropriation? The dilemma is precisely the following one: Either one returns to a biological organi- cism or one accepts the mechanical organicism that is cybernetics. The former is fragile because as long as we are not in a state of pure nature (and if it exists at all), it is hard to hold this harmony without directly confronting contemporary technologies and transforming them: We have seen that the Chinese “philosophy of organicism” surrendered to modern technology after the two Opium Wars and now returns in the guise of the political concept called tian xia (under the heaven), which more or less resembles Kant’s organicist cosmopolitics. The latter means that we will have to adapt ourselves to a more and more sophisticated technical system, which is moving toward an undesirable situation—namely, the increasing technological determination and constant arrival of algorithmic catastrophes. It is to the sharp eyes of Heidegger that the “organic” is another name of the mechanistic- technological “triumph” of modernity over nature. Therefore the idea is no longer simply to restore an organicism (though its importance must not be ignored), whether it be that of Chinese philosophy or that of Needham, Bertalanffy, and Whitehead, but rather, as I want to suggest in this book, the necessity of developing future technological thoughts through the reading of the philosophy of nature, organicism, cybernet- ics, organology, and cosmotechnics. 
§6. THE CONFLICT OF ORGANS 
Just as Ernst Cassirer has remarked that Kant, in his three Critiques, gave only a very peripheral role to technology, it is only in cybernetics and organology that Kant could be seen as a guiding spirit.50 Organi- cism is a recursive thinking. Organology signifies a shift from one recursive form (in nature) to another. It is a synthetic thought that not only integrates but also searches for a new epistemology that creates a new loop. Georges Canguilhem was the first, as far as we know, to propose the term general organology in order to rethink the rela- tion between organism and machine, a rethinking that does not take the machine as the equivalent of the human but rather thinks of the human-machine as an organic whole. This was inspired by many think- ers, especially the holism of Kurt Goldstein, the exteriorization theory of André Leroi-Gourhan, and the Hegelian philosopher Ernst Kapp. Leroi-Gourhan has claimed that technology is both exteriorization of memory and liberation of organs; the technical object is also what Kapp calls a “projection of organs,” in the sense that tools are fundamentally shaped according to organs such that the former extend or even replace the functions of the latter. In his 1947 lecture “Machine and Organism,” Canguilhem praised Bergson’s Creative Evolution as the precursor to a general organology, since evolution is primarily a creativity of the élan vital. It may also seem ironic that we come back to the vitalists (Berg- son and Canguilhem), an identity that was ridiculed by the organicists, but significance sometimes hides itself in ironies. 
Canguilhem didn’t elaborate much further on his general organol- ogy. Instead, we can find traces in Simondon’s general allagmatic (or universal cybernetics), as well as a much more detailed treatment in the work of Bernard Stiegler, who took the term organology from musicol- ogy. It is fair to say that it is only in Stiegler’s work that we can see a more schematized picture of a general organology consisting of psycho- somatic organs, social organs (e.g., institutions), and all kinds of tech- nical organs. Those three systems of organs are intimately intertwined and evolve on the basis of changes in the technical organs, but Stiegler has a rather strict understanding of organology as a study of organs, and in doing so, he also bypassed the question of organicism. Stiegler started using the term general organology in 2003 and continues to develop into what he now calls exorganism.51 However, in this book we will not address the later thought of Stiegler, since it is still a promising project under development. In addition to the division between germ plasm and soma by August Weismann,52 and that between genotype and phenotype, Stiegler sees a third type of heredity, one that is nei- ther somatic nor genetic but rather technical. This remains a powerful deconstruction of biology and it only seems apparent today in light of the coming program of human enhancement and genetic engineering. 
The inorganic in the form of technology is also a form of heredity, which, more than genotypes and phenotypes, is subject to mutation. Technology is what is passed to us as culture, which consists of a means of living, but it is not handed to us as an eternal being; rather, it mutates all the time with accelerating speed. Technology joins imme- diately with the environment in the theory of evolution. The process of evolution is a dialectical movement between adaptation and adoption, since mere adaptation to the environment refuses the question of the will and mere adoption idolizes the will. I would like to suggest further that organology is not only a systematic study of the human-machine relation but should also study how culture and technics interact: that is, how different cultures—for example, Chinese, Indian, European, Amazonian, and so on—are able to produce new thinking that inte- grates modern technology into their traditions and also transforms those traditions in order to reopen technodiversity, which is now dominated by the transhumanist imagination of the technological singularity. If in the nineteenth and twentieth centuries—an epoch characterized by a perpetual tension between mechanism and organicism/vitalism—we witnessed non-Western cultures surrendering to war machines equipped with mechanical technologies, in the twenty-first century we have to push ourselves to ask if cybernetics could be appropriated in order to open up the question of technodiversity. 
With the organicist/organological thought that he calls mechanology, Simondon wants first to resolve the problem of alienation caused by the misunderstood human-machine relation, and second to overcome the antagonism between culture and technology. When Simondon talked about culture and technics, it is possible that he was referring only to Western societies. However, we must take it further and try to understand what is at stake in other cultures in the process of globaliza- tion. Organology is not simply about adopting a machine by giving it a new purpose, since organology is also a theory, which is grounded in the studies of the evolution of machineries. Simondon’s mechanol- ogy aims to reorganize the technical ensembles in order to resolve the problem of alienation through internal resonance (which is one of the terms that Simondon used to translate feedback). The human being has lost the status of a technical individual, since in the age of artisans the latter are able to make themselves an associated milieu for their tools. But in the epoch of industrialization, the automated machines own their associated milieu, workers being rendered redundant for such purposes. The organicity promised by feedback seems to Simondon a possibility to resolve the problem of alienation by reorganizing the relation between h Maybe one can say that, in comparison to Simondon’s concept of contingency, Stiegler is more Nietzschean, in the sense that contin- gency is not only that which motivates a system but also that which transforms the system. It is the concept of a structural transformation qua elevation that characterizes Stiegler’s reading of Simondon’s quan- tum leap as the threshold of individuation. In Stiegler, this contingency has to become necessity, not in that it is naturalized as part of the system, but rather in that it has to become a pivot through which the individual and the collective are transformed. Organicism and organol- ogy, developed from the concept of organic, are constantly challenged by technological development. What we are witnessing today is a shift from the organized inorganic to the organizing inorganic, meaning that machines are no longer simply tools or instruments but rather gigantic organisms in which we live. In the time of Schelling, and later in Hutton and Lovelock, nature is considered as a gigantic organism, of which we are a part. However, this term general organism, which was attributed to nature considered as a source of contingency, seems more appropri- ately to designate the technological system that we now inhabiting—for example, smart homes, smart cities, and the Anthropocene. Instead, we are observing the becoming of an “artificial earth,” and we are living within a gigantic cybernetic system in the process of forming: This comprises our contemporary condition of philosophizing. As we will explain in chapter 1, Gaia must first of all be seen as the meeting point of James Lovelock’s cybernetics and Lynn Margulis’s organicism. 
Cybernetics is a methodology for understanding the operation of being and society, but such understanding of society is realized in machinic and material terms.53 The problem with the idea of going back to nature is not that it was wrong but rather that it failed to see the irre- versibility of the historical trajectory, something that Marshall McLuhan had already observed in the 1970s when he said that the end of nature is the birth of ecology. Ecology is very much based on the concept of the organism (the relation between organism and environment accord- ing to Ernst Haeckel), which, as we will try to demonstrate, constitutes umans and machines—a thesis we will look closely in the later chapters. 
a new condition of philosophizing since Kant. But this organic nature, which was considered by Kant to be the guarantee of perpetual peace, in that the universal history of humankind is understood as the realization of nature’s hidden plan, is directly challenged when ecology replaces such a concept of nature by taking up its organizational structure. If a new cosmopolitics is possible after Kant, it will have to start with a deconstruction of such a concept of nature, in order to see more clearly whether this “hidden plan” is still realizable, or whether, on the contrary, a new program should be introduced. What would be tragic would be to misunderstand ecology as a “return to nature,” since the return to a Romantic and innocent nature is only an illusion, which is also why Bruno Latour wants to invent Gaia as a political concept.54 In modern urban areas we experience the contingency of nature, but more and more we experience the delay of trains and buses, traffic jams and industrial accidents. Contingency takes another form, since we are not only dealing with meteorology, trying to predict the chance of rain or snow, but also trying to avoid two Tesla autopilot cars from crashing with each other on the highway. With ideas such as big data analysis, machine learning, and smartification, we will see that urbanism will be fully automatized, and that such an automatization is aiming to be ecological and sustainable, which implies the realization of gigantic recursive cybernetic machines. Just as Wiener claims to have discov- ered a general principle of control and communication in machines and animals, the cybernetic machines are acquiring a kind of organicity, thus becoming the organizing inorganic. This is the reason I wanted to add to this genealogy of the organic the concept of cosmotechnics, as developed in my earlier work The Question Concerning Technology in China: An Essay in Cosmotechnics. Cosmotechnics means primarily the unification of moral and cosmic order through technical activities; this unification is a reattachment of the figure to the ground, but it is not a return to metaphysics of the one and the all. We follow Simondon and adopt the figure and ground metaphor from Gestalt psychology. Simondon attempts to understand the history of technology as constant bifurcation from a magic phase where there is no separation between subject and object, and ground and figure coexist in harmony: The figure is the figure of the ground, and the ground the ground of the figure. The constant bifurcations—first the bifurcation into technics and religion, then each bifurcates into theoretical and practical parts—lead to the continuous divergence between figure and ground. It demands a philosophical thinking (in view of the failure of aesthetic thinking) to ceaselessly converge the figure to the ground. The genesis of technicity also proposes a multiplicity of cosmotechnics with different relations to different grounds. In other words, we have multiple cosmotechnics in different cultures, instead of a Greek techn?? or a modern technol- ogy analyzed by Martin Heidegger in his famous 1949 lecture “The Question Concerning Technology.” In order to reopen the question of technology in the twenty-first century, it is necessary to reconstruct the genesis of technicities in different cultures, which have their own cos- mic specificity.55 Simondon speaks often of technical reality; it seems that he makes allusion to another term, human reality. Along with being there (être là), human reality (réalité humaine) was used to translate what Heidegger calls Dasein into the French language. Like the human Dasein, which needs an articulation of “being there,” modern technol- ogy should also be resituated and reappropriated in its genesis. 
One may wonder: Are we not here again opposing human beings and machines, culture and technology? This is what an organology attempts to avoid, since we reject any substantialization of human and culture. It means that humanity is not a consistent and everlasting substance, but rather accidental. We are playing with the distinction between sub- stance and accidents in classical philosophy, and when we say accident, it also carries the meaning of contingency. The concept of the human is a contingent historical concept. We are already posthuman when we subscribe to the view that the human is a technical existence. If the concept of posthumanism is a theoretical attempt to bring forward an ethics against anthropocentrism, it will fail if it does not take the trajectory that we are going to outline—that is to say, the study of the human-machine relation—into account, since otherwise it will remain a burlesque attempt to produce “high-level ontologies.” Organology could be considered a materialist science, but it is not a materialism that opposes spirit and matter. Rather it seeks, at every opportunity, to allow spirit to exercise its freedom without producing the alienation of the soul.56 Technology is a product of the spirit. Naive materialists fail to understand this, and see spirit as the product of technology—which is unfortunately the case in our time. 
§7. AFTER ECOLOGY, BEFORE SOLAR CATASTROPHE 
Is it possible to give a new meaning to the concepts of chance and the automatic that Aristotle defined in his Physics in the epoch of the artificial earth, which is moving toward higher and higher degrees of automation? One will inevitably associate the word automaton with automation, because what is meant by automaton here is probabilities constrained by the capacity of the being—for example, a marching horse will stop before the abyss, a tossing coin will show either heads or tails. With the advancement of machinic automation and the applica- tion of statistical mechanics, we are confronting a singular situation in which automata are eliminating contingency on one level—the level of calculative reason (like replacing Paul the octopus with a machine learning algorithm to predict the World Cup champions)—and elevat- ing it to another level—the level of speculative reason. Calculative reason limits the possibilities to multiple choices, like a checklist from which one is forced to choose between some definite options (for exam- ple, nationality, gender, etc.). Speculative reason is reactionary while confronting calculative reason, since it will have to go beyond the latter. In this sense Heidegger’s speculation on Being is the most outstanding example, in which Being takes different names, such as the last god or the Unknown. But what does it mean by “beyond” here? 
Speculative reason wants to go beyond the sensible, beyond the immediately given data, to a realm in which the appearance is recognized as only one of many possibilities as well as necessities. But speculative reason must also be restricted in order not to fall prey to the Schwämerei. It must, on the contrary, be guided by the phenomenon in order to go beyond the phenomenon. Science can become philosophy only when it moves beyond the phenomenon and ceases to reduce everything to empirical evidence, but rather elevates thinking to a new terrain, for example, a speculative physics that Schelling proposed. However, speculative reason will finally fall prey to a bad infinite if it is no longer able to integrate calculative reason as part of itself, as when one opposes luck to automaton, since it fails to see, as Aristotle has already seen, that all chances/luck are automata—though retrospec- tively we see that Aristotle is talking about nature here instead of tech- nology. It is necessary to resituate tyche and automaton in the epoch of cybernetics, and to see such a possibility of luck not as a mere subset of possibilities, but also to produce such possibility of luck by adopting the automaton in its own thinking. 
Concerning technological acceleration and the ends of the human, maybe we should say that what remains is neither posthuman nor trans- human, but rather the inhuman. The inhuman is a term proposed by Lyotard in his thought experiment regarding the explosion of the sun, expected to take place in about 4.5 billion years.57 The female inter- rogator (along with a male philosopher) in this experiment questions if the research in science and technology is not a preparation for the survival plan after the solar catastrophe, since these research programs tend to look for an absolute separation between thinking and the organic body, so that thinking can survive when all organic lives are destroyed after the solar explosion. The inhuman is first of all a negation, but a negation of two different facts. First of all, it is the negation of what is considered to be human, for example, the unity of the organic body and the soul. In this sense it is a separation between the mind and the body, the thinking substance and the bodily substance, an ultimate scientific project of humankind in anticipation of the solar catastrophe. Lyotard identifies this first sense of the inhuman with system. He writes, “[T]he inhumanity of the system which is currently being consolidated under the name of development (among others) must not be confused with the infinitely secret one of which the soul is hostage.”58 What does it mean that the soul is the hostage of “the infinitely secret one”? What is this infinitely secret inhuman? It is the Unknown, the improbable, as Lyotard says when commenting on Saint Augustine, that the inhuman is “more interior in myself than me.” For Augustine, it is God, but after the death of God, it is the Unknown (das Unbekannte), as we can see in Heidegger, or the improbable, as Stiegler claims, following Maurice Blanchot and Yves Bonnefoy. It is something that cannot be reduced to calculability, to statistics, and to preemptive algorithms. The incalcu- lable is the preindividual reality with which the soul is able to elevate, to unfold itself, that is to say, to exercise its freedom. But what exactly is this inhuman of which the soul is hostage? And what does it mean that the soul is hostage? 
Rationalists, whether they be old or new, didn’t see the point of the Unknown or the Unknowable, though they could have rationalized the Unknowable by giving it meaning—that is to say, giving sense to the existence of the inhuman without falling prey to a twenty-first-century nihilism.59 We are not forcing the irrational or the Unknown as the ground that immediately corrupts all the discourses developed from it; rather, the nonrational is the limit of the rational.60 Retrospectively, the aim of rationalism is to construct a system of reason, such as that which Leibniz achieved. But existence has never been a rationalist project. Rather, like symbolism, rationalism is only one of the organons of thinking. Rationalism attempts desperately to retain a monotheism after it has murdered God, which takes the name of system. Evolutionary biologists and eco-modernists see the Anthropocene as a gigantic sys- tem and think that the way to solve the problem of ecological mutation is to modulate such a system, but presupposing a unified system doesn’t really provide us with any weapon other than adapting ourselves to the system. How to resolve the tension between the passivity of adaptation and the activity of freedom? Human freedom is the possibility of both good and evil. As Schelling attempted to show, evil emerges when the figure is taking over the ground (again like Figure-Ground in Gestalt psychology), when the self-will takes over the universal will; seek- ing a solution in the self-will is an affirmation of the perversion of the ground, the perpetual loss of the universal will. Criticizing the system doesn’t mean that we should be afraid of the Absolute. On the contrary, it is the Absolute that gives us the force to relativize, and it is through relativization that we affirm the Absolute. 
The red Tesla Roadster that Elon Musk’s SpaceX launched into space in February 2018 is the demonstration of a cosmotechnics in which the cosmos is mere standing-reserve. It is a significant gesture after the photo of the earth taken by Apollo 17 in 1972, after the becoming Gestell of the earth. It is now the turn of Mars, ready for industrial extraction. Nietzsche said in The Will to Power that “[s]ince Copernicus man has been rolling from the center toward X.”61 Now in the Anthropocene, humankind is reelevated from X to the center, a return of anthropocentrism in new dress. The game of Elon Musk is a demonstration of a planetary project in which geopolitics is no longer about the earth but rather the industrial extraction of Mars. In what way can one think about cosmology after this world picture presented by Musk, which is not only abstract but also concrete in terms of technical domination? The only possibility is to give this world picture meanings beyond that of standing-reserve, not only in theory but also in practice. In order to do so, we will have to reframe the modern technological thinking whose essence is enframing (Gestell). This reframing of the enframing will demand first of all the fragmentation of the system in order to give new realities and meanings to technology—or, in the words of Augustin Berque, “to recosmicise (récosmiser) the earth” by going beyond the “acosmic” of the modern.62 Ellul sharply pointed out the trickiness of “giving meaning to technology,” since it repeats the same path of ancient times, when humans gave meaning to natural phenom- ena as the mediation between humans and God, which evolves into a natural theology or a concrete religion.63 Ellul sees the difficulty here of not repeating the same path of religion in which God is replaced by technology. We will have to interpret it in different senses. One is to see technology as the Unknown, since it is something that we haven’t encountered in our previous experience, otherwise it wouldn’t be new and there wouldn’t be progress, so giving meaning to it means to give it rationality. The other is to see technology as the manifest of the Unknown and as the means through which the Unknown is rationalized. This returns us to theology and spiritual life, which is in the process of being abandoned or enjoyed only by the privileged ones who can afford to launch rockets to Mars during the daytime and practice yoga and meditation in the evening. Ellul was, however, right to point out that it easily gives rise to two responses64: one is to resolve the issue with nontechnical means, for example, meditation or LSD; the other is to respond with technical solutions, which, when not carefully reflected upon, will only reinforce the positive feedback of the system, like the eco-modernists, who want to repair the earth damaged by technologies with more advanced technology, including geo-engineering. 
§8. THE FUTURE COSMOLOGISTS 
If contingency is the driving force of systemization, for both techni- cal and social systems, is it possible to have an absolute contingency, a contingency that cannot be absorbed at all and that exceeds any expectation? Some philosophers attempt to resist recursive totality by turning to absolute contingency as an emergency exit, which may lead to freedom and autonomy. Absolute contingency for the moderns gains its transcendental height as did nature for the Romantics, which will surpass all systems and human cognition, a contingency that functions as does the signal of a lighthouse, the savior of disoriented ships. But when we say expectation, didn’t we already presuppose a subject, a subject that thinks and expects? 
Cournot’s objective contingency, which we briefly mentioned above, is pushed much further in what Quentin Meillassoux calls “absolute contingency.” For Meillassoux, absolute contingency is an attempt to break away from the nature and mind correlation, to go behind it to show that it is possible to have a new epistemological foundation. This epistemological foundation does not start from the Absolute as Unbedingt but from the Absolute as contingent. If the correlation in idealism and in phenomenology failed to grasp this contingent ground, it is because these schools insist on the subject-object correlation as the ground of knowledge. 
The subjectivists (Meillassoux chooses to use the word subjectivists instead of idealists) wanted to approach the arche-fact through enforc- ing the power of thought; that is, how thought can penetrate into the realm of the unknown. For Meillassoux, the Absolute has to be posited outside thoughts, outside the reach of the mind, outside all causalities. In contrast to what he calls the “facticity of correlation” of the cor- relationist tradition, Meillassoux wants to propose what he calls the “principle of factuality,” meaning to identify a reality or material that is independent of thought. For example, we cannot say if God exists or not, since he may or may not exist; he may appear in front of you tomor- row morning when you wake up, or you may not see him at all within the finitude of your life. I quote Meillassoux: “We will call ‘contingent’ any entity, thing, or event which I know could be, or could have been, other than it is. I know that this vase could have not existed, or could have existed otherwise—I know that the falling of the vase could have not happened.”65 Distancing from correlationism is a way to open up a new inquiry into the existence of the possible. 
The mission of speculative reason could be understood in terms of Meillassoux’s new treatment of facticity, which proposes “to make fac- ticity no longer the index of a limit of thought—of thought’s incapacity to discover the ultimate reason of things—but the index of thought’s capacity to discover the absolute irreason of all things.”66 Meillassoux wants to produce a new ontology in which one can find a new category or entity called “over-chaos” (surchaos), which he wants to distinguish from chaos theory in mathematics. This over-chaos is “an absolute” that “escapes the desabsolutization of correlationism.” This over-chaos is not purely chaos, meaning without any possibility of deriving order or law. Within an absolute, inconsistent being, there is hardly any con- tingency, as he writes: “[A]n inconsistent—universally contradictory— being is impossible, because this being could no longer be contingent. For the one thing that an inconsistent being cannot do is to change, to become other, since, being contradictory, it already is what it is not.”67 The necessity of contingency is not a proposal for a return to chaos (as in some mistaken impressions of the postmodern), but rather to affirm the absoluteness of contingency. 
But what kind of epistemology will absolute contingency give us? The subject-object correlation can and must be criticized as an anthro- pocentric approach, in which speculation is subordinated to sense cer- tainty: the dead is less valuable than the living. Meillassoux wants to transvalue knowledge, which could be summarized by the following anticorrelationist and antivitalist epistemological question: 
Would there not be more modesty, then, in considering that the Universe has nothing to do with our subjective qualities, that it could very well do without them at any degree whatsoever, and to say, more soberly, that there is no absolute scale that makes our properties superior (because more intense) to those of nonhuman living creatures or inorganic beings?68 
What kind of epistemology can this be other than a mathematical formalism? But doesn’t this reliance on mathematical symbolism itself presuppose a humanism, which Ernst Cassirer calls animal symbo- licum? The escape from the subject-object correlation gives room to speculation, but how does it give us a new epistemology? The con- cept of absolute contingency definitely merits our attention, since it is that which sets limit to a system, meaning its incapacity to reduce the contingent event into one mere probability of statistics. Absolute contingency is an antisystemic concept, and it is so because once the system fails to grasp it, another foundation of the system will have to be constituted, which means another system. For example, according to the rule of Edme Mariotte, considering a container of gas, the product of the pressure and its volume is a constant (PV = C), but this constant is derived only from experience and hence can be contingent. In De la contingence des lois de la nature, Émile Boutroux tried to show that any necessity is always open to something outside it, or even demands something outside for its law to be completed, while, for a technical system, a certain case of contingency is always already presumed and understood as necessary.69 Like Leibniz’s best-of-all-worlds hypoth- esis—the world created by God as the technical system created by its designer—it is implemented in the way that it anticipates contingency, meaning that they are only relative contingencies. By admitting that all laws of nature are contingent, as Boutroux proposed, it only shows that an absolute ground is impossible. We still have no new way to found a new epistemology, since all truths become relative. 
Herein lies the positive use of the concept of absolute contingency in Meillassoux, since it means precisely that a single system cannot hold. A superintelligence may not be illusory, but it will be the end of plural- ism. Herein lies the spirit of cybernetics: After every dead-end there is a new epistemology, a new ground; even if this ground is groundless, don’t be afraid of it. Absolute contingency affirms the plurality of sys- tems, including systems that are not comprehensible by human beings. The black box is such an example, since a black box, if it merits the name, remains impenetrable to human cognitive power. Black box is a term used to describe algorithms completely opaque to their users. This is a quantitative challenge to the finitude of knowledge, since imagination is not able to capture (or zusammenfassen, as Kant says) the magnitude of data and therefore can only be surprised. The positive use of absolute contingency is that it affirms the necessity to fragment the system and therefore sets limit to any single all-encompassing sys- tem, as Kurt Gödel has done in mathematics. The negative, as we have seen, is that there is no absolute ground for any epistemology. This fragmentation is not a repetition of postmodern discourse, since the post- modern remains a discourse on the universal; this fragmentation is a return to locality in order to rethink technological development. But this return is not a longing for a nostalgic concept of nature, what we at the begin- ning called a Romantic nature, because nature as such may have ceased to exist since the Industrial Revolution. Nor is it a continuation of the standing-reserve that was condemned by Heidegger. Another standpoint is necessary. I was tempted to call this higher standpoint culture, but I think it is not justified, since it is easy to fall prey to cultural essential- ism or ethnocentrism. It is rather what I call a cosmotechnical thinking, which situates technology in its genesis and attaches it to its ground, which is the cosmic reality. This journey through the organism, organi- cism, and organology will be the departing point for the reconceptual- ization of cosmotechnics to come. In the affirmation of plurality lies precisely the question of freedom, since to be free is to have the capacity to differ and to defer both the figure and the ground, to allow the bifur- cations of future after centuries of modernization qua synchronization. 
*** 
This book is divided into five chapters. Chapter 1, “Nature and Recur- sivity,” attempts to read Kant’s reflective judgment as a precursor to recursivity and to show how this reflection is naturalized in the thought of Schelling’s philosophy of nature, which I argue prefigures twentieth-century organicism in biology (for example, Bertalanffy, Needham, Woodger, and Donna Haraway) as well as in the concept of Gaia (Lovelock and Margulis). Chapter 2, “Logic and Contingency,” further investigates how the concept of reflection is mechanized in Hegel’s reflective logic and realized in cybernetics, as Gotthard Gün- ther famously claimed, which we also call a mechanical organicism. We will look into two major concepts of cybernetics: feedback and information, through Wiener, Gödel, Alan Turing, Bateson, Simondon, and Heinz von Foester, and how these concepts embody recursivity and contingency (especially Gödel’s general recursive function). Chapter 3, “Organized Inorganic,” attempts to move away from an organicism to an organology, which is presented by Canguilhem in his 1947 article titled “Machine and Organism.” In comparison with organicism, organ- ology attempts to understand the relation between life and technology by refusing mechanism. In organology, science and technology is understood as a medium to return to life. This chapter reconstructs the concept of general organology in Bergson’s and Canguilhem’s thought. Chapter 4, “Organizing Inorganic,” looks further into the organology of Simondon and Stiegler and the role of recursivity and contingency in the theory of individuation. It suggests that the “organizing inorganic” is surpassing the “organized inorganic” and therefore enforces a new initiative to rethink organicism and organology. Finally, chapter 5, “The Inhuman That Remains,” takes up this challenge and suggests that after the end of Enlightenment humanity it is necessary to develop an inhu- manity (in the sense of Lyotard) as a response to the end of philosophy as well as an opening of a true pluralism, or a multiple cosmotechnics. 
Nature and Recursivity 
Nature loves to hide. 
—Heraclitus, Fragment 123 
Contingency is always contingent upon something, as long as this some- thing is considered probable or even necessary in time—for example, laws of nature. Not all laws of nature are in themselves necessary, though they are only laws insofar as they are considered to be so. Laws of nature remain necessary until they are disproved by exceptions. In this case, they become contingent, meaning things can be otherwise. It ceases to be a law and is now a fact. It is this particular relation between nature and contingency that we would like to elaborate as a point of departure for reflecting upon the realization of systemic thinking and, finally, technical systems. We are going to examine two fundamental points: 
1. Contingency is fundamental to the understanding of nature, not least because nature demonstrates an irregularity deviating from rules that are derived from empirical observations. In order to develop a philosophy of nature, it is necessary to recognize such contingency as a necessity. 
2. Any systemic philosophy, either ideal or real, will have to address nature external to the mind (the “I”), and in consequence is obliged to deal with the question of contingency,1 since contingency challenges 
3. the very foundation of such systems: If the foundation of a system is contingent then all knowledge might be suspended and deprived of its validity. Systemic philosophy will have to render contingency necessary, not only factually but also logically. 
4. These two motivations are key for reflecting upon eighteenth-century Naturphilosophie and its successors. (As we will argue in this chapter, its twentieth-century successors are organicism and the Gaia theory.) If philosophy wants to become a system, it will have to develop a mechanism allowing it to resolve the threat posed by contingency. If the a priori laws become contingent, then the system will collapse imme- diately. The system would therefore better respond to contingency by not having predefined rules, and instead allow rules to emerge during its confrontation with contingency and irregularity. We pass here from a transcendental characterized by rules to a transcendental character- ized by teleology, analogous to the movement from Kant’s first to his third Critique. At the center of this systemic thinking is the concept of the organic, which comes from discoveries in the natural sciences, especially biology. Being organic is not merely maintaining part-whole relations, but also designates self-organization and autopoiesis, which we want to call recursivity. And if we want to address the question of technical systems, it is necessary to examine the history of the concept of nature, which is always the other of itself in the Hegelian sense. It is only through a close examination of the concept of nature that we can see clearly the question of technology, since the two have been opposed throughout the history of philosophy. In other words, without understanding the relation between nature and system, we will not understand technical systems: as Heidegger says, “technics: history of nature” (Technik: Historie der Natur).2 
5. §9. KANT AND THE MODEL OF SYSTEM 
6. I would like to refer to a very intriguing quote from Schelling’s late philosophy here in order to open up the question of contingency, not only because Schelling will be guiding us throughout this chapter, but 
7. also because it in a certain sense reverses our conventional concept of 
8. necessity: 
9. The first impression (and this is decisive not only in life, but also in knowledge) of this thing, on the whole and in the particulars so highly contingent, that we call the world—this cannot possibly be the impres- sion of something that has arisen out of rational necessity, that is, through a mere logical emanation. The world resembles nothing less than it resembles a product of pure reason. It contains a preponderant mass of unreason, such that one could almost say that the rational is merely the accidents.3 
10. This seems to be a conclusion that Schelling has given to his early career of systemizing nature, an attempt made between 1794 and 1833 and continuing for a period of almost forty years. Schelling’s verdict is astonishing, not only in his rejection of rational necessity as the ground but also in his consideration of the rational as merely “the accidents.” This contingency is not only related to the particular, to the very instan- tiation, but rather concerns the whole, the system. We may conceive the system that Schelling is referring to here as a system regulated by the laws of nature. Schelling’s critique is very radical, probably even more so than that of Boutroux, author of the classic On the Contingency of the Laws of Nature (1874). Boutroux argued in this work that contingency is omnipresent, and that each law of nature always contains contingen- cies that can be logically deduced. It is also different from what we know today as Gödel’s incompleteness theorem, since what Schelling is claiming is that contingency is probably the ground, the “substan- tial,” while the rational is nothing but its accident, and remains one of its expressions. In other words, if contingency is the original ground (Urgrund), it is also a nonground (Ungrund), or an abyss (Abgrund). 
One may want to ask, doesn’t this conceptualization in the late Schelling contradict the usual impression of the regularity of the con- cept of system—a philosophical credo of eighteenth-century philoso- phy? The task of creating a system, or taking philosophy as a system, can be seen as an effort to revive metaphysics after the domination of science and the French Revolution. Schelling stands out as one of the most systemic thinkers—probably even more systemic than Hegel— especially in his last publication, Treatise on the Essence of Human Freedom, often referred as the Freiheitschrift. In this essay Schelling famously declares that the system is not able to get rid of evil; on the contrary, evil is always present in the system as the possibility of freedom. It is sufficient to see that contingency, which can be evil or a state of exception, is immanent in the system. It is of our interest here to carry out a historical-critical exposition of Schelling’s Naturphiloso- phie, since it is an effort to eliminate oppositions (real-ideal, subject- object, contingency-necessity) through the construction of a generic system, which we can call, following Schelling, the general organism (Allgemeiner Organismus). 
It is within such a conception that the system as an organic being is postulated, and from there we would like to understand it as a precur- sor of cybernetics. When it is seen in this way, nature is dissolved in cybernetics: the end of nature. It is such an end in the sense that an innocent, Romanticist, productive nature ceases to be; it is succeeded by cybernetics, as what happened to philosophy in general according to Heidegger.4 However, what exactly is a system, and in what sense can subject and object (nature) be reconciled? 
Before Schelling, other philosophers, notably Kant, had already attempted to answer this question. In his three Critiques, Kant laid down two fundamental methods of systematization. In the first Cri- tique he proposed his famous architectonics to analyze the relation between nature and subject. Nature appears to the subject as phenome- non, the transcendental faculties regulating the apperception of it. The transcendental deduction of categories of the understanding defines the limit of the understanding as well as the limits of the appearance of phenomenon according to the four groups of categories: namely, quality, quantity, relation, and modality. The model presented in the first Critique is constitutive, in the sense that nature must be submit- ted to concepts legitimated by the transcendental deduction. Kant’s strategy can be understood in two points: On the one hand, Kant wants to avoid the phantasm of speculative reason, the well-known Schwämerei, hence reason is confined to the unification of rules of the understanding under principles5; second, Kant was obliged to develop a new mechanism or heuristics capable of addressing the Humean challenge on the “contingency of necessity.” A second model is men- tioned in the appendix titled “The Amphiboly of Concepts of Reflec- tion” in the first Critique,6 in the Groundwork for the Metaphysics of Morals,7 and more precisely again in the Critique of Judgement, in which reflective judgement is elaborated. The reflectivity here is regu- lative instead of constitutive, since it is no longer about the submission of nature to the mind according to concepts, but rather a heuristic (as Lyotard describes it) in search of an un-predefined end and its pur- posiveness (Zweckmässigkeit).8 In Kant’s own words, determinative judgment is the imposition of the universal on the particulars, whereas reflective judgment is the search for the universal in the particulars. Simondon was very sharp to point out that in the first two Critiques, criticism was not able to think cybernetics since, like Auguste Comte’s positivism, Kant’s criticism still tends to think in terms of structure. It is only in the Critique of Judgment that Kant was able to address the question of cybernetics.9 
The second book of the Critique of Judgment is dedicated to teleological judgment, in which Kant presents an organic model. Kant’s writing on teleological judgment had a profound impact on the natural scientists of his time,10 as well as on the next genera- tion of philosophers such as Fichte, Novalis, the Schlegel brothers, Schelling, and Hegel (among others). The clearest definition of the organic form can be found in §64, where Kant defines the organic being as follows: “a thing exists as a natural end if it is (though in a double sense) both cause and effect of itself.”11 Kant then provides the example of a tree, highlighting three elements that define it as an organic being. Firstly, the tree reproduces itself according to its genus, meaning that it reproduces another tree; secondly, the tree pro- duces itself as an individual, absorbing energy from the environment and turning them into nutrients to sustain its life; thirdly, different parts of the tree establish reciprocal relations and thus constitute the whole—as Kant writes, the “preservation of one part is reciprocally dependent on the preservation of the other parts.”12 The concept of the organic being consists in the reciprocal relations between parts and the whole and the capacity of reproduction. It also affirms the two important categories of relation: namely, community (Gemeinschaft) and reciprocity (Wechselwirkung).13 In other words, they constitute a primitive form of self-organization. As Kant writes: “[N]ature, on the contrary, organizes itself, and does so in each species of its organized products—following a single pattern, certainly, as to general features, but nevertheless admitting deviations calculated to secure self-preser- vation under particular circumstances.”14 
§10. THE ORGANIC CONDITION OF PHILOSOPHY 
The organic constitutes a new condition of philosophizing, for the reason that the organism provides an exit for philosophy, enabling it to move out of the systemic determination by a priori laws, which surrender freedom to mechanical laws and fatalism. We would like to emphasize again that we are not talking about a philosophy of the organism but rather arguing that the organic imposes on philosophy a new condition and new method of thinking. The heuristic of the reflec- tive judgment that Kant elaborated in the Critique of Judgment is the model based on which the final cause (Endursache) is interpreted. The natural end is something that cannot be observed objectively. We can see such and such a tree or such and such an animal, but we cannot grasp nature as a whole through mechanical rules. Reason can only understand the natural end through reflective judgment, meaning that it recursively arrives at a self-organizing being. Teleological thinking is in this sense circular: A?B?C?A.15 
The figure of the organism gives Kant the means to resolve several problems. First, it provides the inspiration to imagine a system that is not based on mechanical laws. Mechanical laws are not sufficient to explain contingency and the teleology of nature; this comprises one of the major arguments in the antinomy of judgment.16 Secondly, organism provides the framework or foundation through which natural scientists should consider their object of study, without referring to merely mechanical explanation. Third, it allows Kant to systematically refuse mechanism, hylozoism (living matter), Spinozism (pantheism), and theism.17 Fourth, it is at the core of Kant’s political philosophy, since nature is “the great artist . . . the eventual ‘guarantee of perpetual peace.’”18 Nature is not 
something that can be judged from a particular point of view, just as the French Revolution cannot be judged according to its actors. Rather, nature can be comprehended only as a complex whole, and the human species, as one part of it, will ultimately progress toward a system (or republican constitution) that approximates the natural end, to that “cos- mopolitan whole, i.e., a system of all states that are in danger of acting injuriously upon one another.”19 
In §72 of the Critique of Judgement, Kant refused two approaches to explaining purposiveness of nature—namely, the idealist and the realist approaches. The idealist approach implies a lack of design (Ansicht), which Kant illustrates with the accidentalism of Epicurus and Dem- ocritus as well as the fatalism of Spinoza. Kant criticized Spinoza’s metaphysics as a “fatalism of purposiveness” precisely because, in such an undesigned system, although the world is derived from the original being, it ignores its intelligence and sees it as emergence out of a mere “necessity of nature” of the original being, which leads to Kant’s objec- tion that Spinoza’s system eliminates all contingency.20 This is because deus sive natura can be interpreted as implying that God is reduced to the substance of nature, thereby losing its transcendence: a dead god. The realism of purposiveness, on the other hand, assumes a life of mat- ter as the result of design—namely, hylozoism—which implies that “life being either inherent in it or else bestowed upon it by an inner animating principle or world soul.”21 We will see later how Schelling took up Spinoza’s concept of nature and Plato’s world soul—namely, hylozoism—by integrating it into the organic.
We may want to consider that this development in the study of the organism, which later took the name biology, has provided a new condi- tion for philosophizing, of which Naturphilosophie is part. Philosophy of nature is not one that is independent from other disciplines such as moral and political philosophy; in the third Critique this relation is clear, though sometimes seems to be only symbolic. For example, Kant wrote in the famous §49, “Beauty as symbol of the moral,” that one can always contest that the beautiful is not merely symbolic, but rather analogic in terms of operation since the organic, here taking the form of reflective judgment, shares the same mode of operation as practi- cal reason. Therefore, Kant wrote, “in the latter (reflective judgment) connexion . . . must on the contrary be confined to the service of just the same practical faculty of reason in analogy with we considered the cause of the purpose in question.”23 The new causality, meaning that the organism is at the same time cause and effect of itself, is distinguished from the efficient cause (means to the end). With self-causation in mind, Kant opens the question, which resonates with what is called complexity theory today. In §71, “Introduction to the solution of the antinomy of judgment,” Kant states that “we cannot see into the first and inner ground of the infinite multiplicity of the particular rules of nature, which, being only known empirically, are for us contingent, and so we are absolutely incapable of reaching the intrinsic and all-sufficient principle of the pos- sibility of a nature—a principle which lies in the supersensible.”24 It is sufficient, if not impossible, to list all the mechanical causes, however, the world cause (Weltursache) forces us to situate the phenomenon within a broader perspective: the uncanny whole. 
This epistemological limit impels Kant to criticize the narrow-minded scientific explanation, at the same time elevating the epistemological question to a metaphysical one. From the perspective of epistemology, Kant skillfully avoids the question of contingency, since if contingency enters into the list of mechanical causes, then certainty can never be assured. From the perspective of ontology, he effectively absorbs con- tingency, since the contingent is already inscribed and immanent in the movement toward the natural end. This absorption of contingency in production is significant in Schelling’s philosophy, as we will see later. Here we can see an affinity between contingency and freedom. The organism also underlies a possible solution to the gap between nature and freedom, or theoretical reason and practical reason, that has already been expressed as the third antinomy in the Critique of Pure Reason: 
Thesis: The causality according to laws of nature is not the only causality, from which the appearances of the world can thus one and all be derived. In order to explain these appearances, it is necessary to assume also a causality through freedom. 
Antithesis: There is no freedom, but everything in the world occurs solely according to laws of nature.25 
The three Critiques of Kant, together with the Opus Postumum, comprise the effort to overcome this gap in order to integrate the sys- tem of nature and the system of freedom into a single unified system.26 Theoretical reason, according to Kant, is not able to overcome the gap between nature and freedom since it is not able to accommodate free- dom due to the necessity that is already implied in theoretical reason. Only in practical reason is freedom able to self-legitimate as moral laws in order to reach the highest good and happiness. The theoretical reason that is considered as such is a form of determining judgment or mechanism, leaving no room for practical reason and contingency. However, the execution of practical reason is also constrained by the necessity of nature in its realization. The elaboration on the teleological judgment, which was largely influenced by the research in the natural science of his time, seems to serve the purpose. As Paul Guyer writes, “[T]he critique of teleological judgement is to bridge the gap between the realms of nature and freedom precisely by showing us that it is pos- sible to realize within nature the final end the pursuit of which is made necessary by practical reason.”27 
Reflective judgment seems to have overcome the problem of the mechanism of nature and gives reason a nondetermined heuristic (or, as Kant says, according to “laws that are not yet given”)28 to move toward the highest good, as in the case with the categorical imperative.29 Hence now with Kant we have two models, one mechanical and one organic. It is this organic model that resonates with the Idealists and the natural scientists, such as Johann Friedrich Blumenbach (1752–1840), whose concept of the formative drive (Bildungstrieb) was frequently taken up by both Kant and Schelling. Schelling’s task consists in overcoming the opposition between the mechanical and the organic—that is to say, in creating a general system that effectively integrates both models into one, instead of looking for a bridge between two separated realms. One may say that Schelling takes up what Kant criticized as the dogmatic system of hylozoism (a principle of the world soul).30 
§11. RECURSIVITY IN FICHTE’S ICH 
Before we move to Schelling and explore recursivity in the concept of the nature, it is necessary to pass by Fichte, since Fichte pushes much further the reflective act in Kant’s system and hands it down to Schelling’s Naturphilosophie. It is known that Fichte questioned Kant’s Ich denke (I think) as a mere fact (Tatsache) without being able to account for its causality.31 The I think of Kant is still motivated by Cartesian considerations, and Kant’s renunciation of the idea that the human subject is capable of intellectual intuition, whereby it can theoretically comprehend ideas such as freedom, God, and immortality, suggests the limit of the I think. Fichte took up the critique of Karl Leonhard Reinhold and Christian Gottfried Schütz, arguing that the I think should be firstly understood as an act (Tathandlung). This is by no means a simple act that directs from the ich to the empirical world like any ordinary consciousness, but rather an act that is capable of self- positing (selbst setzen) and reflection. In the later passages, we will see how this self-positing is associated with the ancient question of the soul in Schelling’s reading of Plato’s Timaeus. The self-positing of the ich allows Fichte to further develop the “absolute I.” The absolute I is the starting point of knowledge. It is absolute, since it is not conditioned by anything else other than itself. Fichte rejects the thing-in-itself as the inscrutable source of material diversity among representations32 and takes intellectual intuition—an intuition that is not granted to mortals in Kant’s system—as the ground of such self-positing. Such an intuition indicates the I’s nonrepresentational awareness of itself.33 Unbedingte is also related to the “thing” (Ding), meaning that it is not taken as a thing, as Schelling explained: “Conditioning is the act through which something becomes a thing conditioned, that which is made into a thing, which at the same time reveals that nothing can be posited by itself as a thing, i.e. that an unconditional thing is a contradiction.”34 
In the chapter “Antiphysics and Neo-Fichteanism” of Philosophies of Nature after Schelling, Iain Hamilton Grant has clearly shown the mathematical model of Fichteanism as a “recursivity” or “itera- tion.” For Fichte, the absolute I is the only pole of freedom, since it originates from a self-positing instead of being posited by other beings. Fichte ascribes his transcendental philosophy to the realm of “sheer contingency.”35 The mind and nature, the I and the non-I, constitute a duality. The unconditional I has a non-I as negation or as check (Anstoß); what is outside of the unconditional I is only the product of such a negative effect. Hence nature is only the product of the productive imagination.36 This does not mean that nature outside us doesn’t exist, but that nature as both mechanical and organic model can be only the abstraction of the I through the non-I. After the first principle (self-positing) and the second principle (Anstoß), there is also a third principle, which concerns the unification of I as act. The absolute I is the place where thing and consciousness unite, and the duality between the divisible I and the divisible non-I is resolved: the ideal-real, real-ideal.37 
Nature is the object of the Wissenschaftslehre. However, it is only an abstraction of the intelligence. For example, consciousness can grasp a straight line, and abstract its form, and in this sense it pretends to be a system of absolutely universal consciousness, which has no tolerance of a realism beside it.38 Fichte’s system can be conceived as a system of form, like mathematics. Indeed, the two open letters from Kant and Friedrich Heinrich Jacobi have attacked Fichte’s Wissenscahftslehre. Kant, of whom Fichte considered himself a disciple, criticized it as being “merely a logic, thus devoid of content and without reference to reality,” whereas the latter criticized it as mathematics and charged him with “nihilism.”39 Herein also lies the fundamental difference between Schelling and Fichte concerning the definition of the Absolute, as Schelling replied to Fichte that what he wants to call philosophy is “the material proof of idealism.”40 Schelling wants to reconcile the real and the ideal without reducing the real to the ideal as Fichte does, which seems to him an “annihilation of nature,” but rather to found the ideal on the real. This is the core concept of Schelling’s philosophy of nature and philosophy of identity, since nature is not something outside us, neither is it simply something in us like human nature: One must recognize the unity between nature and subject so as to abolish the dualism between subject-object. This distinction is later described by 
HegelinhisTheDifferencebetweenFichte’sandSchelling’sSystemof Philosophy, according to which Fichte aims for a “subjective subject- object” while Schelling is looking for an “objective subject-object,” meaning that for Schelling nature is considered to be independent (selbstständig).41 The Absolute for Schelling is no longer the subjective pole but rather the absolute unity of subject-object, which is constantly in recursive movement. I use the term recursive instead of reflective for a significant reason, since according to Schelling, reflection is also a process of separation: the subject reflects on itself in order to separate the “I think” and the “I being thought.” Recursivity is possible only under the condition of an absolute unity.42 
Fichte is different from Schelling in the sense that for the former the recursive operation happens only in the ich, in which the “self-gen- eration” (Selbsterzeugung) of nature takes place.43 Fichte adopted the concept of the organic in Kant’s third Critique, and the organic model becomes an “antiphysics” par excellence, since the mechanical model is only a mere abstraction of theoretical reason but not the realization of practical reason. The question of recursivity in Fichte has been explored by the French philosopher Pierre Livet in his 1987 article “Intersub- jectivité, réflexivité et recursivité chez Fichte,” in which he defines reflection as “recursivity which loops [boucle] immediately on itself.”44 Livet shows that the recursive process of the I and non-I constitutes a sort of endomorphism, in which the Other is not different from the I. The I in Fichte’s own words is the “mirror that mirrors itself,”45 or an “acting that goes back onto itself” (in sich zurückgehendes Handeln).46 For Grant, the dialectical duality of the I and the non-I, the two pure forms, generates what he calls “abstract materiality.” Grant cites the following passage from Fichte’s Wissenschaftlehre to demonstrate this recursive model: 
Thus the activity returns into itself by way of the reciprocating [des Wechsels], and the reciprocating returns into itself by way of the activity. Everything reproduces itself, and there can be no hiatus therein; from every link one is driven to all the others. The activity of the form deter- mines that of the matter, this is the matter of the reciprocating, and in turn its form; the form of the reciprocating determines the activity of the form and so on. They are all one and the same synthetic state [Zustand]. The act returns to itself by way of a circle. But the whole circle is absolutely posited. It is because it is, and no higher ground can be given for the same. (W I, 170–71; 1982: 158–59)47 
Grant hence concludes that Fichte has produced a “dead nature of inert material particulars,” and becomes “an unrecognized contributor to formalist models in mathematized science.” The question of the recursivity developed from Kant’s Critique of Judgment (which can itself be traced to Leibniz’s mathematics and metaphysics) has great importance here, since it is what allows a system—and by system we return to the definition by Schelling as a “self-contained whole”—to emerge.48 For Schelling, the question is no longer one of granting priority to the I, but rather to nature as the one and the whole, to give freedom to it and derive the Ideal from the real.49 Hence Schelling’s description of the system is even more powerful, and in certain ways anticipates a more complex system of recursivity such as Ilya Prigogine’s dissipative system and Francisco Varela and Humberto Maturana’s autopoiesis.50 
§12. CIRCULARITY IN SOUL AND NATURE 
In a letter to Fichte, Schelling writes, “I speak of object being introduced to consciousness and of consciousness being introduced to object. In this wording, the unity [of the two factors] seems an [external] addition.”51 How is such a recursive model based on the unity of subject and object possible? The question is no longer simply the concept of knowledge but rather the enterprise of philosophy. Philosophy for Schelling is the material proof of transcendentalism. The systematization of philosophy is also a systematization of the way that knowledge is acquired and evolves. In the recursive model, the subject and object have a reciprocal relation, through which a unity is reached after every bifurcation. If I know the world outside of me, it is because I am aware of the fact that I am aware of it. And by coming back to myself, I acquire knowledge of the outside world, like a constant process of negotiation—or more precisely, as Schelling says, an eternal action (ewiges Handeln).52 This is also the ancient concept of the soul as “coming back to the self” (Zu-sich-selbst-Kommen) that we will examine in the following pas- sages and in chapter 3. Bernd-Olaf Küppers concisely summarizes this constant differentiation and unification as follows: 
Only in this way can the Absolute infinitely differentiate itself while preserving its identity. Since the absolute enters into a difference to itself, which dissolves into an indifference at a higher stage of development, there is a development of the absolute from the general to the particular.53 
The conception of nature sees nature and the I as two instances of a generic model, and it is by showing such a generic model that the real and the ideal can be reconciled. In such a view, the object and its determination are never separated in intuition.54 Matter for Schelling is composed of two forces, attraction and repulsion, a concept that he took from Kant’s 1786 Metaphysical Foundations of Natural Science and that was also confirmed by a chemist of his time, Alexander Nicolaus Scherer (1771–1824). In Idea, Schelling quoted Scherer extensively, especially the latter’s conception of the properties of bodies as a result of the activated basic forces of the body. For Schelling, different mat- ters, including light and heat, are only compositions of these forces according to different degrees or proportions, like the cosmogony in Plato’s Timaeus. The reciprocity between the attractive and repulsive forces is best demonstrated in magnetism and Galvanism which begins with polarities.55 
Schelling reads §64 and §65 of Kant’s Critique of Judgment in a similar manner. In these sections, Kant treats the organic form of living beings in terms of reciprocity and community, using the example of the part/whole relation of a tree. In Ideas for a Philosophy of Nature, Schelling refers to this passage in Kant and, more specifically, the idea of absolute individuality in an organism: “Its parts are possible only through the whole, and the whole is possible, not through assem- bling but through interaction.”56 The unity of the parts and the whole is accomplished through an idea instead of through matter. The idea stands as the third that “contains” two potentially opposed entities. Nature can be regarded as a whole—a whole that later takes on the name of the general organism (allgemeiner Organismus) in Von der Weltseele.57 This is a whole that also consists of two opposing concepts: on the one hand, mechanism, which is “a regressive series of causes and effects”; on the other, purposiveness, which is “independent of mecha- nism, simultaneity of causes and effects.”58 These two confront each other as two irreconcilable parts, but by unifying them through the idea, nature emerges in circular form, like the world soul described by Plato: 
If we unite these two extremes (mechanism and purposiveness), the idea arises in us of a purposiveness of the whole; nature becomes a circle which returns into itself, a self-closed system. The series of causes and effects ceases entirely, and there arises a reciprocal connection of means and end; neither could the individual become real without the whole, nor the whole without the individual.59 
Through the third, here taking the form of the idea, which unifies and contains the two extremes, we discover an isomorphism between nature and mind. This isomorphism is revealed in the famous statement in the Ideas: “Nature should be Mind made visible, Mind the invisible Nature.”60 The relation between mind and nature is not constituted by a monism, but rather they share a generic model of individuation. Now the question that has to be posed is the following: Whence comes this organizing force? To posit God as the answer of creation seems far too easy for the young Schelling, who identified himself as a Spinozist in an early letter to Hegel.61 It seems that in Schelling’s early period his affinity to Spinoza and distance from religion allowed for a Naturphil- osophie and later a speculative physics to emerge. 
However, as an Idealist, Schelling also has the difficulty of address- ing the existence of matter prior to the I. Only by synchronizing the gen- esis of matter and the genesis of the mind can such a gap be eliminated. In his 1786 Metaphysical Foundations of Natural Science, Kant claims that all change in matter has external causes; therefore matter “has no absolutely inner determinations and grounds of determination.”62 In the same book, Kant proposes to understand the movement of matter according to two fundamental forces: attraction and repulsion. Like Kant, Schelling sees matter as the composition of attractive and repul- sive forces. However, unlike Kant as well as other mechanists, espe- cially the Swiss physicist Georges-Louis Le Sage (author of Essai de chimie mécanique [1758)], Schelling is skeptical that one can presup- pose matter before force. For example, Le Sage’s atomism sees matter as being made up of divisible particles.63 Schelling’s question was, where does the division end? Presupposing the existence of particles, to Schelling, is only an intuitive way of understanding nature, not the philosophical way.64 Schelling’s counterproposal is very speculative: He refuses the existence of individual particles as the foundation of matter and recognizes it as a genesis of forces. When the two forces cancel each other out such that equilibrium is attained, then there is just dead matter.65 This leads to the question: If active and unbalanced forces are no longer to be found in dead matter, how can one explain the existence of such an object before us? The answer must be: Such dead matter does not, because it cannot, exist in visible nature. Herein also lies Schelling’s critique of Newton’s interpretation of gravitational force: Gravitational force is for Newton only attraction, but Schelling argues, on the contrary, that it is not enough to deploy attraction without repulsion; this is merely “a scientific fiction” that reduces “the phenom- enon as such to laws without thereby intending to explain it.”66 
Nevertheless, the two forces, negative and positive, are also not enough, for Schelling goes on to introduce a third force: gravity (Schwerkraft). Gravity is the force that contains and unifies the two oppositional forces, and brings the ideal into the real: 
If Kant’s expansive and attractive forces (he names “attractive” what we have called “retarding” up to this point) represent nothing other than the original opposition, then he cannot complete the construction of matter from two forces alone. He still requires the third force which fixes the opposition, and which, according to us, is to be sought in the universal striving toward indifference, or in gravity.67 
Gravity is the unifying force, yet it is not simply one of many syn- thesizing forces but the appearance of absolute identity, the indifference between the real and the ideal.68 As a result, we need to bear in mind that this Indifferenz (absence of difference) is not the cancelation of all forces, nor is it the void. Rather, it is the full cohesion of the universal in 
the particular (like sand) or the particular in the universal (like liquid). The conflict of these two forces will be resolved to give an identity, and hence a metastability. What we see in front of us as natural objects are in a metastable state. Any modification in the material, energetic, and informational condition, when strong enough, can trigger further individuation. Schelling did not employ the term metastability but rather cohesion. He also draws an analogy between the Ichheit and the cohesion of the two forces in magnetism: “Cohesion, or, in other words, magnetism, is the impression of self or the I in matter, by which it first emerges as something peculiar to the universal identity and rises into the realm of form.”69 
Such a physical-cum-metaphysical paradigm for force is also to be found in the idea of an organism. The emergence of life cannot be explained merely by chemical operations, though they are what give rise to the inorganic nature that is necessary for organic nature (and, indeed, Schelling admits that chemical operations are the only grasp- able determinate form [bestimmte Form].70 In his discussion of the principle of life, Schelling is responding to Blumenbach’s concept of the Bildungstrieb (driving force for formation), a concept also at stake, of course, in Kant’s third Critique.71 Schelling’s critique of the Bildung- strieb is that it alone cannot act as the primary cause of life. The Bil- dungstrieb, according to Schelling, is “only an expression [Ausdruck] of every original unification of freedom and lawfulness [Gesetzmäßig- keit] in all formations of nature, but not a fundamental explanation [Erklärungsgrund] of this unification itself.”72 
Let us reiterate the above thesis: If Schelling rejects the Bildung- strieb, it is because life demands both forces and a third that is able to retain the contradiction and strive for Indifferenz. There are many more fundamental principles of nature that underlie the process of individu- ation. These principles are not material, but abstract principles that can be postulated in terms of two opposing tendencies, namely, unification as the positive principle and differentiation as the negative principle. 
In Von der Weltseele, Schelling points out that John Brown’s theory of animal excitability (tierische Erregbarkeit) and powers of excitation (erregende Potenzen) correspond to the positive and negative principles of life.73 
How does matter emerge from these forces? How does nature give rise to life? These are core questions that were not resolved in Schelling’s philosophy of nature, since his is ultimately a philosophy of form. The Ideas for a Philosophy of Nature (1797) attempted to give a metaphysical reading of inorganic matters such as light, heart, magnetism, and chemistry; and “On the World Soul” (Von der Welt- seele), published in 1798, proposed a treatise on organic matter. In the following years, Schelling published his First Outline of a System of Philosophy (1800), along with an introduction to his First Out- line, in which he integrates both Ideas and Weltseele to construct his system of nature through what he calls speculative physics. Schelling’s philosophy of nature could be read as an analysis of the potencies to prepare a ground for unfolding the structure of nature. The potencies of nature are investigated according to increasing complexities: A = B, A2, A3, namely, from the inorganic to the organic (dynamic) and the organism. Schelling draws on Carl Friedrich Kilmeyer’s definition of the organic as the repetition of the inorganic at a higher potency, and sees the plant as a preliminary stage of life, and in animals the form of a true organism.74 
The work of the American philosopher Bruce Matthews, Schelling’s Organic Form of Philosophy: Life as the Scheme of Freedom,75 is instru- mental to our reconstruction of the appropriation of the concept of the organic in Schelling’s Naturphilosophie. Matthews traces Schelling’s appropriation of the organic from his commentary on Plato’s Timaeus, traversing his reading of Kant and Fichte, up to his mature writing on freedom. This form, or the original form, is identified in Plato’s Philebus, a form that is the gift of the gods, according to Socrates: 
This form is a gift of the gods to men, which together with the purest fire was first given to them through Prometheus. Therefore the ancients (greater men and closer to the gods than us) have left the story behind, that everything which has ever emerged out of unity and multiplicity (plu- rality), in that it united within itself the unlimited (apeiron, universal) and the limit (to peras, unity): that thus we too in light of this arrangement of things should presuppose and search for every object one idea. . . . —It was the gods then, who taught us to think, learn and teach like this.76 
This form is the unity of the infinite in the finite, the multiplicity in the one, a form that caricatures the soul in Plato’s Timaeus: a circular movement that constantly comes back to itself. It is only with such a form that we are able to perceive the infinite in the finite, unity in multiplicity, as it is in nature as well as in art. Schelling’s reading of Plato’s world soul through the lens of Kant’s Critique of Judgment is foundational to his later development, including his appropriation of Spinozism. In his commentary on the Timaeus we find a passage that is almost identical to what Kant describes in §64 of the third Critique, which we have already seen: “We must remember from distance that Plato sees the whole world as a ????, that is to say, as an organized being [Wesen], as a being, whose parts are only possible in related to the whole, whose parts are reciprocal with each other as means and ends, and with each other produce their form as reciprocal connections.”77 
This original form is opposed to the mechanical form, but it is not on the other side of it; rather, the organic form allows the integration of the mechanical into itself, into a higher potency. There is therefore no longer an opposition between the organic and the mechanical, since the opposition is subsumed within the structure and operation of the organ- ism. The organic form stands for both nature and freedom. It is possible to read Schelling’s early philosophy of nature between 1795 and 1799 as an elaboration of such an organic form, which is at the same time a retrieval of Greek philosophy through the lens of modern natural science as well as an endowing of a new metaphysical meaning to nature that was in the process of being subsumed by mechanical laws.78 
§13. RECURSIVITY IN NATURPHILOSOPHIE 
In the First Outline it is very clear that Schelling is largely influenced by Spinoza, as he claims to develop a “Spinozism of physics.”79 Schelling adopts Spinoza’s view that nature is one and infinite,80 and that such a nature as a whole is “at once the cause and the effect of itself.”81 Spinoza distinguishes two types of causation: transitive causa- tion, in which the effect comes from an external cause, and immanent causation, in which the cause and effect are in the being itself, that is, a self-causation.82 Secondly, Schelling employs the vocabulary of Spinoza, notably natura naturata and natura naturans. In his Ethics, Spinoza defines the two terms as follows: 
[B]y Natura naturans we must understand that which is in itself, and is conceived through itself or those attributes of substance, which expresses eternal and infinite essence, in other words . . . God, insofar as he is con- sidered as a free cause. 
By Natura naturata I understand all that which follows from the neces- sity of the nature of God, or of any of the attributes of God, that is, all the modes of the attributes of God, that is, all the modes of the attributes of God, in so far as they are considered as things which are in God, and which without God cannot exist or be conceived.83 
Hence natura naturata refers to the products of nature, and natura naturans designates the productivity of nature. According to Schelling, when nature is mere product (natura naturata), we take it as object, but as productivity, we then take it as subject.84 Now, the subject has a dif- ferent relation to nature, in the sense that the I is part of nature and can be deduced from nature. Nature as productivity constantly generates products, and hence nature is always in becoming. Schelling uses the same vocabulary as Spinoza, but he may not employ the same senses and weights that Spinoza gave to these terms. Joseph P. Lawrence proposes that Schelling gives a stricter priority to natura naturans than does Spinoza,85 and in so doing also reintroduces contingency into the process of production. If in Spinoza what is revealed rationally is ratio- nal, for Schelling, as Lawrence notes, what is revealed rationally can turn out to be irrational.86 
The products are forms of appearance (Erscheinungformen); they are temporary products. The products of nature are produced when the force of nature is inhibited (gehemmt) by something else, and in the products we observe the constant manifestation of nature. Schelling often employed the example of the whirlpool. When the flow encoun- ters an obstacle, it produces a whirlpool. However, it is not a whirlpool as such, but like a whirlpool changes constantly according to the axis of time and the force of the flow. It means that the product is also under transformation and becomes productive: “The whirlpool is not some- thing stationary, but rather constantly changeable and newly repro- duced every moment. No product in nature is thus fixed, but produced in every moment by the power of the whole Nature. (We do not see the persistence [Bestehen], but the constant reproduction of the natural products.)”87 
The visual example of the whirlpool demonstrates the becoming of the infinite of nature, and its manifestation in finite beings, and situates them in a system in which finite beings are carried within the force of nature, whether it is generation or corruption. In the constant becom- ing of nature there are different forms of metastabilities, and in fact the organic form of being as well as philosophy are constituted by the metastability and are conditioned by the unconditional nature. Fred- rick Beiser observed some reconciliation of the preceding oppositions among the early Romantics. The first consists in the reconciliation between idealism and realism, incarnated in Fichte and Spinoza. Fichte stands for idealism, in which the subject is posed as the foundation of knowledge, and Spinoza stands for realism, which affirms the existence of nature outside the human subject. Or, more precisely, in Schelling’s writings the philosophy of nature means to overcome both transcen- dental philosophy and empirical science by founding the ideal on the real through the search for its regularity and purposiveness. The second reconciliation is the reading of Spinoza through Leibniz, especially the latter’s monadology. Beiser states: “The romantics fused Leibniz’s vis viva with Spinoza’s single infinite substance, creating a vitalistic pantheism or pantheistic vitalism. If they accepted Spinoza’s monism, they rejected his mechanism; if they rejected Leibniz’s pluralism, they accepted his vitalism, his organic concept of nature implicit within his dynamics.”88 
The vis viva is a concept from Leibniz that is defined as “a power that acts as long as there is suppression of obstacle, without identifying itself with the activity.”89 The force acts spontaneously without being confined to a predefined activity. The vis viva is compatible with Leib- niz’s concept of substance, which stands in contradiction to Aristotle’s, since for the latter, substance, either animated or not, is not really spontaneous but rather acts according to the excitation from exterior- ity. In this marriage between Leibnizian force and Spinozist substance we can see the omnipresence of an organic and recursive form com- posed of monadic reflections according to a preestablished harmony.90 The monad becomes the simple factor of the composition of nature, and their totality conditions their own movements. When Schelling describes what he calls dynamic atoms, he means original actants (ursprüngliche Aktionen) that are comparable to atoms that constitute the physical material for the corpuscular philosophers, which Schelling also calls nature monads (Naturmonade).91 However, it is important to bear in mind that these simple actants are not masses themselves; they are more like forces or syntheses of forces. Schelling worked out a three-stage operation that sounds very much like a Simondonian con- ception of individuation: When a limit or inhibition is imposed upon the actants, it is followed by an “alternation” of forces in the being in question; the alteration finally leads to a metastability.92 Now Schelling wants to avoid an opposition between the atomistic system and the dynamical system, since the starting points of both ideas are problem- atic and oppositional93: 
The dynamical system denies the absolute evolution of Nature, and passes from Nature as synthesis (= Nature as subject) to Nature as evolution (= Nature as object); the atomistic system passes from the evolution, as the original, to Nature as synthesis; dynamics passes from the standpoint of intuition to that of reflection; atomistics from the standpoint of reflection to that of intuition.94 
Schelling wants to use his system as a third, which means to con- sider the duality of nature as both productivity and product, situated in a recursive reciprocity. This recursive reciprocity is clearly expressed in the “On the World Soul” and in the third division of the First Outline. Schelling offered three hypotheses regarding the first cause of life, and as usual the third is the most probable system. The first hypothesis asks, should one find the first cause of life in living matter itself? The second hypothesis asks, should one find the first cause of life outside of living matter? The first hypothesis is not valid, since life is not the property of its products, but the other way around: Matter is the product of life. The second hypothesis is based on the medical discovery of irritation and allergy, meaning that the animal matter is stimulated by the outer environment. This is rejected because it is only an explanation based on transitive causality, and hence cannot explain life. Compared to specu- lative physics, what physics and chemistry cannot do is to go beyond nature as phenomenon or nature as object to nature as subject. The third hypothesis runs like this: “[T]he first cause of the living [Lebendigen] is contained in opposing principles, of which one (positive) is to be sought outside the living organism, the other (negative) in the organism itself.”95 For example, life is the organic form of the positive and nega- tive principles of combustion96: on the one hand phlogistication-deox- idization and on the other dephlogistication-oxidization.97 It is worth noticing here that the question of heat is at the center of this recursive model, as it was one of the most significant discoveries of eighteenth- century science, and we will return to it in chapter 2. This “eternal loop” is the form of the general organism (Allgemeiner Organismus) that is also nature conceived by Schelling, a revival of the concept of the world soul in Plato’s Timaeus. Here also lies the element of Schelling’s concept of organization: 
Organization is to me nothing else than the stopped stream of causes and effects. Only where nature has not inhibited this current does it flow forward (in a straight line). Where nature inhibits the current, the current returns (in a circular line) to itself. Not all succession of causes and effects is excluded by the concept of the organism; this term denotes only a suc- cession, which flows back within itself within certain limits.98 
We will further discuss the question of form in Schelling’s early philosophy in chapter 3. It is necessary here to state that this model of nature is not only limited to explaining a particular phenomenon, but rather aims to explain nature in its totality. This is the aim of a “sys- tem” instead of a mere hypothesis. In every single finite being there is, however, an infinite process through which binary oppositions are overcome: mechanical and organic, finite and infinite, rules and free- dom, necessity and contingency. This infinity is produced by a produc- tive recursivity. And if, for Fichte and Schelling, the Unconditioned is to be found as the starting point of this recursive thought; however, this unconditional is nature itself as a whole. The Idealists, from Kant to Fichte to Schelling, aim for a mathematical model of theoretical reason, which is able to unveil the “infinite metamorphosis” of spirit and nature.99 
§14. ORGANICIST AND ECOLOGICAL PARADIGM 
Why are we taking such a long journey to reconstruct Schelling’s philosophy of nature, considering that such a concept of nature is considered to be nonscientific, if not simply nonsense? First of all, we want to show how recursivity and contingency are articulated in the philosophy of nature through a historical critical exposition; second, we want to bring Schelling’s Naturphilosophie into dialogue with our contemporary situation. Some historians argue that Schelling’s phi- losophy of nature exercised a significant influence on the scientists of his time, and this cannot be ignored. However, the question here is not one of subordinating philosophy to science and considering philosophy as a pseudoscience or a mere source of inspiration. Schelling’s treatise on nature is a philosophical attempt to integrate natural science and to give wings to science instead of succumbing or reducing philosophy to science; while he does so without rejecting or condemning science, he makes science stronger than it was. Schelling’s philosophy of nature is a system that attempts to resolve the oppositions between nature and spirit, necessity and freedom, as readers of Schelling’s Treatise on the Essence of Human Freedom may find that the system of freedom is based on the system of nature that he constructed in the earlier days. As part of Romantic nature, Schelling’s nature is opposed to mechanism. 
Such antimechanistic thinking exists in parallel with the Industrial Revolution, which is the reign of mechanism, since an organic machine was not yet possible. Feedback mechanisms were applied in James Watt’s centrifugal governor (1788), but a theory of feedback did not appear until 1948. 
There are two ways to take Schelling’s concept of nature further. One is to conceive a “Romantic ecology” around the intersection of the eighteenth and nineteenth centuries, and to further understand the role of this concept of a rich and living nature in the expressions of art and philosophy, as in the work of the historian Robert Richards.100 The other is to see that, in the thought of Schelling, there is a proto-cybernetics or a proto-systems theory. However, these two are not separable, and this is precisely what we would like to show. Some authors101 claim to return to nature and therefore see Schelling’s Naturphilosophie as an ecological thinking, since there is first an original concept of nature (in the Greek sense of phusis) and second a holism that takes the wholeness and reciprocity as the principle of nature, for example, in the Gaia hypothesis. However, rushing toward this conclusion is problematic since it fails to see Schelling’s Naturphilosophie as a pre- cursor of both organicism and organology. This proto-organicism and proto-organology in Schelling’s thought is, however, in tension with the Romantic reading of it, since it at the same time affirms the productivity of nature as a general organism and produces a general form that can be mathematically modeled. In his seminal work, Grant shows only that the model shares similarities with digital technology or the “artificial life,” and sets up an opposition between “abstract artificiality” and “physical actuality.”102 However, Grant didn’t go further than this. He might have intended to eliminate the distinction between nature and technics; how- ever, it seems to me that he goes too quickly on this point, and it is our task to elaborate on these historical transitions. 
We would like to see how such an organic whole, which is fun- damental to Schelling’s Naturphilosophie, comes back in the early twentieth century and is developed into variant schools of thought around what is known as organicism.103 A valid question to be raised is if Schelling’s philosophy of nature is fundamentally a vitalism. If we understand vitalism in terms of the concept of an élan vital or entel- echy, then we don’t really find such a vital force in his thinking, and indeed he also criticized it as being mysterious. If we understand it as a thought that eschews Aristotelian hylomorphism, in which matter is understood as inert and form is the only force that gives identity and movement, then one may want to associate Schelling with it. However, in between vitalism and mechanism there is also the “third way” known as organicism, associated with authors such Ludwig von Bertalanffy, Joseph Needham, Joseph Woodger, and Conrad Waddington (who were very much influenced by the famous Spemann-Mangold organizer104 experiment in 1921), and many others.105 The organicists wanted to overcome the opposition between vitalism and mechanism, and show that, for example, a cell can neither be reduced to physico-chemical explanations nor be understood as a mysterious vital force, but rather comprises different forms and levels of organization (for example, there can be different levels of organizers in the sense of Hans Spemann). 
What is this organizer, which is able to induce another embryo from a transplanted graft? And what are its physico-chemical activities? 
These were the questions that interested Needham (as well as Wad- dington, among others), and later brought him to the laboratory in Berlin-Dahlem. The organizer is like the nucleus in crystalization—the supersaturated solution can start to crystalize only in the presence of the nucleus; likewise, the individuation of the embryo will take place on the graft only in the presence of the organizer. This contrasts with Driesch’s explanation of the determination of the embryo as the act of the entel- echy, according to which the determination is understood as a reduction of prospective potency (Prospektive Potenz) to prospective significance (Prospektive Bedeutung). Driesch claims that “a system in the course of becoming, is unable to increase its manifoldness of itself.”106 Needham sees Driesch as a mechanist who dressed mechanism in the new cloth- ing of the entelechy but remains mechanical regarding the development of the embryo. Along with Woodger, Needham sees the spatialization in the development of the embryo as a process of constant complexifica- tion: “(1) [T]he number of components is increased; (2) the complex- ity of the relations in which those components stand to one another is increased; (3) the intrinsic patterns of the components become different from one another.”107 Organicism can be considered a paradigm of thinking that moves from materialistic science to organic material- ism. As Whitehead has shown, in the nineteenth century atomism was introduced into science by John Dalton, later spreading into biology and influencing the development of cell theory.108 In this trend of thought, physics was regarded as the ground for understanding nature, and there- fore biology had to adopt concepts in physics. In the twentieth century, on the other hand, Whitehead saw a necessary change in moving from materialistic mechanics to an organic mechanics, since an atomic phys- ics that places emphasis on substantial matter was no longer able to give a full account of all natural phenomena: “The appeal to mechanism on behalf of biology was in its origin an appeal to the well-attested self- consistent physical concepts as expressing the oasis of all natural phe- nomena. But at present, there is no such system of concepts.”109 
Whitehead’s ontology of the organism is based on interaction and becoming. A tree is not an independent tree, but rather a tree in the for- est; it has interaction with all other individuals of its surroundings, and such dynamics can be analyzed through internal and external relations (whereas, on the contrary, in material atomism there are only external relations). Whitehead’s take on the transformation of materialistic sci- ence into a philosophy of the organism had a great influence on the organicists—notably, on Needham, Waddington, and Woodger, who believed that modern thought demonstrates a very different philosophi- cal worldview from the Cartesian one and has its origin in the concept of the organism110: “The doctrine which I am maintaining is that the whole concept of materialism only applies to very abstract entities, the products of logical discernment. The concrete enduring entities are organisms, so that the plan of the whole influences the very characters of the various subordinate organisms which enter into it.”111 
What the organicists took from Whitehead is not a vague holism but rather a new analytical method of studying the organization in dif- ferent levels; as Needham says, “[A] logical analysis of the concept of organism leads us to look for organizing relations at all the levels, coarse and fine, of the living structure. Biochemistry and morphology should, then, blend into each other instead of existing, as they tend to do, on each side of an enigmatic barrier.”112 Morphology here refers to the work of D’Arcy Thompson, especially Growth and Form (1917), which also inspired Alan Turing’s 1953 paper “The Chemical Basis of Morphogenesis Morphology,” a paper that resonates very much with Needham’s statement. Needham distinguished two types of organi- cism: one “obstructionist” or “dogmatic,” the other “legitimate.”113 The former overemphasize the whole, claiming that all parts are indispens- able from the whole and that the whole can never be rendered transparent; Needham gives the example of J. S. Haldane (to be distinguished from his son, J. B. S. Haldane, who we will discuss in the next chapter, concerning cybernetics). The legitimate organicists, on the other hand, analytically inquire into different relations between parts and the whole: (a) indepen- dence, (b) functional dependence, and (c) existential dependence.114 
We will have to leave the complex history of organicism to profes- sional historians of science, without surveying all these authors, though some of them (such as Waddington) we will engage with later. We would like instead to look particularly into the general systems theory founded by Bertalanffy, whose insistence on the mathematization of the Rationalisierbarkeit of nature was admired by Needham and Wood- ger.115 Herein lies also the difference between organicism and vitalism: Though they share the view that the study of the parts cannot explain the whole, vitalism introduces some quasi-metaphysical entities as the ground of explanation, while organicists insist that the three principal elements of life (namely, wholeness, directness, and regulation) are explainable even without such mysterious notions.116 While Schelling was writing his philosophy of nature, the Industrial Revolution had just started. The opposition between the mechanical and the organic, as well as the latter’s superiority over the former, remained theoretical, while with the advent of the Industrial Revolution the mechanical worldview triumphed not by regaining its theoretical priority but by reintroducing itself through a material transformation of the world.117 Bertalanffy developed a general systems theory based on an organismic view of the world, which seemed to him a decisive conceptual tool to reverse the situation brought to an impasse by mechanism and industrialism: 
The mechanical world view, taking the play of physical particles as ulti- mate reality found its expression in a civilisation which glorifies physical technology that has led eventually to the catastrophes of our time. Pos- sibly the model of the world as a great organization can help to reinforce the sense of reverence for the living which we have almost lost in the last sanguinary decades of human history.118 
Organicism is fundamental to thinking of an open system, which is different from a closed system for the reason that the former exchanges information with its environment, which defers its destruction accord- ing to the second law of thermodynamics. Systems theory investigates a form of organization of which organicism is an advanced form. If classical physics produces a theory of unorganized complexity, sys- tems theory concerns “organized complexity,” as Bertalanffy claims that “the theory of unorganized complexity is ultimately rooted in the laws of change and probability and in the second law of thermodynam- ics. In contrast, the fundamental problem today is that of organized complexity.”119 Bertalanffy’s organizing principle—the Gestaltprinzip or Ganzheitfaktor—is immanent in all levels of organization. His emphasis on organicism can be summarized in terms of the following four main elements: appreciation of wholeness (regulation), organiza- tion (hierarchy and the laws proper to each level), dynamics (process— or later, the behavior of open systems), and mathematization.120 
Bertalanffy seems to have been a reader of Kant, but probably not a careful one. He criticizes the categories in the first Critique by saying that it is symptomatic that Kant didn’t introduce the notion of interac- tion and organization.121 He also cites the famous concluding sentence of Kant’s second Critique—namely, “the starry sky above me and the moral law within me”122—in order to make the criticism that “Kant’s moral imperative even if not eroded, would be much too simple for a complex world.”123 It is astonishing that Bertalanffy didn’t comment on the third Critique, which anticipates his own verdict on the “organismic 
revolution”: 
Considered in the light of history, our technology and even our society are based on a physicalistic world picture which found an early synthesis in Kant’s work. Physics is still the paradigm of science, the basis of our idea of society and our image of man. . . . in the meanwhile, however, new sciences have arisen—the life, behavioural, and social sciences.124 
Bertalanffy didn’t notice that the term organic, or, in his own terms, organismic, had been the new condition of philosophizing since the time of Kant. His organismic revolution is an awakening in the natural sciences affirming Kant and Schelling’s reflections on the methodology of those sciences. From this perspective, we may see an isomorphism between systems theory and Naturphilosophie, since they all emphasize “part-whole relations.” However, what sustains systems theory as a science is that it wanted to become a “logico-mathematical discipline, in itself purely formal but applicable to various empirical sciences,”125 for example, thermodynamics, biological and medical experimentation, genetics, life insurance statistics, and so on. Bertalanffy’s approach reso- nates with Lorenz Oken’s comment on Naturphilosophie: “[P]hilosophy of nature is only science if it is mathematizable, i.e. if it can become mathematics.”126 
Bertalanffy’s general systems theory is one that resonates with cybernetics, which was developed independently in the same epoch and constitutes the same paradigm with various other schools: Wie- ner’s theory of feedback, W. Ross Ashby’s theory of self-organiza- tion, von Neumann’s theory of automata, and the like. And indeed, as Waddington criticized in his correspondence with the author Arthur Koestler, Bertalanffy’s general systems theory would “eventually become the lingua franca of the computing and artificial intelligence realm.”127 In chapter 2 we will try to understand how Waddington’s comment is relevant here, though that could be what Bertalanffy himself would resist.128 It may be too provocative for the scholars of German idealism to see Schelling as a precursor to the cybernetic concept of life, but it is also by no means acceptable to ignore the historical development and isomorphism that we have attempted to construct here. This is the reason that the current return to Naturphil- osophie as an ecology would be suspicious if it fails to understand that ecology is already a cybernetic concept; that is to say, the conceptual shift from Naturphilosophie to systems theory reflects the concretiza- tion of ecology. 
Ecology is a term coined by the German Darwinian biologist Ernst Haeckel (1834–1919), for whom ecology meant “the entire science of the relationships of the organism to its surrounding external world, wherein we understand all ‘existence-relationships’ in the wider sense.”129 Haeckel follows Darwin in conceiving of an economy of nature130 that concerns adaptation and competition. As Haeckel says on a later occasion131: 
[B]y ecology we mean the body of knowledge concerning the economy of nature—the investigation of the total relations of the animal both to its inorganic and to its organic environment; including, above all, its friendly and inimical relation with those animals and plants with which it comes directly or indirectly into contact—in a word, ecology is the study of all those complex interrelations referred to by Darwin as the conditions of 
the struggle of existence.132 
Does ecology, a theme that has so many political and social connota- tions today, still hold the same meaning as naturalists such as Haeckel took it to have? We have briefly discussed Latour’s claim that nature must die. While the death of nature does not mean that all that was nature now becomes artificial, it does mean that nature as a category in our knowledge system has taken up a very different meaning because the perspective from which we observe it has been shifted by technol- ogy. We would like to invoke here an intriguing remark from an inter- view with Marshall McLuhan, conducted in the 1970s: 
Sputnik created a new environment for the planet. For the first time the natural world was completely enclosed in a man-made container. At the moment that the Earth went inside this new artefact, Nature ended and Ecology was born. “Ecological” thinking became inevitable as soon as the planet moved up into the status of a work of art.133 
Ecology, in McLuhan’s view, is no longer about an economy of nature of which human beings are part, like other plants and animals. Technology has elevated humanity to another level, as the microscope and telescope had done in the time of Kant, with the difference that human beings are still on the earth. With this elevation, the earth is no longer the “original ark” in the sense of Edmund Husserl, but rather it is submitted to engineering; or, more precisely, it becomes an artificial earth or a Spaceship Earth, according to the architect and designer Buckminster Fuller—not that it is made artificially, but rather that it is an object of engineering. Retrospectively we can see that McLuhan has predicted Latour, as well as many others (i.e., Timothy Morton) who argue for an ecology without nature.134 If the question of ecology is tech- nological then we cannot avoid a direct confrontation with the question of technology. If we follow McLuhan’s verdict and Grant’s historical survey, we can see that the artificial earth was already underway during the time of the Idealists and was completed in the time of cybernetics. It is no coincidence that we also find such a trajectory in geology, and more significant is that it is related to James Watt’s improvement of the steam engine and its wide application during the Industrial Revolution. 
§15. GENERAL ORGANISM, GAIA, OR ARTIFICIAL EARTH 
In 1795 the geologist and natural philosopher (as it was called by then) James Hutton published the first two volumes of his Theory of the Earth (the third and last one was not published until one hundred years later). Before that, Hutton had already given a talk in 1785 in the Royal Academy of Edinburgh, in the presence of almost all Scot- tish Enlightenment thinkers. These three volumes are the complete demonstration of the researches and journeys that he had made in the previous decades. It is very interesting to observe how Hutton’s earth system is developed into two interpretations and probably two mod- els. The contribution of Hutton to modern geology is fundamental, and he is often referred to as the father of the discipline. To under- stand Hutton’s theory in simple terms, let us distinguish him from two schools. One is neptunism, which believes that the rocks are formed from the precipitation of the earth according to the following order (from bottom up): granite, gneiss and schist, basalt, limestone, sedi- mentary rocks. Hutton shows that instead of considering it as a linear formation, a cyclic model is more appropriate for understanding the formation of different layers of rocks. At the center in this model is heat, the driving agent of the earth system (like Schelling and many others of his time, Hutton believed in phlogiston). It consists of a cycle of deposition, lithification (causing loose sediment to convert to sedimentary rock), and uplift (vertical elevation of land caused by pressure of plates).135 With this cycle we can imagine the movement from rock to soil to rock, from sea to air and back to sea again. Hut- ton’s theory was in great contrast with the biblically inspired mosaic timeline of the earth. According to the later, the earth is dated 6,000 years old. However, Hutton’s research shows that it dates at least 800 million years old—much longer than the speculations of Buffon and Georges Cuvier. This is also known as the deep time of the earth. To understand it better, let us illustrate it with the example of the Siccar point, which consists of two separate sets of rocks: at bottom is called the Silurian greywacke; the upper layer, sandstones. Approximately 425 million years ago the collision of plates created a trench that trig- gered the settlement of sediments. The pressure compacted the sedi- ment and minerals in the water and cemented the grains together to form rocks (lithification). Further movement of the plates compressed and wrinkled the sediments and lifted them above sea level to form mountainous land.136 
The translators of Schelling’s Historical-Critical Introduction to the Philosophy of Mythology have noticed Schelling’s hidden refer- ence to Hutton’s theory, when he talked about the “geological hypoth- esis of uplift” (Erhebungstheorie) by Johann Wolfgang von Goethe,137 which “proceeds from an idea in which one can no longer speak of something that is certain and lawful but rather only of contingent and disconnected events.”138 Schelling is using the geological hypothesis to demonstrate his theory of the contingency of nature. However, this contingency has to be distinguished from another school that Hutton’s theory was in conflict with—namely, the Cuvier-inspired catastroph- ism.139 In the context of Hutton, it stands for the idea that the land was formed by a series of catastrophes, for example, the Great Flood. According to Hutton’s theory, the earth is an ongoing and cyclic pro- cess driven by heat known as uniformitarianism, and no catastrophic event is necessary for such a process. Hutton’s theory gave rise to an ambiguous reading of the earth as both organic and mechanical. In parallel to the concept of the earth as a “superorganism,” there was also a mechanical view of it. Hutton was not innocent of this, since he also calls earth a “living machine.”140 The earth does not work without regularities; indeed, the observations of Hutton confirmed that “in nature there is wisdom, system and consistency.”141 However, a deeper relation between the earth and the machine is revealed by the correspondence between Hutton and James Watt, the inventor of the steam engine: 
I have a letter from Watt—he has brought his curious wheel to go, it works by steam—he says it appears to be right in all essentials and goes with an equitable motion & great power. In short, I believe it will answer (his own words) this will raise his fame yonder it being so new a thing for that is what catches the multitude; though I think the improvement of the reciprocating fire engine is the thing of greater utility but every one will not be sensible of the merits of that great improvement.142 
Hutton befriended Watt through Watt’s mentor Joseph Black, profes- sor of medicine at the University of Glasgow, when Watt was an instru- ment maker at the university. Black had discovered a very important theory of heat, that is, latent heat. Hutton considered latent heat to be equivalent to “Newtonian gravitation” in terms of scientific insight.143 Hutton considered latent heat as part of the “repulsive force” or “solar substance” of the earth, and specific heat as “expansive force,” whose dynamics explain the natural cycles of rock formation.144 Latent heat is believed to be decisive to the steam engine, since it is crucial for calculating and calibrating the heating and condensation of vapor. Watt himself had proved with his experiment that the steam could heat six times its weight of water to 212°F.145 Despite the dispute over the role of the theory of latent heat in Watt’s improvement of the Newcomen engine, it is, however, evident that latent heat itself plays a very impor- tant role in the working principle of the engine. One can speculate on the relation between Hutton’s earth system and the steam engine, and to what degree can one set up analogies between the two mechanisms; for example, the volcano is to the earth machine as the safety valve to the steam engine. For this, at least the fire-water model is assured: 
Hutton himself observed that his theory is “a system in which the subter- ranean power of fire, or heat, co-operates with the action of water upon the surface of the Earth. . . .” He perhaps saw the elevation of new con- tinents by subterranean heat, and the lowering of the older continents by atmospheric agencies, as in some way analogous to the rise and fall of the piston in one of Watt’s early single-acting pumping engines.146 
This organo-mechanism or teleo-mechanism of the earth was taken up in the Gaia hypothesis proposed by James Lovelock, and later developed together with the biologist Lynn Margulis. Lovelock refers to Hutton’s famous 1785 lecture at the Royal Society of Edinburgh: “I consider the earth to be a super organism and that its proper study should be physiology.”147 It is not unreasonable to see a lineage from Schelling’s general organism to Hutton’s super organism and Lovelock’s Gaia, in which we see how a philosophy of nature progresses into cybernetics. Lovelock also borrows the concept of homeostasis from cybernetics to describe the geophysiology conceived by Hutton. To Lovelock’s eyes, the cybernetic system is superior to the mechanical system and transcends all ideologies, whether it be capitalism, Marxism, tribalism, or nationalism.148 Gaia is a term that was recommended to Lovelock by the author William Golding to replace the phrase “a cybernetic system with homeostatic tendencies as detected by chemical anomalies in the Earth’s atmosphere.”149 In the early stage of the Gaia theory, Lovelock understands Gaia as a single organism composed of various vital organs at the core and in the periphery. This homeostatic system maintains constant conditions in terms of temperature, acidity, alkalinity, and gas composition: 
Gaia responses to changes for the worse must obey the rules of cybernet- ics, where the time constant and the loop gain are important factors. Thus the regulation of oxygen has a time constant measured in thousands of years. Such slow processes give the least warning of undesirable trends. By the time it is realized that all is not well and action is taken, inertial drag will bring things to a worse state before an equally slow improve- ment can set in.150 
In contrast to Lovelock’s strong form of Gaia,151 consisting of a single organism, Margulis forced Lovelock to admit that Gaia does not consist of a single organism but is rather a symbiogenesis of a great variety of organisms,152 including plants, animals, fungi, protists, and bacteria.153 The concept of symbiogenesis in turn comes from Varela and Maturana’s concept of autopoiesis. Some authors therefore claim that with the participation of Margulis, the Gaia theory moves from first-order cybernetics to second-order cybernetics.154 On the other hand, we can equally say that cybernetics and organicism converge in the Gaia theory, in which the earth becomes an organo-mechanical being.155 If Schelling’s Naturphilosophie, systems theory, organicism, and Gaia theory have been attempts to overcome mechanism extended to the planetary scale after the Industrial Revolutions, and in a certain sense to surpass the modern—and, paradoxically, or better, dialecti- cally, by doing so the earth becomes an artificial earth—the Anthropo- cene signifies first of all the accomplishment of such an artificial earth as a cybernetic system. Lovelock claims that technology, especially communication technology, “has vastly increased Gaia’s range of perception,” and therefore “she is now through us awake and aware of herself. She has seen the reflection of her fair face through the eyes of astronauts and the television cameras of orbiting spacecraft.”156 Lovelock seems to imagine that planetary computation will wake up Gaia or assist it in maintaining its homeostatic functions, but isn’t this a further accomplishment of the technical system, and will this really move us away from the crisis of modernity? Or is it only the aggrava- tion of the symptom of modernity, as Nietzsche says in Twilight of the Idols, that philosophers’ war against decadence is only another expression of decadence?157 The questions asked by the Idealist phi- losophers regarding system and freedom return in our own epoch in a very different form, but with increasing urgency. This constitutes the new paradigm as well as the new condition of philosophizing after Kant’s “organic turn.” This is also the reason for which we have to sketch a new trajectory of thought. However, before one can condemn cybernetics as a mechanism of control or governance, we will need to develop a new understanding of cybernetics according to recursiv- ity and contingency, and see what the impasses are that we need to confront. 
Logic and Contingency 
I am as I am not. 
—Heraclitus, Fragment 81 
In chapter 1 we attempted to examine the notion of the system, and to show how it becomes a conceptual tool for resolving some classical philosophical problems, through our interpretation of two categories, recursivity and contingency. The recursivity of natura naturans and natura naturata, on the one hand, captures the one and all (hen kai pan) of all beings; on the other hand, it also makes reason a product of the self-unfolding of the Absolute. Kant’s treatise on teleological judgment provided a new condition for reflecting on the systemic organization of organic and inorganic beings and endowed it with a metaphysical sig- nificance; in doing so, contingency is not overcome as mere negativity, since it is now necessary. We can find similar but somewhat nuanced operations in Hegel’s logic of reflection or dialectics. If, in Schelling’s recursivity, the identity between mind and nature is assumed, then con- tingency is necessary because it functions as a hindrance in the reflec- tion of the natura naturans.1 In Hegel’s recursivity this identity is not assured at the beginning. He starts with contingency, which is a step or a test that reason must pass in its journey toward the Absolute.2 The division of Hegel’s Encyclopaedia (1817) into three parts—namely, logic, philosophy of nature, and philosophy of spirit—can be read as a movement toward the world of the spirit through the constant exterior- ization (Entäusserung) and sublation (Aufhebung) that characterizes the life of the Notion. 
There are two operations of recursivity and two functions of contin- gency in Schelling and Hegel. These two methods nonetheless share a similar task and epistemological paradigm, namely, the rise of biology or science of the organic and chemistry, as well as the discovery of magnetics and electricity, in which the antique concept of essence is reinterpreted as a process. Schelling seizes the necessity of contingency as the outcome of a productivity and creativity of the artistic act, while for Hegel the necessity of contingency is possible only when it is submitted to the examination of logic. The objective of this chapter is twofold: First, we attempt to elabo- rate on Hegel’s concept of recursivity in relation to contingency; second, we want to see how such system building is relevant to the concept of recursivity in cybernetics. Without claiming that Hegel’s and Schelling’s concepts of the organic are identical with recursive algorithms, we want to show how the trajectory that we work out here is beneficial for reflecting on the relevance of idealism for a contemporary philosophy of technology. If Schelling’s Naturphilosophie is a precursor to biological organicism, Hegel’s logic anticipates a machinic organicism, which is cybernetics. Like idealism, which wants to surpass mechanism and mythical vital forces, cybernetics wants to overcome the opposition between mechanism and vitalism, and to provide an operational logic in all domains, which is characterized by a “self-consciousness.”3 
§16. RECURSIVITY IN THE 
PHENOMENOLOGY OF SPIRIT 
We will clarify a fundamental difference between the two models of recursivity, which hinges on the following question: Where is the unconditional or the Absolute located, at the beginning or the end? If the Absolute is at the beginning then contingency only arrives after- ward and will be recognized; if the Absolute is at the end, on the other hand, then one starts with contingency in order to arrive at necessity. One will always have to start with something, either an absolute or unthinged (Unbedingt) ground, as in Fichte and Schelling,4 or to arrive at the Absolute by sublating immediate sense certainty or abstract universality, as in Hegel. This constitutes Hegel’s main critique of Schelling in the preface to his System der Wissenschaft (1807), which later becomes known as Phenomenology of Spirit,5 a work that crystal- ized during years of collaboration with Schelling and at the same time serves as a fundamental critique of Schelling’s method (even though the name of Schelling is absent).6 If one starts with the Absolute, which sets itself in intellectual intuition, then there is no develop- ment, since what rests is only a blind force, an intelligibility in which everything is equal, like all gray cows looking similar in the dark. At the very beginning of the Phenomenology of Spirit, therefore, Hegel rejects the universality of immediate sense certainty: namely, this and now. The Absolute is not at the beginning of reason, but rather at the end. The Idealists, claims Hegel, accepted the “immediate certainty,” for example, that “I am I,” “my object and my essence is I,” but fails to see that it is an “absolutely negative essence.”7 This applies also to nature: The joyful natura naturans and natura naturata of Spinoza is deficient for Hegel, and so subjected to criticism in The Science of Logic. In Schelling the natura naturans and natura naturata of Spinoza consists of a constant reflection that gives rise to products and bifurcation; for Hegel, Spinozism is deficient in the sense that there is a lack of “immanent reflection” in the doctrine of emanation.8 Hegel presents another model in which immanent reflection is a dialectical movement that renders the Absolute transparent. That is to say, the Notion is a recursive process that arrives at itself as a comprehension of itself and the other as a whole. However, this journey is not simply a return to itself but rather a constant process of Aufhebung, in the senses of both preservation and cancelation. 
Phenomenology is thus a description of the processes of reflection, from immediate determinacy to certainty and finally to truth. Or, to put it in other words, following the remark of Wolfdietrich Schmied- Kowarzik, Schelling starts with philosophy of nature and attempts to understand nature, process, and evolution philosophically, where nature is the existing Wirklichkeitszusammenhang that founds and enables human history; while Hegel starts with logic, in which think- ing wants to grasp nature in the absolute Idea.9 The Hegelian dialectics is motivated by a negativity, negative due to the immediacy of being and reason’s confusion with the other, which becomes an obstacle to grasping the in- and for-itself of being that is its essence; while in order to move away such confusion, it needs to carry out a series of reflections, in which the self-positing and self-identifying of the given is accompanied by the recognition of the other as the other of the self and as contradiction, and finally the elevation to a synthesis that over- comes such contradiction. In The Science of Logic, this is presented as three reflections in the logic of essence, positing reflection, external reflection, and determining reflection. By resolving the unconscious threat of the negative other we arrive at the essence, in which sub- stance is understood as subject, or subjective reflection. Like what we did by reading Kant and Schelling through the lens of recursivity, the Hegel scholar Edmundo Balsemão Pires proposes to read Hegel’s preface to the Phenomenology of Spirit not as a “romance of culture” or a critique of modern philosophy but rather as a calculus.10 Pires sees the movement of the spirit as a recursive algorithm that progressively proceeds to the Absolute: 
Analogically speaking, Hegel’s work is an example of the Spirit’s algo- rithm from an initial state defined by the empirical certainty and a plain hetero-reference of knowledge. We may examine his work as a progres- sion of successive instructions to solve hetero-referential conditions of knowledge in self-referential schemes of the absolute motion.11 
Pires’s characterization of Hegelian phenomenology as a recursive algorithm is analogical, in the sense that such a claim is based on an operational similarity between the movement of the spirit and the recursive form of the algorithm. This movement starts with the imme- diate data, moving from abstract to concrete, from sense certainty to absolute certainty; while every movement is a reflection, or, as we may see later in the language of cybernetics, a reentry (in the sense of Spencer-Brown), a self-reference (in the sense of Luhmann). Hegel refers specifically to the self-movement of the ether, which has its “completion and transparency through the movement of its becom- ing.” It is in this sense that Pires claims “only self-reference justifies the idea of the reine selbsterkennen im absoluten Andersseyn. . . . Self-reference implies difference and a unity between knowing and the difference.”12 
This self-reference is the mechanism toward totality, since it is in the reflection that the self and its other are grasped as a whole, while the truth no longer lies in the self or the other, but in the whole. As Pires has noted, one of the central themes of the preface to the Phenomenology of Spirit is precisely the relation between truth and the system of science. Truth is not to be discovered through a system of science, as a proposi- tion whose validity has to be proved against a logical system, but rather truth “can only be the scientific system itself.”13 This wholeness is the organic totality, which Pires touches upon though he does not elaborate on either its relation with recursivity or the nature of such recursivity.14 The Hegelian recursivity remains obscure. 
§17. ORGANICIST AND REFLECTIVE LOGIC 
Recursivity allows the grasp of the organic whole through the recogni- tion of the other—this other is always plural and situated in different reflections. The Phenomenology of Spirit has already anticipated a large system that Hegel was going to elaborate over the rest of his career. And as John Findlay has pointed out, in the Phenomenology Hegel was already heavily engaged with the question of life and the organic.15 Like the organic in Kant’s Critique of Judgment, Hegel emphasizes the holism of the living being as well as the reciprocity of its different parts. The organic whole is considered as a self-sufficient movement that constantly produces and abolishes differences, and for this reason is no longer an abstract concept: 
Differences, however, are just as much present as differences in this simple universal medium, since this universal fluidity only has its nega- tive nature in so far as it abolishes such differences: differences must be there if differences are to be abolished. But this fluidity, as self-sufficient uniformity, is itself the subsistence of the substance of these differences, in which they occur as distinct members and independent parts. Their being no longer has the meaning of being in the abstract, nor their pure essence the meaning of abstract universality: their being is just the simple fluid substance of pure movement in itself.16 
It is evident here that Hegel was very much influenced by Goethe’s writing on the metamorphosis of plants,17 as was Schelling, who wrote about it in his Von der Weltseele. Beyond plants, Goethe was very interested in the debate between Étienne Geoffroy Saint-Hilaire and Georges 
Cuvier on animal structures, which took place in 1830.18 Cuvier pro- posed the concept of type as marking a rigid distinction between spe- cies, while Geoffroy Saint-Hilaire declared that such distinctions were mere fictions: “[T]here are no different animals. One fact alone domi- nates; it is as if a single Being were appearing.”19 Commenting on this debate, Goethe recognized regularities of nature according to rules and laws but at the same time that the law is also living, “that organisms can transform themselves into misshapen things not in defiance of law, but in conformity with it.”20 Goethe didn’t relinquish the idea of “types,” but attempted to understand rigid forms as products of a genesis that can be scientifically perceived through the “oscillations between ideas and experience.”21 However, we have to recognize that Goethe didn’t aim for a genealogy of species but rather only for a dynamic process that is termed “idealistic morphology.”22 As concerns morphology, Hegel understands that for life to understand life, it is necessary that con- sciousness shares the same genetic structure as living beings.23 We can therefore understand the Phenomenology no longer as transcendental philosophy but rather the science of the experience of consciousness. Though both were influenced by Goethe, there is a significant differ- ence between Hegel and Schelling with regard to the role of nature. For Hegel, nature is an object of observing reason from the outset, whereas for Schelling it is first of all preconsciously sensed (Emfunden) and detected (Angeschaunt) before becoming an object of reflection.24 
Hegel also touched upon the concept of the organic in The Science of Logic, and more fully elaborated upon it in his Naturphilosophie, the second part of his Encyclopaedia. It may worth mentioning that Hegel’s philosophy of nature is considered to be a very peculiar work, and his theory of heat and sound has been considered to be “gibberish.”25 In order to grasp the plant as a whole, argues Hegel, it is necessary to con- ceptualize the different stages of the development of the plant as conti- nuity and totality, that is, from germ to leaves and to calyx, and so forth. Section 346 of the Naturphilosophie consists of a detailed description of the metabolism of the plant. The production and abolition of differences is the motor of individuation. Hegel divides organisms into geological, botanical, and zoological organisms. Unlike Schelling’s emphasis on the Hemmung, which acts like an external negative force giving form to the productive force of nature, in Hegel there is an immanent negativity in the movement of the organism. We may want to consider these as two different readings of the Spinozist omnis determinatio est negatio. In keeping with his dialectical method in general, for Hegel, plant nature consists of a movement from the abstract to the concrete via an encounter with the other than itself: 
The process of the plant falls into three syllogisms. . . . [T]he first of these is the universal process, the process of the vegetable organism within itself, the relation of the individual to itself. In this process, which is that of formation, the individual destroys itself, converts itself into its inorganic nature, and by means of this destruction, brings itself forth from itself. In the second process, living being does not contain its other, but faces it as an external independence; it does not constitute its own inorganic nature, but meets it as an object, which it encounters through an apparent contingency. This is the process which is specified in the face of an external nature. The third process is that of the genus, and unites the first two. This is the process of the individuals with themselves as genus, or the production and preservation of the genus.26 
With this syllogism in mind—identity to the self, recognition of the other, unity—Hegel describes plant nature in terms of three major processes: a process of formation (§346), a process of assimilation (§347), and a generic process (§348). We may want to ask, isn’t the end of this process already contained at the beginning, as with Aristotelian teleology? Isn’t the life of the plant only the constant unfolding of the genetic information inside the seed, consisting of the preservation of the self and the genus? Hegel writes in §346a, concerning the development of the seed: 
The germ [seed] is the unexplicated being [das Unenthüllte] which is the entire concept [Begriff]; the nature of the plant which, however, is not yet Idea because it is still without reality. In the grain of seed [Samen- horn] the plant appears as a simple, immediate unity of the self and the genus. . . . [T]he development of the germ is at first mere growth, mere increase; it is already in itself the whole plant, the whole tree, etc., in min- iature. The parts are already fully formed, received only an enlargement, a formal repetition, a hardening, and so on. For what is to become, already is; or the becoming is this merely superficial movement. 
Concerning this determination already present in the germ, we may want to ask how far away Hegel is from preformationism. Hegel rejected the preformation hypothesis since it implies that there is no real development.27 Hegel, as we have seen, emphasizes the correspondence between organic life and the phases of the Notion, with the difference that the Notion is self-consistent in time while the plant differs from itself in time—the Notion actualizes itself more completely than the plant. The way to move away from this preformation theory is to rein- troduce the concept of contingency into nature, and this is precisely the aim toward the end of Philosophy of Nature. Hegel starts with logic and ends with logic, since what he means by logic is not formal logic, but rather what Findlay calls the “self-grasping Notion” (selbstbegreiffende Begriffe), meaning that the Begriff becomes the Begriff explicitly: “the Begriff of the Begriff.”28 However, since there is an asymmetry between organic life and the Notion, there is also an asymmetry between two contingencies. 
§18. “FEEBLENESS OF THE NOTION IN NATURE” 
It is with this remark in mind that we will come back to the question of contingency. Contingency becomes a question for Hegel because, in order to ground Idea as the essence of being, reason must recognize contingency in its development, not simply as something irrational and chaotic (though it may appear temporary), in order to free itself from the world in the world itself29; otherwise reason is exposed, first to its own limit of knowing, and second to an antagonism against nature. If the question of contingency is not recognized and therefore overcome, it also means there will be a discrepancy between the real and the ideal, thinking and being, consisting in the challenge put forward by Wilhelm Traugott Krug: namely, will the Idealists be able to deduce a pen from thinking?30 Retrospectively, if Hegel had access to a 3-D printer today, he would simply show Krug how the pen is recursively deduced. Hegel’s fierce response to Krug, first in his 1802 review of the latter’s work and his article in the Kritischer Journal der Philosophie,31 as well as later in footnotes in the Phenomenology of Spirit and in the Anmerkung of §250 of the Encyclopaedia, demonstrates that the Notion is not merely abstract, since concrete doesn’t mean physical and sensible, and we can understand the development of the concept as a recursive process in which it comes back to itself in order to know itself. This process can only be an individuation of the concept itself, in which the real and the ideal are no longer separated. This genesis will have to recognize contingency as part of itself, and therefore it is not an attempt to avoid or ignore contingency, but rather it is only by addressing it as necessary that reason is able to overcome it. In Hegel, therefore, we can see two senses of contingency: one is chaotic nature; the other, the logical category or even the category of being, as Bernard Mabille puts it.32 Some authors have suggested that these two notions 
of contingency are incompatible: “[C]ontingency in nature is not the category of contingency, but a pre-categorical sense of contingency.”33 However, these two contingencies are not unrelated, and we would like to propose that it is only when contingency becomes a logical category that the other contingency, which is characterized in nature, will be recognized as necessary. 
If we say that in early Schelling’s philosophy the question of contin- gency is considered to be necessity because the Absolute stands at the beginning of the genesis as the unconditional, in Hegel, the Absolute is no longer at the beginning but rather the process and the end. The reason is clearly spelled out in Hegel’s preface to the Phenomenology of Spirit: The presupposition of an intellectual intuition as such an abso- lute beginning is nothing but monochrome. But the more fundamental problem for Hegel is that it ignores the question of development, since the genesis is not a simple production, leaving recursion to the interven- tion of contingency as inhibition (Hemmung), which for him is only the realm of nature but not yet the world of spirit. 
If inhibition for Schelling is both contingency and the condition of production, like the halting status of an algorithm, then for Hegel such inhibition must undergo a dialectical process that doesn’t accept being given as such but sets it into movement. This is a process that we don’t find in Schelling’s recursivity, where the contingent is always already necessary. And it is for this reason that, in the early Schelling, there is no longer an easy opposition between freedom and necessity. As we have said, for some commentators, contingency in Hegel carries two senses: First, it is irrational and chaotic, as it is demonstrated in nature; second, it is a modal category like the possible, the actual, and the necessary.34 However, what does this distinction mean here? Does it mean there is no relation between the two senses at all? Or shall we conclude, as I will try to argue, that the irrational contingency has to pass into a rationalized one, which is implied already in the confronta- tion between nature and reason, and as is indicated already in one of 
the three syllogisms, namely Spirit, Logic, and Nature? Hegel’s dis- cussion of contingency in The Science of Logic occurs in the section titled “Actuality.” Actuality for Hegel means the rational (Vernünftig), as he famously claims in Philosophy of Right: “was vernünftig ist, das is wirklich, und was wirklich ist, das ist vernünftig”—the actual is real but the real is not necessarily actual (actual here means being sensed). The Science of Logic, which we will discuss later, seems to suggest such a transitional phase, in which immediate existence is passed into a “sensed sensible, rationalized existing” (sensible sensé, existant ratio- nalisé) in the words of Bernard Bourgeois.35 
The contingency of nature is a key subject in the Encyclopedia. In §250, Hegel contrasts it with the Notion that “the impotence of nature is to be attributed to its only being able to maintain the determinations of the Notion in an abstract manner, and to its exposing the foundation of the particular to determination from without.”36 Nature presents to philosophy a frontier that the latter finds difficult to cross, since if the Notion is able to explain all these contingencies, then the Notion will also be open to contingency. In §368 of the Encyclopedia, titled “The Genus and the Species,” one of the sections on zoology, Hegel contin- ues to expose the “feebleness of the Notion in nature”: 
The feebleness of the Notion in nature in general, not only subjects the formation of individuals to external accidents, which in the developed animal, and particularly in man, give rise to monstrosities, but also makes the genera themselves completely subservient to the changes of the exter- nal universal life of nature. 
What does Hegel mean by “the feebleness of the Notion in nature”? Unlike Notions developed by human beings, which are systematic and consistent, and hence protect themselves from external influences, nature is easily affected by contingent events, especially coming from the outside, since its Notion is too feeble to digest these contingencies and therefore it exhibits the irregularities and inconsistencies in its spe- cies. As Hegel wrote a few paragraphs later, in §368, Notions devel- oped by humans are subject to defects, but it is even more so in nature: 
If one is prepared to admit that the works of man are sometimes defective, it must follow that those of nature are more frequently so, for nature is the Idea in the mode of externality. In man, the basis of these defects lies in his whims, his caprice and his negligence. . . . [I]n nature, it is the external conditions which stunt the forms of living being; the forms of nature can- not be brought into an absolute system therefore, and it is because of this that the animal species are exposed to contingency. 
We therefore understand why nature sets limits to philosophy, as Hegel wrote earlier in the remark to §250: “[T]his impotence on the part of nature sets limits to philosophy, and it is the height of pointlessness to demand of the Notion that it should explain, and as it is said, construe or deduce these contingent products of nature.”37 We must not make the mistake here of assuming that Hegel is simply condemning contingency as chaotic and insignificant; on the contrary, he gives meaning to con- tingency, however, in order to overcome it. This is rather clear in §251, where Hegel writes: “[B]eginning with the externality in which it is first contained, the progress of the Notion is therefore a turning into itself in the centre, i.e. the assimilation into subjective unity or being-within- self of what is, to the Notion, the inadequate existence of immediacy or externality.”38 It would be an excuse if reason withdrew itself from this externality and ignored its existence, however. It is for this reason that we can interpret Hegel’s treatise on contingency in The Science of Logic as a unity with that in the Encyclopaedia. 
§19. DEATH OF NATURE AS AFFIRMATION OF LOGIC 
Hegel’s strategy of using nature as a transition toward spirit is well spelled out in the interpretation of Songsuk Susan Hahn. Since nature has no history, history can begin only with the sublation of the weak 
Notion in nature, by elevating it to the ethical: 
[W]e can’t say, strictly from a position inside nature, that nature reacts emotively with abhorrence to contradiction, without lapsing into a kind of anthropomorphism. At this level, nature is a blind, nonrational mecha- nism, and negation and contradiction appear to nature differently than they do to rational discursive beings such as ourselves. Since nonrational nature is morally neutral with respect to the presence or elimination of contradictions, Hegel seeks to derive from nature a normative notion which is appropriate to a rational self-consciousness who can react mor- alistically with abhorrence to contradictions and pursue the rational goal of eliminating them.39 
We want to suggest that this overcoming of contingency is already contained in the very possibility of the Notion itself. This is because for Hegel Begriff (Notion or Concept) is a living notion—living in the sense that it is reflective and self-determining, and thus not a static notion but rather a self-grasping notion, a notion in which the tension of duality and incompatible terms is overcome through unification in the reflec- tive moment.40 The reflective logic as a movement from appearance to essence is characterized by three reflections in The Science of Logic: first, positing reflection, which starts with appearance, for example, being that is immediate—such being is only negative, so the reflec- tion is a sublation of being as self-positing; second, external reflection, which is recognition of the other as the condition and contradiction of the self; and third, determining reflection, which is the unification of the positing reflection and the external reflection. In “Actuality,” chapter 2 of the “Doctrine of Essence” in The Science of Logic, using the same recursive form, Hegel demonstrates the necessity of contin- gency by describing movements of the Notion according to the modali- ties of actuality, possibility, and necessity. I will attempt to reconstruct Hegel’s argument here by highlighting how he starts with contingency and immanent reflection in order to arrive at a complete notion or a notion of the totality. The title of section A is “Contingency or For- mal Actuality, Possibility and Necessity.” Contingency here refers to the formal, meaning that the content is not yet taken into account. A being is given as such: for example, a stone. Its existence is concrete but merely contingent. It is a formal actuality, which also indicates a possibility. This possibility is actuality reflecting into itself. Reflecting into itself means that such being is a possibility out of many possibili- ties, such as all different sorts of stones from different origins. And it is for this reason that both actuality and possibility are formal; formal possibility means that everything is possible, for example, A could be A or could be -A. In the “reflecting into” there is “limitless manifold- ness,” since “everything is possible that does not contract itself.”41 This unity between the actual (of the being as such) and the possible (that this being is one of the possibilities) is contingency42: 
The contingent thus presents these two sides. First in so far as it has pos- sibility immediately in it, or, what is the same, in so far as this possibility is sublated in it, it is not positedness, nor is it mediated, but is immediate actuality; it has no ground . . . second, the contingent is the actual as what is only possible, or as a positedness.43 
Contingency carries such a paradox within it since as far as it is con- tingent, it is a mere possibility, and therefore is groundless, but since it is also actual, it has a ground. If being is, it is necessary, but such necessity is only formal, since it is determined as a possibility, and in this sense it is contingent. Contingency is regarded as the recognition of the formality of the existence of the being. If this first reflection is a mere self-identity, we enter a second reflection in relation to another44— in section B, “Relative Necessity, or Real Actuality, Possibility, and Necessity.” The real actuality is so far only the “in-itself of another actual,” and it immediately carries in itself real possibilities. Unlike the formal possibility, which is a reflecting into self as identity, real pos- sibilities are conditions of becoming as projection in reflection. Here Hegel talks about the self-sublating of the real possibility in which, first, the actuality becomes reflective being, “the moment of an other, and thus comes in possession of the self”; and, second, the in-itself of the other is therefore passed into actuality.45 It is not the turning back from itself to itself, but rather a “turning back into itself from the restless being-the-other-of-each-other of actuality and possibility [die Rückkehr in sich aus jenem unruhigen Anderssein der Wirklichkeit und Möglichkeit gegeneinander].”46 These two movements consist of a rejoining of possibility and actuality, by reflecting into the self and the other, which constitute a unity that Hegel calls real necessity. The real necessity is only a relative necessity, as is indicated in the title. It is relative because if we ask why is A necessary, it is because B and C are its conditions. Hegel didn’t distinguish the possible and the prob- able, but we may want to suggest that the real possible is the probable, since it is that which conditions the becoming of the real actual. It is in this sense that Hegel claims that “in actual fact . . . real necessity is in itself also contingency.” Now this contingency comes from content instead of form. 
The second reflection leads to a third reflection on the organic total- ity, in section B, which carries a much shorter title: “Absolute Neces- sity.” It is absolute since it constitutes its own ground,47 namely, that the notion attends its fullness: It is because it is.48 If we follow the opposition between necessity and contingency, one may arrive at a conclusion that absolute necessity is the negation of all contingencies, since the essence as necessity is not self-mediated and does not need to do so through another. However, this is the opposite of what Hegel would claim: “[T]his contingency (the possibility of an other) is rather absolute necessity; it is the essence of those free, inherently necessary actualities.”49 The McTaggarts explain this seemingly contradictory claim that though being is necessary, its constituent parts have ground in other parts; each part, when looked at separately, would be contin- gent.50 However, this sounds like only a repetition of the argument of section B. We may want to state in this way: If absolute necessity means that the global condition of existence is taken into consideration (meaning that it is no longer local [therefore relative]), this absolute necessity is still contingent. Hegel completed the circle by starting with the contingency of being and finishing there after three reflections; this circular movement continues in the course of time. 
As we have seen in Hegel, the necessity of contingency is in fact a process in which the Idea reflects on its abstract and formal possibili- ties to recognize its existence as contingency. By reflecting on itself as the possibility and contradiction of becoming other, it recognizes the other—as the other of the self—as the condition of its becoming, and therefore arrives at relative necessity. Reflecting on the ground of its existence, it recognizes that its existence as absolute necessity must contain in itself contingency. Contingency is therefore not a merely negative concept. Contingency is double: first, an unreflected contin- gency indicates the weakness of the Notion, for example, nature; its diversity is an indulgence, as Dieter Henrich writes: “Nature, because it is the frenzy [Aussersichsein] of the concept, is free to indulge in this diversity. So there are some sixty species of parrots, one hundred and thirty seven Veronica, and so on. To enumerate it seems to Hegel to be a mindless and boring occupation, because in such diversity there is no spirit.”51 
A reflected contingency, or rather a “domesticated contingency,” is necessary to comprehend the movement of the notion. We can therefore say that nature’s irrationality has its very reason.52 However, when we arrive here, we are now no longer in nature but in the realm of spirit, since it is a realization of self-consciousness. In this sense nature, as Hegel himself said, is “die Idee in der Form des Andersseyns” (Idea in the form of the other) or “der sich enfremdete Geist” (a self-alienated spirit), “ein bacchantischer Gott, der sich selbst nicht zügelt und fasst” (a bacchanalian god who does not control himself).53 Nature expresses itself as the exteriority of contingency and is consequently overcome, and through it the Notion reaches the in- and for-itself. Nature to the Idea is necessarily the other of itself, and serves as a passage toward the free conscious spirit.54 As Schmied-Kowarzik remarks, when the Idea releases itself from nature as its other to the self-grasped spirit, it necessarily sublates the last externality of nature, which leads to the 
“death of natural being” (“Tod des Natürlichen”): 
Superseding this death of nature, proceeding from this dead husk, there rises the finer nature of spirit. Living being ends with this separation, and this abstract coincidence with itself. . . . The purpose of nature is to extinguish itself, and to break through its rind of immediate and sensu- ous being, to consume itself like a Phoenix in order to emerge from this externality rejuvenated as spirit. Nature has become distinct from itself in order to recognize itself again as Idea, and to reconcile itself with itself.55 
We will take a pause here regarding Hegel’s philosophy of nature, since the death of nature is the affirmation of logic. A superficial read- ing leads to an opposition between the ecology of Schelling and the anti-ecology of Hegel. It is probably more appropriate to see that in this passage from nature to spirit, the death of nature doesn’t mean that there are no trees, animals, or bacteria, but rather that nature is recognized as the other of the Idea in the whole, and therefore “consumes itself like a Phoenix in order to emerge from this externality rejuvenated as spirit.” The death of nature is the birth of spirit. This is an extraordinary expres- sion of Hegel’s humanism. Instead of seeing spirit as part of nature, he sacrifices nature to spirit in order to release nature. 
We will stop on this point concerning the relation between recur- sivity and contingency, which we have attempted to characterize as Hegelian. Let us recapitulate what we have done so far: In Schelling’s and Hegel’s Naturphilosophie, contingency is enabled and overcome by two recursive models. In Schelling’s early philosophy of nature, contin- gency is the expression of freedom and nature. In Hegel’s philosophy of nature, contingency is a test for the auto-determination of the Notion. If in Schelling we see an allure toward a reenchantment of nature, or even a Romanticization of nature, it is, however, not sufficient. Instead we propose to see in Schelling’s Naturphilosophie a proto-ecology or even a proto-organicism that is moving toward a systems theory in the sense of Bertalanffy. Contrary to biological organicism, spirit and nature have a different relation in Hegel. Here the progress of the spirit is also the progress toward the death of nature. In this sense, Hegel is probably a step closer to cybernetics, or mechanical organicism.56 In the early twentieth century, recursivity is formalized and systematized in cybernetics and other, parallel developments: for example, computa- tion theory (Gödel-Turing-Church) and automata (John von Neumann). Then it arrives at artificial intelligence (AI), machine learning, and more complex forms of automation. Alternatively, paraphrasing Hegel, maybe one can say that this machinic organicism characterizes the new form of the absolute spirit of our epoch. 
The central question for cybernetics, as Gotthard Günther has famously claimed, is as follows: “[T]he question is not what life, consciousness, or self-reflection ultimately is, but: can we repeat in machines the behavioral traits of all those self-reflective systems that our universe has produced in its natural evolution?”57 The reflective mechanism in Hegel seems to Günther a perfect model upon which the “second machine” should build and provides a theoretical founda- tion for articulating the soul and the machine. Following Hegel, he claims that after idealism one knows that being is not reflective; it limits itself to itself, but the form of consciousness is reflective and goes beyond itself to interact with the environment.58 Günther associ- ates self-consciousness in idealism with the project of cybernetics, not only because he was a Hegel scholar, but also because of the change in the epistemological paradigm after Kant’s formalization of the con- cept of the organic—a point we have emphasized many times. In this sense we can also understand what Heidegger says concerning Hegel: 
“[T]he completion [Vollendung] of metaphysics begins with Hegel’s metaphysics of the absolute knowledge as the Will of the Geist.”59 We can raise another question along the lines of Heidegger’s verdict: Isn’t the systematization of the recursive in cybernetics the new condition for philosophizing, and no longer the same as in the time of Kant? This means that the completion of metaphysics is also the realization of it in cybernetics, and it therefore demands a new task of philosophizing. Metaphysics concerns the understanding of being as such and beings as a whole: the existence of being, for example, this crystal before me and the plant outside the window, a particular being; and the existence of all beings in their totality. The pre-Socratic philosophers, especially the Ionian naturalists, wanted to grasp being as a whole, but they remained premetaphysicians or nonmetaphysicians, as Heidegger calls them. Metaphysics seeks to reconcile being and becoming, the particular and the universal. It is in Plato and Aristotle that metaphysics took shape, with its full explanatory power of being, through a theory of form and ontology, which Heidegger calls onto-theology, and that was later fully combined with Christian theology. The organic is an epistemological concept, but it is also a metaphysical concept precisely because it is able to integrate both mechanical and living beings, as Heidegger has sharply pointed out: 
The fact that in the age of the absolutely unconditioned “organization,” the ready-made disposable arrangement [Einrichtung] of all beings, the “organic” must become the only one addressed and proclaimed, shows only that now the “mechanical” in the broad sense of the plannable work- able [Machbaren] and the “living” have stripped away the long-held gleam of a difference between them.60 
We have to pay attention here that for Heidegger, the organic is not opposed to the mechanical; instead, it is fundamentally mechanical- technological, as Heidegger already noted toward the end of the 1930s in the so-called Black Notebook that “it might very well still take a con- siderable time to recognize that the ‘organism’ and the ‘organic’ present themselves as the mechanistic-technological ‘triumph’ of modernity over the domain of growth, ‘nature.’”61 
This metaphysical task—to grasp being as such and beings as a whole—is not simply a theoretical discourse. It is not something in our mind, but rather a will. This will changes the way the world is known, felt, experienced, and constructed. The metaphysical will subordi- nates itself to the transcendence of the divine and becomes a means of theodicy. Yet God is not necessarily a mysterious transcendence, but rather the ground of rationalization, as we have found regarding the various proofs of the existence of God. The pantheism of Spinoza is a way to unite the divine with nature (deus sive natura), in which the divine is immanent in the substance that it produced. It is not until Nietzsche that God is murdered and a new ground of rationality has to be constructed—a ground that is at the same time a ground and an abyss. In cybernetics, Heidegger sees a totalizing force tantamount to an exclusive rationality. Therefore, Heidegger was looking for the other thinking, which no longer bears the name philosophy. It matters little for us whether it is called philosophy or something else, but Heidegger sees the necessity of taking another trajectory of thinking in order to think about another beginning. Heidegger wants to take another path, which I formulated as cosmotechnical thinking.62 However, he wants to do this without cybernetics, meaning that he wants to find a beginning that is already pronounced but not yet thought. 
In order to understand in what sense cybernetics is the end of philoso- phy, we will still need to understand what cybernetics is and how the organicist and reflective logic of Hegel stands as a precursor to cyber- netics. It is not possible to directly map Hegel’s reflective logic onto cybernetics as if the latter is a theoretical application of the former, but it is possible to see how reflection complexifies itself. We will have to reconstruct a philosophy of cybernetics, which is not really a discipline but rather a movement with enormous richness and heterogeneity of ideas. We will need to understand the cybernetics project as a whole. We will elaborate on the “advancement” of mechanism as recursive algorithm by following the work of Kurt Gödel and Alan Turing as well as the development of cybernetics in the twentieth century, which has formulated the organic in terms of a more general model with two key concepts—feedback and information—as elaborated by the mathemati- cians Norbert Wiener, Claude Shannon, and others. We will also trace the development of first-order cybernetics into second-order cybernetics via the systems theory of Ludwig von Bertalanffy, Heinz von Foerster, and Niklas Luhmann, as well as Gilbert Simondon (who attempts to reformulate the concept of feedback in a different fashion) and others 
§20. GENERAL RECURSIVITY AND TURING MACHINE 
Hegel starts from logic and ends at logic, since the Notion acquires its universality and absoluteness only through laborious reflections: immediacy, negation, negation, and reproduced immediacy (wieder- hergestellte Unmittelbarkeit). Günther has attempted to construct a non-Aristotelian logic (meaning a multivalued logic) through the for- malization of Hegel’s reflective logic. An Aristotelian logic, or classical logic, according to Günther, presupposes the metaphysical identity of thought and being.63 If we refer to the language of Schelling, the clas- sical logic rests on A = B, and did not yet arrive at A2 = (A = B), not to mention A3 = A2 = (A = B). That is to say that, in German idealism there is a self-consciousness as reflection that elevates the A = B to a higher order of logic. Günther spoke a more Hegelian language than a Schellingian language, since the Hegelian reflection, as we have seen, is more concrete and structural.64 In his seminal work, The Conscious- ness of Machines: A Metaphysics of Cybernetics, Günther claims that “[i]n cybernetics, finally, the idea of Hegel, that reflection is essentially a real process, is made serious when we systematically attempt to trans- fer processes of consciousness in analogy to machines. 
If the subject-object relation represents a two-valued logic, Günther sees in Hegel’s reflective logic the possibility of a three-valued log- ic.66 He understands the evolution of machines as a progress toward the Hegelian logic: A classical machine is a Reflexion in anderes, a von Neumann machine is a Reflexion in sich, but a “brain machine” is a Reflexion in sich der Reflexion in sich und anderes of the “brain machine,” as “Hegel says in the greater Logic.”67 Let’s illustrate this with Günther’s own example. We have I, which is the self, then we have R, which is the first reflection, namely, subjectivity; now we can have a double reflection D, then with these three values we can compute a truth table of three values: IR, ID, DR.68 Or, to generalize Günther’s construction, he defines different R-levels, the 0th R-level being one without self-consciousness, while the next R-level takes the 0th level as its object, which could be schematized as: Ss ? (So ? Os). As Charles Parsons writes: 
Every R-level . . . can itself be object of a further reflection. Thus the iteration involved is infinite. Günther draws the further conclusion that it cannot at this point be characterized axiomatically, as a new logic would require, “because it is in no way possible to arrive at final, most general propositions about this open subject and to define it as self-consciousness.”69 
What is spotted here in Günther’s logic is an extension of the Hegelian reflection to multiple levels, where another reflection is always possi- ble. Günther started with a very convincing proposal for a many-valued logic, but in The Consciousness of Machines and other writings it is not very clear that his formal logic could be fully realized by cybernet- ics, and at points it seems that cybernetics exhibits a greater flexibility. However, one point is clear: that Günther, with his elaboration of Hegelian reflective logic, has attempted to structuralize the mechanism of reflection as Hegel himself tried to do. We will see how Günther’s metaphysics of cybernetics is significant by strolling through the work of Gödel and Wiener, as well as that of Gregory Bateson and Heinz von Foerster. We start with Gödel instead of Wiener since Gödel’s recur- sive function represents the fundamental mathematical basis for the implementation of feedback in modern computation machines. Moving to Wiener later will allow us to understand the broad implications of recursive thinking. 
In 1952 Günther initiated correspondence with Gödel and discussed his philosophy of logic with him. Gödel, who had been a reader of Kant’s Critique of Pure Reason since he was sixteen, had also read some Hegel and Schelling. In the letter to Günther dated June 30, 1954, Gödel had already marked his interest in German idealism as a pos- sible correction of metaphysics: “The reflection on the subject treated in idealistic philosophy (that is, your second topic of thought), the distinction of levels of reflection, etc., seem to me very interesting and important. I even consider it entirely possible that this is ‘the’ way to the correct metaphysics.”70 A few years later, in 1957, as Charles Par- sons has noted, in a letter from Gödel to Günther dated April 4, Gödel again raised his interest in the “total reflection” that was once promised by Günther, but absent in his recent work.71 This general interest of Gödel in reflective logic seems to be underelaborated in the literature. Gödel’s trajectory in philosophy has been documented by several authors, including Hao Wang,72 Mark van Atten,73 and Charles Par- sons. His interest in Plato, Kant, and Leibniz, and his 1952 turn toward Edmund Husserl’s transcendental idealism as a possible foundation of the sciences has been much discussed. It is therefore not our intention here to reiterate what has been said elsewhere but rather to attempt to connect Gödel’s interest in reflection with the concept of recursivity he developed in the 1930s. We will attempt to reconstruct Gödel’s notion of recursivity and its relation to computability or decidability by revisit- ing the history of computational theory. We will not be able to recount the whole history here, but only the essential steps toward its definition. It is of great interest here, though it sounds somewhat speculative, to understand recursion as a realization of the idealist project. Only after we have worked out a precise concept of recursion in computation will we be able to move into the discussions of second-order cybernetics. In order to do so, however, we must firstly broaden our understanding of the term logic, which is also why Hegel was introduced in the first place. Gödel definitely holds a broader view of logic in comparison to how logic is conventionally understood, in terms of syllogisms and logical inferences. Commenting on Hegel’s logic, Gödel told Wang: 
Hegel’s logic need not be interpreted as dealing with contradictions. It is simply a systematic way of obtaining new concepts. It deals with being in time. Not Hegel’s logic but some parts of it might be related to a proposition (not concept) producing its opposite. For example, if A is defined as in Russell’s paradox [namely, A is the set of all sets that do not belong to themselves], “A belongs to A” produces its opposite. In Hegel, a condition produces its opposite condition in history: that is a process in time, and truth depends on time. Hegel’s interpretation is like the figures in a puppet show; the second beats the first down. In terms of the unity of opposites and the idea that contradiction gives direction, antinomies receive a different interpretation. The Russell set becomes a limiting case of a succession of belonging-to and not-belonging-to; it is no longer circular.74 
Gödel sees in Hegel’s logic the crucial place of time, meaning that Hegelian logic is temporal, since it is fundamentally a series of reflec- tions, and if we base the question of truth on time, then it opens a different experience of logic. It is probably this unconventional under- standing of logic that led Gödel to expose the limits of formal logical systems, namely, in his famous 1931 paper on the incompleteness of formal logical systems exemplified by Bertrand Russell and Alfred North Whitehead’s Principia Mathematica. This logical proof sends Gödel back to the necessity of mathematical intuition, which constantly reformulates the development of mathematics.75 What interests us in Gödel’s proof is his ingenious invention of a method that first turns axioms into strings of natural numbers and consequently allows him to arithmetically prove the given proposition through calculation. This leads to the discovery and formal definition of the recursive function and later general recursive function, the equivalence of the universal Turing machine. The arithmetization of symbolic logic and the auto- mation of mathematical proof through calculation was a great leap in modern science. 
The mathematical development of recursivity and its realization in the universal Turing machine during the 1930s witnessed the emergence of what we call an algorithm. Many people, including computer scientists and social scientists, often refer to recipes when they explain what an algorithm is. This is not completely wrong, since an algorithm does specify certain procedures and rules that have to be followed, but it is also absolutely not right, since a recipe cannot explain at all what an algorithm of our time is. Hence, I would like to propose that algorithmic thinking should be understood in relation to the concept of recursion or reflection. A recursive function simply means a function that calls itself until a halting state is reached. Doug- las Hofstadter, in his Gödel, Escher, Bach: An Eternal Golden Braid, explains the concept with a joke: Imagine that a German professor has to give a lecture in one long sentence with a lot of Nebensätze; at the end he will only have to pronounce verbs in order to complete each interaction. 
First, Richard Dedekind, in his 1888 essay “Was sind und was sollen die Zahlen?” formally used recursive functions to define operations and natural numbers. This motif was taken up again by the mathematician Thoralf Skolem in 1922 to reconstruct the logical system of Whitehead and Russell’s Principia Mathematica. Skolem’s radicality is that he removed the existential quantifiers used by Russell and replaced them with functions. This is now known as Skolemnization. Literally, we can also read that the question of existence is no longer a question, since it is, in the end, a mathematical function.76 
Second, Gödel defined recursive functions in his 1931 paper titled “On Formally Undecidable Propositions of Principia Mathematica and Related Systems I,” and general recursive functions later in his 1934 paper “On Undecidable Propositions of Formal Mathematics.” In his 1931 paper, in contrast to the formalization of symbolic logic, Gödel arithmetized the formal systems with numbers (now known as Gödel numbering) so that the relations between different axioms could be expressed numerically, as we can see in Table 2.1: Each symbol, including bracket, is given a natural number. 
This allows Gödel to turn proofs from logical inferences into com- putation consisting of recursive functions. In his 1931 paper, Gödel defines recursive functions as follows: 
A number-theoretic function Ø is said to be recursive if there is a finite sequence of number-theoretic functions Ø l, Ø2, . . . , Øn that ends with Ø and has the property that every function Øk of the sequence is recursively defined in terms of two of the preceding functions, or results from any of the preceding functions by substitution, or, finally, is a constant or the successor function x+1.77 
We may want to simplify this in the form of F(x) = k F(x-1), in which the function goes back to itself, as well as the result of the preceding function. It is for this reason that we can associate it with Gödel’s inter- est in Günther’s proposal for a reflective logic. In 1934 Gödel introduced the general recursive function in his paper “On Undecidable Proposi- tions of Formal Mathematical Systems.” Gödel was aware (through his correspondence with the French logician Jacques Herbrand) that his primitive recursive functions didn’t include all effectively calculable functions. In section 9, “General Recursive Functions,” Gödel gives a similar definition of recursive function: 
One may attempt to define this notion as follows: If Ø denotes an unknown function, and ?1, . . . ?k are known functions, and if the ?’s and Ø are substituted in one another in the most general fashions and certain pairs of the resulting expressions are equated, then, if the resulting set of functional equations has one and only one solution for Ø, Ø is a recursive function.78 
The second definition is more abstract, and involves another order of complexity since it involves two functions, but it also highlights the three main perspectives of the theory of recursion: (1) That which can be represented in terms of recursive functions is recursively enumer- able, namely, it is computable; (2) it starts with a simple function in order to arrive at a complex one, often illustrated by the phenomenon of emergence; and (3) it presupposes the unknown in order to produce the known, meaning that the black box cannot be known exactly but can be substituted by other known functions whose recursive operation may arrive at the same result as the unknown function.79 The following example that Gödel gives may help to understand it: Third, following the publication of Turing’s 1936 paper and Ste- phen Kleene’s (student of Alonzo Church) further definition of gen- eral recursivity after Gödel—for example, the Fixed Point Theorem80 and Normal Form Theory81—Church, in a 1937 review of Turing’s 1936 paper “On Computable Numbers, with an Application to the Entscheidungsproblem,” claims that the Herbrand-Gödel-Kleene gen- eral recursive function is equivalent to Turing’s universal machine and Church’s Lambda definable functions.82 We may be able to say that for Turing, what is computable or decidable is always recursively enumerable, even though some computer scientists (such as Rob- ert Soare) propose to use decidability and computability instead of recursion,83 for the latter carries various meanings in everyday use. However, that would miss the importance of the recursive form cen- tral to the mathematical and philosophical understanding. A recursive function may not attain its goal, and hence it doesn’t halt. In this case it gets lost in its infinite looping until it has used up its computational resources, such as the exhaustion of memory or even the physical fatigue of the machine. However, this undecidability is what a uni- versal Turing machine wants to determine, hence to avoid such an infinitive looping. 
Let’s consider the example of computing the Fibonacci number (1, 1, 2, 3, 5, 8, 13, 21 . . .), in which the next number is the sum of the two preceding numbers. Given a number N, we want to see all the numbers in the series from 1 until N, the recursive function calls itself, and enters a spiral operation until it arrives at its halting status, that is, when the value of the number N becomes 0; N = 0 or N = 1 is an indicator of the telos. 
It is worth mentioning that the Fibonacci number was also a source 
of inspiration for Turing’s later paper “The Chemical Basis of Morpho- genesis,” in which Turing suggests that it may be possible to understand morphogenesis as a recursive process of pattern generation. Turing proposes a “chemical embryology,” inspired by D’Arcy Thompson’s analysis of biological forms, to “account for the appearance of Fibo- nacci numbers in connection with Wr-cones.”84 Turing’s algorithm (reaction-diffusion) has succeeded in showing that it is possible to generate patterns and forms that assemble life,85 like the singling out of formal cause and final cause as the causal model of computational biol- ogy. One may want to claim Turing as a computationalist who wants to reduce biology to computation, but one would thereby miss the point that it is an opening up of a new epistemology. At stake is how is one is going to proceed with this new mode of knowing, and how this mode of knowing should contribute to the understanding of life. We will be able to tackle this question only in later chapters. 
To reiterate the concept of digital recursion in a simplified way: Recursion here means that a function calls itself in each iteration until a halting state is reached, which is either a predefined and executable goal or a proof of being incomputable. This notion of recursivity has to be further extended from mathematical proof to wider applications. This can be a mathematical proposition, software or a system like Google, or even a living being recurrently interacting with its living milieu. The realization of this general recursive thinking is the rise of what I term algorithmic thinking. Contrary to automation considered as a form of repetition, recursion is an automation that is considered to be a genesis of the algorithm’s capacity for self-positing and self-realization. Gödel is not a Hegelian since he rejects—in a way of misunderstanding—the Absolute by saying that “there is no absolute knowledge, everything goes only by probability.”86 However, reflection is considered to be a way of moving toward higher-order logic. With the notion of recursiv- ity and its realization in the mechanism of the Turing machine, it seems to be a decisive moment in the history of technology, one that assimi- lates the concept of the organic and, with its constant amelioration and breakthrough, constitutes a new condition of philosophizing two centu- ries after Hegel and Schelling. However, we are not rushing to the end now. It is necessary next to investigate further how the concept of the organic is further appropriated in the context of early twentieth-century cybernetics. 
§21. WIENER’S LEIBNIZIANISM 
Unlike Günther, Wiener did not employ Hegelian language, but instead named Leibniz as the patron saint of cybernetics. For sure, Leibniz is no Hegel, but they share reflection on the organicity of being and its operation in reflection; this is also why Günther identifies Wiener’s feedback with Hegel’s reflection. This does not mean that Leibniz was the first to ponder upon mathematics and logic. Descartes before him had already constructed a theory of automata. However, in Leibniz one finds a mathematical and physical account of a reflective model that folds the infinite into the finite being, like the best of all possible worlds hypothesis and the monadology. Wiener writes: 
If I were to choose a patron saint of cybernetics out of the history of sci- ence, I should have to choose Leibniz. The philosophy of Leibniz centers about two closely related concepts—that of a universal symbolism and that of a calculus of reasoning. From these are descended the mathemati- cal notation and the symbolic logic of the present day. Now, just as the calculus of arithmetic lends itself to a mechanization progressing through the abacus and the desk computing machine to the ultra-rapid computing machines of the present day, so the calculus ratiocinator of Leibniz con- 
tains the germs of the machina ratiocinatrix, the reasoning machine.87 
Wiener names Leibniz as the first thinker of universal symbolism and calculus of reasoning, but this remains very vague and general. What exactly is Leibniz’s contribution to cybernetics? By exploring this ques- tion we will be able to ground a philosophical investigation of cybernet- ics and connect it with what we have discussed so far regarding German idealism. 
As we have already seen in the introduction, Joseph Needham regarded Leibniz as the first thinker of organic philosophy in the history of Western philosophy. It is no coincidence that in Leibniz the thinker of the “reasoning machine” and the philosopher of the “organic” come together; this combination is the real reason that Leibniz is the patron saint of cybernetics. Leibniz’s corpus is far beyond our efforts to exam- ine here, but we would like to outline some important ideas in Leibniz’s contribution as they are interpreted by Wiener. It should noted here that Wiener is a careful reader of Leibniz. We would like to demonstrate two important elements in Leibniz’s cybernetics: (a) the theory of the combinatorial and (b) the mathematical model of the infinite in the finite. Both elements are core to his Monadology, since there the world is composed of monads that see the world according to a point of view giving rise to various combinations; monads have mirrors that reflect the world in itself, which generates a richness far beyond the limit of rules. Leibniz’s 1666 doctoral dissertation, De Arte Combinatoria, is a proposal to understand human thinking, such as ideas, as different combinations of signs. The human mind is a set of operations based on combinations of symbols, and reasoning is the organization of these operations. It is noted that for Leibniz, thinking is calculating, which is analogical to symbolic operations and also to a large extent relies on them. This is also why he leveled the criticism at the Cartesians that they think much more with signs than they believe, and that therefore signs for them remain unthought (impensé).88 The combinations are not random, they have a reason to be as they are, and this is the principle of sufficient reason: Nihil est sine ratione, nothing is without ground or reason. The formulation of thinking in characters aims to “eliminate controversies,” “especially to exterminate the controversies in matters that depend on reasoning, because then reasoning and calculating will be the same.”89 
Concerning the relation between the infinite and the finite, this is fundamental to Leibniz’s analogy between symbolic operations and thinking, meaning the inscription of the infinite in the finite. To Des- cartes, for reason to be perfect it should contain finite steps so that the mind can move along them; the contribution of Leibniz is to inscribe the infinite in the finite, so that the mind is able to comprehend the infinite by means of finite symbols. For example, the famous irrational number π can be represented by Leibniz as a formula with finite symbols: 1 - 1/3 + 1/5 - 1/7 + 1/9 - . . . = π/4 
The way that the infinite is expressed in the finite is the fold, if we recall Gilles Deleuze’s treatise on Leibniz here. This is at the center of his notion of the “individual substance” developed in Discours de la métaphysique, and later on the “monad” (simple substance) in the Mon- adology. Leibniz rejected the Cartesian definition of substance as exten- sion in space; instead, he incorporates a dynamic in the substance. The individual substance does not contain the predicates of all substances, since only God can do it, but rather it contains in itself predicates that express the world according to its point of view. A monad, we are told, does not have a window; rather, it possesses a strange apparatus: a mir- ror. In the Monadology the mirror is the key to the understanding of such points of view, and it is also a mechanism of interiorization. We may want to relate this mechanism of the mirror to the concept of recur- sion. The monad is in the world, and “in the world” means that there is something outside of it. If the monad is able to look at itself, like the “I” looking at the “me,” then it is outside of itself, so there is a contradic- tion. On the contrary, if all relations are already contained in the notion of individual substance, defined by the unity and absolute discernment, and not by its metaphysical condition of existence, as Louis Couturat says,90 then we will not be able to grasp the sophistication of Leibniz’s thought. Leibniz has integrated this negative metaphysical condition by reflecting it into the monad, through which the “in the world” is not a condition but the correlate of the monad’s expression of the world. 
If we attempt to understand Leibniz’s logic as formal logic—that is, merely in terms of inferences between logical propositions—we are far behind Leibniz’s own thought; we are still within Cartesian mechanism. In Leibniz, the monad is not “inorganic” or mechanical, it is organic and relational; in fact, no substance is inorganic for Leibniz. The Cartesians reject the notion of substantial form since it is the legacy of the Aristotelian concept of the soul—in De Anima, Aristotle draws an analogy between soul and form, as dynamnis—that implies that one has to endow stones and plants with a soul, a thinking subject. Leibniz on the contrary proposes an identification and unity between being and one, which demands an analogy of the soul. This perhaps reminds us of what Günther says regarding the foundation of cybernetics: self- consciousness as reflection. 
Wiener’s 1948 Cybernetics Communication and Control in Man and Animals starts with a refreshing chapter titled “Newtonian and Bergso- nian Time.” What does he want to convey here regarding Newton and Bergson? Wiener wants to set up an opposition between two concepts of time: Newtonian, reversible, mechanical time, on the one hand; and Bergsonian, irreversible, biological time on the other. In Creative Evolu- tion, Bergson argued that mechanism failed to understand the “concrete time” of “real systems,” instead constructing “artificial systems” of “abstract time.”91 But Wiener’s ultimate aim is to argue that Bergson’s criticism of mechanism is losing its efficacy today, since mechanism has followed a different path since the discovery of quantum mechan- ics and statistical mechanics in the line of James Clerk Maxwell–Lud- wig Boltzmann–Willard Gibbs. The significance of Boltzmann-Gibbs statistical mechanics in relation to Newtonian laws is that it distances itself from the determinism of Newtonian mechanics and sees laws of nature as statistical, meaning that it puts contingency at the center of nature; as François Jacob writes, “[I]n the second half of the nineteenth century, several so-called laws of nature became statistical laws.”92 In other words, statistical thermodynamics renders order and chance com- patible.93 The microcosm of the quantum world assimilates the vitalist view of time that Wiener attributed to Bergson. The uncertainty and the indetermination of the quantum activities discovered by Heisenberg and the wave-particle duality discovered by Louis de Broglie, together with statistical mechanics, suggested to Wiener the establishment of a new scientific foundation that effectively surpassed the classical opposition between mechanism and vitalism. 
For Wiener, this new scientific foundation is a realization of Leib- nizian metaphysics. Leibniz, the contemporary of Newton, had inde- pendently arrived at the discovery of differentiation, but with rather different metaphysical presuppositions. Leibniz’s theory of individual substance, its predicates, the mirroring relations, and the dynamics of force seem to Wiener to march hand in hand with the new physics. Furthermore, Leibniz had already provided the application of such metaphysics, which is the precursor of cybernetics. We may want to understand this in terms of the following two points—even if, for some Leibniz scholars, these may sound like bad analogies. First, the mirror- ing activity of the monad gives rise to vague impressions, which Wiener relates to the uncertainty of quantum activities: 
As I have said, some of the Leibnizian monads mirror the world more clearly, some less clearly. This lack of clearness in mirroring is respon- sible for our impression that there is chance and indetermination in the world. Now, in the modern quantum theory, the indetermination which is an essential feature of the world, as represented in the ordinary four dimensions of time and space, is resolved, according to Heisenberg, if a sufficient number of additional, unperceived dimensions are superadded.94 
Wiener thus understands an electron as a monad; he continues: 
Thus, each electron possesses its own world of dimensions, which mirrors the many-dimensional universe of perfect cause and effect in an imper- fect, four-dimensional, non-causal image. It is surely not fanciful to see in this a parallel to the Leibnizian monads, which live out their existences in a self-contained existence in pre-established harmony with the other monads, yet mirror the entire universe.95 
Wiener has another reason to draw an analogy between electrons and monads. In a short commentary titled “Quantum Mechanics, Hal- dane, and Leibniz,” responding to another article from Haldane96 titled “Quantum Mechanics as a Basis for Philosophy,” Wiener suggested that “[t]he paper might indeed be called, ‘Leibniz as a basis for Quan- tum Mechanics.’” The argument of Haldane is that quantum mechanics, especially the wave-particle dualism discovered by de Broglie, suggests a new way to understand the body-mind problem of inanimate beings. For an inanimate being—for example, a rock—at the macrophysical level it ceases to repair, but at the microphysical level we can see that electrons attempt to repair: For example, when an atom loses an elec- tron, it will get it from somewhere else. George Gale summarized this point precisely: “Each material system has an associated system of de Broglie waves in its phase space, travelling faster than light. The more organized the matter, the greater the quantum mechanical degeneration, i.e. drop in the number of degrees of freedom, and the more resonating the wave system.”97 
This is the reason that Wiener identified the particle-wave duality with the body-soul relation proposed by Leibniz, which Wiener relates to the preestablished harmony that Leibniz developed in Le système nouveau de la nature (1696). In the Cartesian model, this relation takes the form of a causal interaction, meaning that they accord with each other according to mutual action. In the Occasionalism of Nicolas Mal- ebranche, God tirelessly reacts to every instant of the soul, a veritable deus ex machina. In Leibniz’s model, on the other hand, the body and the soul are replaced by monads that, as Wiener points out,98 are based on preestablished harmony, like two clocks marking the same hour, harmonizing with each other according to their respective mechanisms. 
By comparing monads with electrons, Wiener sees that Haldane is actu- ally assimilating the living organisms to particles; he therefore refers to Leibniz as the base of quantum mechanics. 
From this perspective we can see that Wiener’s cybernetics, as well as his ambition to overcome the opposition between Bergsonian creative time and Newtonian mechanical time, is largely Leibniz- ian in inspiration; he also uses Leibniz’s monadology to incorporate Haldane’s organicism. He situates this overcoming within a history of technological progress, from clockwork (mechanical machines) to the steam engine (thermodynamic machines) to twentieth-century informa- tion machines: “If the seventeenth and early eighteenth centuries are the age of clocks and the later eighteenth and the nineteenth centuries constitute the age of steam engines, the present time is the age of com- munication and control.”99 
The seventeenth-century philosophy of Leibniz is not rendered obsolete in the course of this historical progression. On the contrary, Leibniz’s theory of communication is central to the cybernetic project, since communication is fundamental to the monads, even though they don’t have windows but only mirrors.100 Mirrors are more effective than windows, since mirrors reflect, and they can reflect into infinity with limited resources. Among the monads there are circulations of impres- sions, messages, and functions that are intrinsic to the substance. We can therefore notice that Wiener wrote with a Leibnizian tone when he described contemporary automata: “We deal with automata effectively coupled to the external world, not merely by their energy flow, their metabolism, but also by a flow of impressions, of incoming messages, and of the actions of outgoing messages.”101 
Leibniz’s philosophy is contemporary to the age of quantum mechanics, and with the discovery of quantum mechanics the world described by Leibniz could be repositioned, if not realized, through cybernetics. Wiener mobilizes Leibnizianism through the lens of the modern sciences (quantum mechanics and the neurosciences). The opposition between Newtonian mechanics and the quantum mechan- ics of Niels Bohr and Max Planck (which, according to Wiener, forms a “Hegelian antinomy”) is resolved by the statistical mechanics of Maxwell-Boltzmann-Gibbs102: 
This transition from a Newtonian, reversible time to a Gibbsian, irre- versible time has had its philosophical echoes. Bergson emphasized the difference between the reversible time of physics, in which nothing new happens, and the irreversible time of evolution and biology, in which there is always something new.103 
Statistical mechanics, which uses Newtonian mechanics with the insights of quantum mechanics, renders every definite movement into a set of probabilities. It seems to Wiener that the oppositions between vitalism and mechanism, irreversible time and reversible time, are not real oppositions—at most they belong to different orders of magnitude, while it is possible to establish a theory of the relations and dynamics between these two different orders of magnitude.104 This leads to Wie- ner’s refusal of Bergson’s vitalism in a very decisive way: In what way can Bergsonian time not be realized by modern automation? “The mod- ern automation exists in the same sort of Bergsonian time as the living organism, and here there is no reason in Bergson’s considerations why the essential mode of functioning of the living organism should not be the same as that of the automation of this type.”105 
Wiener is referring to the concept of feedback, which he presented with Arturo Rosenblueth and Julian Bigelow.106 What is feedback? It simply means that the difference between the output and the expected output is fed back to the system in order to improve the operation. For example, when we reach out our hand to grasp an object, there are multiple feedback loops taking place among the muscles, motors, and perception, and this feedback allows us to adjust our positions and gestures. It also occurs in technical objects, like the example given by Rosenblueth: Torpedoes possess a target- seeking mechanism by following the magnetic pull of the hull of the ship or the submarine or the sound of the propeller.107 It also occurs in neuro-activities: As observed by Warren McCulloch and Walter Pitts, the neuro-network is a net of loops that are beyond abstract logical operations.108 
Feedback here means reflection, a circularity between a being and its environment, a nonlinear movement of self-adjustment toward a purpose or telos that defines the whole. This is also why we character- ize cybernetics as a “mechanical organicism.” It is unlike a Cartesian causal chain, which we can visualize as a linear propagation from one proposition to another. Wiener refers to the first feedback system as Watt’s governor of the steam engine, which is able to regulate its velocity according to different conditions of load.109 A more contem- porary example is homeostasis, a concept described by the physiologist Claude Bernard and later coined by W. B. Cannon. Bernard, in his 1865 Introduction à l’étude de la médecine expérimentale, writes that “all the vital mechanisms, however varied they may be, have only one object, that of preserving constant the conditions of life in the internal environment [milieu intérieur].”110 Homeostasis is a mechanism that is able to keep the system within a certain range of constants: for example, temperature, the amount of potassium in the body liquid, and so on. Homeostasis is also used by the British cybernetician W. Ross Ashby to characterize life. Feedback here replaces the reflection of the monads and prompts Wiener to reject notions such as “life,” “vitalism,” and “soul”111: “It is my thesis that the physical functioning of the liv- ing individual and the operation of some of the newer communication machines are precisely parallel in their analogous attempts to control entropy through feedback.”112 
This analogy leads Wiener to define both organism and machine by a common telos, the resistance against “the general tendency for the increase of entropy.”113 For Wiener, the concept of feedback is not limited to technical objects and organisms; he also extends it to analy- sis in economy and other social phenomena. Commenting on what he calls “long time feedback,” he gives the example of Chinese idolatry, according to which the mandate of heaven is correlated to the destiny of the emperor and the dynasty—the suffering of the people due to wars and famines is an indicator that the emperor or the dynasty has lost the mandate of heaven, and therefore that it is destined to fall. Wiener emphasizes this as a feedback.114 With Wiener’s formulation, one can see feedback everywhere; it constitutes a new epistemology. 
§22. CYBERNETICS OF CYBERNETICS 
The concept of feedback, we would like to claim here, is a primordial form of recursion. Gödel’s recursive function and the Turing machine are feedback systems, which inscribe such mechanism in an algorithm with a predefined telos. One may argue that from the perspective of finality, a recursive algorithm is different from thinking, since think- ing does not have a predefined telos. This is both true and untrue at the same time. It is true since thinking doesn’t have to have an immediate telos like an algorithm, which is judged to be good since it can arrive at the telos in the most effective way, that is, measured by execution time. It is untrue since thinking is always thinking of something, this something is the object of intentionality, and to concretely grasp its existence is to comprehend its telos. For example, the gigantic object that Kant calls the natural end, or nature as a whole, is always already a telos, but such telos is not reachable with objective evidence. On the contrary, Kant tells us that this can be approached only through subjec- tive reason. This means that we will have to enlarge the concept of telos and its relation to recursivity, which is addressed in greater detail in so-called second-order cybernetics. Luhmann and Foerster, considered the chief representatives of second-order cybernetics, both use the word recursion instead of feedback, apart from when they make reference to Wiener. I extend the term recursion to include notions such as feedback and self-reference. 
Pierre Livet, in an article titled “La notion de récursivité, de la pre- mière cybernétique au connexionnisme,” suggests understanding the notion of recursivity in three stages. The first stage is associated with Wiener, McCulloch, Shannon, and others; the second stage, with Ashby and Foerster, and (later) Maturana and Varela; and the third stage with connectivism, for example, research on neural networks that employs a nonrepresentational scheme. If recursivity in first-order cybernetics is understood as what we have seen concerning feedback, in the second stage the concept of recursion is extended to other domains of study such as physics, biology, and the social sciences. In a 1947 article Ashby argued that it is possible to conceive the auto-organization of machines, which was often denied.115 What Ashby proposes is to consider the machine as a function identical to a set of variables. If one of the variables is a step-function of time, a spontaneous change of organization will appear to occur.116 Livet notes that for Ashby, auto-organization is the change of function according to the environ- ment (like the homeostasis), but Ashby seems to consider the relation between the environment and the machine as a composition of functions instead of as a single function.117 
Foerster distinguishes two types of machines, the trivial machine and the nontrivial machine. A trivial machine is synthetically determined, independent of the past, and analytically determinable and predicable.118 For example, we can consider a straightforward function: f(x) = x + 2. A nontrivial machine is synthetically determined, dependent on the past, and analytically determinable and unpredictable. A nontrivial machine is necessarily circular, but not merely repetitive; at the same time, it generates complexity departing from a simpler function. For Foerster, this circular form, recursion119—which he calls Eigenform120—is not a circus vitiosus but rather a circus creativus; it is in fact the foundation of epistemology based on observation. What an observer observes is not simply what he or she perceives, but rather a description of the percep- tion, namely, the description of the description. This does not mean that there are only two levels, the description and the description of it, but rather this reference to itself can occur many times until a fixed point, that is, the absorbing state (what is called reality), is reached. This can be illustrated with the commonly used example of the retina: 
First the retina provides a two-dimensional projection of the exterior world which one may call a “description of the first order.” The second post-retinal networks then offer to the ganglia cells a modified descrip- tion of this description; thus a “description of the second order.” And so it goes on via the various stations of computing all the way to descriptions of higher and higher orders.121 
Thus, when we consider epistemology again as the way of acquiring knowledge, we can see that it has to be recursive. This Eigenform seems to be applicable to different domains, including the social sciences. In an article dedicated to Luhmann on his birthday, “For Niklas Luhmann: ‘How Recursive is Communication,’” Foerster tries to demonstrate that communication is ontologically recursive, and, if we extend it further, all living systems and social behaviors are also recursive. However, unlike the recursive function in mathematics, which only deals with numbers, Foerster suggests that sociology also deals with functions of functions, or functors. Foerster has pushed the concept of recursion much further than others, and it was his aim to extend it to different areas such as linguistics, semantics, fields of action, and others. At the same time he lamented that “ironically all of the recursion topics have slid down into what today is called chaos theory . . . which one can sell graphically, numerically, and verbally to the New York Times.”122 
Livet suggests that in the work of Varela and Maturana there is a nuanced usage of the term recursive. The two biologists use it to describe the existence of internal relations between different subsys- tems in a system, analogous to how Wiener describes feedback, which they call structural coupling (organism and its milieu).123 We have to recognize, however, that Maturana and Varela did not agree with Luh- mann’s use of autopoeisis in his communication systems theory; they insist that the notion is only biological. The notion of autopoiesis comes from the study of the circular reproduction of the cell, which Maturana had studied.124 Poiesis in Greek is a synonym of techn??, which means to bring forth something as a product (ergon); autopoiesis is thus the operation of the system that is able to generate itself. Luhmann’s take on autopoeisis and operational closure (e.g., the production of structure) is a further systematization, which truly associates autopoiesis with techn?? in terms of exteriorization as communication (infra)structure. 
We want to refer here to an interesting discussion regarding auto- poiesis and idealism. This discussion merits our attention since the authors—Pierre Livet and Jean-Christophe Goddard—also see the question of recursion as fundamental to idealism, and indeed in Fichte and Hegel one can find different models of recursivity. In his earlier article “Intersubjectivité, réflexivité et recursivité chez Fichte,”125 briefly mentioned in the last chapter, Livet proposes to understand the movement of the I in Fichte’s philosophy in terms of recursion. He shows that the recursive process of the I and non-I constitutes a sort of endomorphism of the kind explored by Varela, the I and non-I form- ing a circular movement consisting of two moments, encountering and comprehension.126 Goddard thus suggests that for Varela, communica- tion is not about transmission, but rather a circular coupling between the I and its environment in which each reentering is indicated by the check (Anstoß). For Fichte, the check takes place at the moment of the encounter and resends the I back to itself in order to integrate this reflection. This non-I can be nature or other social beings, and for the latter the question of intersubjectivity arises. Reflection, according to Livet’s definition, is “recursivity which loops (boucle) immediately on itself.”127 We will see here two reflective moments, the non-I into the I, and these two moments as the object of another reflection. Seen in this way, Livet reformulates Fichte’s Absolute I as the recursive reflexivity itself: 
Fichte’s fundamental problem is to conceive reflexivity as such. How- ever, reflexivity goes beyond the self-consciousness, the subjectivity of the Ego: It is a more general structure, which can allow us to think the Absolute (with this Fichte leads us to Hegel, for whom the processes of thought, independent from the individual subject, are reflexive entities).128 
Livet distinguishes the recursive model of Fichte from that of Hegel, as we have attempted to do as well: a formalism in Fichte that is limited to the I and a more sophisticated logic of reflection in Hegel. Livet con- tinues that “contrary to Hegel, who gives a reflexivity that includes its other, and therefore can feed itself, Fichte wants to think the reflexivity by itself and therefore finds itself confronted with the problem of giv- ing it another consistency than that of a formal structure and without content.”129 
The gesture of Livet and Goddard of relating recursion in cybernetics to German idealism provides a fitting way to end our discussion starting with Hegel, since the similarity between the two lies precisely in the comprehension of organicity, which we have attempted to understand through the two categories of recursivity and contingency. According to Livet’s interpretation, Hegel’s recursivity is “superior” to Fichte’s since it is able to comprehend the whole, which includes the self and the other. But recursivity may take different dynamics, such as contin- gency, and may bear different meanings in different contexts. When one starts wondering whether Luhmann is in fact a modern Hegelian,130 it is an affirmation of Hegel’s reflective logic and his desire for system as the precursor to a universal cybernetics.131 Unlike Günther, Livet sug- gests a cybernetics surpassing self-consciousness. It doesn’t mean that Günther is wrong, but rather Livet did not posit cybernetic machines as quasi-organic, since he is aiming for systematic objective knowledge. But Livet is also right that reflexivity is beyond self-consciousness, since recursivity is not limited to individual self-consciousness: after all, there is thinking of thinking, and thinking of thinking of thinking. What makes the thinking enter into thinking of thinking and thinking of thinking of thinking? Information. 
§23. INFORMATION OF DIALECTICS 
We would like to proceed further by showing that the other key concept of cybernetics—namely, information—is fundamentally contingent and recursive. By introducing the concept of information, we would like to go beyond the seemingly rigid mathematical models. Let’s raise a somewhat weird question: What is the information of Hegel’s dialec- tics? If this question is possible, how can it be articulated? Information for Wiener and Shannon, as well as their precursors, is quantitative in the sense that it is a measurement of order and disorder. We know that for Wiener information is the measurement of order; therefore, more information means that the system is more ordered (less entropic and more negentropic). Information for Wiener is in this case closely related to probabilities. The increase in entropy means precisely a movement from the less probable to the more probable, as indicated by the famous Boltzmann’s equation: S = k log P.132 For Shannon, information is the measurement of “surprise”: An incoming event contains more infor- mation if it contains more surprise. For example, imagine guessing an English word starting with the letter S. If the incoming letter is z it contains more information than a, because a has more redundancy than z, meaning that there are numerous words start with Sa, but there are countable words starting with Sz. So for Wiener, information is opposite to entropy, while for Shannon, it is the inverse. 
Wiener’s information-entropy relation comes from thermodynam- ics.133 The second law of thermodynamics states that in an isolated system entropy cannot decrease, or stay constant in the case of a revers- ible transformation or equilibrium; in a spontaneous process, entropy can only increase, and the process is irreversible. For example, there is a transfer of heat from a hot body to a cold body, but a reversible process, a flow of heat from the cold body to the hot body, is impos- sible (this is also Rudolf Clausius’s critique of the Carnot cycle and Clausius’s definition of the second law of thermodynamics). Maxwell’s demon can be described in the following way: Take a container of a single type of gas particle, in which the particles are moving at different speeds. The container is divided into two chambers. Now imagine a tiny creature (later called Maxwell’s demon) that is able to open a valve on the shared wall of the two chambers in order to let the particles moving with faster speed to go to one chamber and those moving with slower speed to the other. Thus it appears that heat moves from the cold body to the hot body, which means the second law of thermodynamics is violated. What, then, is this demon, and how does it acquire informa- tion? Whether it is a question of light (Wiener), memory (Leó Szilard), or knowledge (Léon Brillouin)134 is not of our main concern here, but it means that it is possible to construct these demons to violate the second law of thermodynamics in order to resist becoming homogeneous and orderless, and to fight against the degradation of the universe. Herein lies the concept of negative entropy that Edwin Schrödinger developed in his What Is Life? to describe the self-preservation of life, and which was later abbreviated by Léon Brillouin as negentropy.135 As we saw earlier, Wiener also integrates this concept of the organism in his theory of feedback and information: “[T]he organism is seen as message. Organism is opposed to chaos, to disintegration, to death, as message is to noise.”136 
The statistical understanding of information is contested by Simon- don, who asks if one can understand information beyond probability.137 This is also the reason that, in L’individation à la lumière des notions de forme et d’information, Simondon suggests developing a nonprobabilis- tic theory of information in which information means incompatibility, disparation (or disparity), asymmetricity. Information is a difference that produces an effect in the individual (considered as a system), which may lead, or at least contribute, to a process of individuation. This disparation can be demonstrated with the example of the formation of an image on the retina. We know that the retinal images in both eyes are not identical. It is this disparation as information that allows an indi- viduation of the image to take place, that is, forming a unified image. 
The doubt one may have is the following: In cybernetics, is there really a lack of a nonquantitative theory of information? I think the best response to Simondon would be one that draws upon Donald MacKay and Gregory Bateson, since for both of them information is precisely qualitative; it is a difference with affection.138 With Bateson it becomes even clearer that information is recursive—it is recursive precisely because it is a difference that makes a difference. How are we going to understand this recursive sentence? Before we approach it, it is impor- tant to note that Bateson coins his method sometimes as recursive epis- temology, sometimes as ecological epistemology.139 The term recursion is decisive and almost equivalent to “ecology,” since it is that which allows the whole to progress, that is, a holistic movement as well as the interaction of the parts. To say it in Bateson’s own words, recursion is central to the definition of autonomy: “Autonomy—literally control of the self from the Greek autos (self) and nomos (a law)—is provided by the recursive structure of the system.”140 
For Bateson, information is first of all difference; such a difference is produced by patterns. This is why Bateson believes that he has invented a new information theory, not against redundancy but rather depend- ing on redundancy.141 It does not aim to eliminate noise, but rather sees contingency as necessary. In Steps to an Ecology of Mind, Bateson evokes the famous phrase of Alfred Korzybski—“the map is not the territory”—to explain this difference. In an intuitive sense a map is different from a territory because the map is only an abstraction of the territory; it is the expression of relations that define the territory. But for Bateson, Korzybski’s proposition has a historical meaning, since it is an inquiry into patterns. European thought, Bateson contests, privileges substantial thinking by asking, for example, what makes up of our earth and air—with an exception in Pythagoras, who asks what is a pattern.142 Every repeated event is always singular in the sense that it demands a recontextualization that Korzybski calls “time-binding.” Thus the same pattern in its repetition can produce difference and sameness as dif- ference.143 This difference is produced through the contextualization of patterns. Human beings learn through knowing and inducing patterns. Information is produced by difference, by the newness that the pattern cannot fully incorporate. Considering a person felling a tree with an axe, an ordinary person (he says “an average Occidental”) will say, “I cut down a tree,” as if there is a self in function, but Bateson considers this a fallacy. Indeed, it should be understood as circulation of difference: 
More correctly, we should spell the matter out as: (differences in tree) - (differences in retina) - (differences in brain) - (differences in muscles) - (differences in movement of axe) - (differences in tree), etc. What is transmitted around the circuit is transforms of differences. And, as noted above, a difference which makes a difference is an idea or unit of information.144 
Information, as Bateson famously claims, is “difference that makes a difference,” which leads to his famous dictum: “patterns which con- nect.” Bateson searches constantly for connections between patterns— for example, relations—as well as unity between these patterns.145 We reserve the notion of difference for latter discussion. What concerns 

Bateson in epistemology is not the system of knowledge but rather the operational process of knowing, that is, how knowing is possible. Knowing, for him, is a recursive process in which differences have to be constantly introduced. Knowing is recursive. As we have already seen with the Idealists, knowing is reflective in the sense that it concerns a return to itself in order to project to the future.146 In this sense, unlike Ernst Cassirer, who says that animals don’t have self-knowledge,147 Bateson questions how can we say that a cat doesn’t have self-knowl- edge when it catches a mouse.148 Wiener maintains that the best way to demonstrate the recursive process of self-knowing is in terms of Ash- by’s homeostasis. Like Ashby, Wiener thinks that learning is certainly a feedback process, but it is “a feedback on a higher level, a feedback of policies and not of simple actions.”149 Ashby’s model consists of multi- ple feedback loops. First of all, there is feedback in parts, and then there is feedback that unifies the parts; this means that the organization of the homeostat consists of multiple levels with increasing complexification. However, both Simondon and Bateson also find an ontological problem in the thesis that homeostasis presents a living system. For Simondon, homeostasis always seeks an equilibrium, which means death. Like- wise for Bateson, homeostasis, though pretending to assimilate a living being, resembles rather a clockwork.150 
In one of Bateson’s most famous articles, “Cybernetic of ‘Self’: A Theory on Alcoholism,” positive feedback—“a bottle will not kill him”151—plays an important role by trapping the alcoholic in a closed circuit until he or she moves away from the feedback circuit to another level, meaning from one system to another, which Bateson calls an epistemological change: “If a man achieves or suffers change in prem- ises which are deeply embedded in his mind, he will surely find that the results of that change will ramify throughout the whole of his universe. Such changes we may well call ‘epistemological.’”152 
The alcoholic is trapped by the “other” to whom he wants to prove that “he can . . . ,” and in these positive feedback loops, which one can call symmetric, he is not able to get out of it until he “hits bottom.” Hitting bottom means precisely seeing the limit of such a feedback loop, usually caused by disasters (for example, the diagnosis of cancer or a serious accident) through which he discovers a broader reality, or another system, according to Bateson, which is called “power.” 
Now we can come back to the notion of difference. What does differ- ence do? Difference is a most arbitrary relation: We can say two apples are different in color even though they are both red; we can say that the twins are different though one cannot find a single difference except difference in space. The recursivity of information is in the second part when such a difference is able to produce a difference, meaning that it modifies the self. The alcoholic’s “hitting bottom” is a difference that brings him out of the positive feedback loop and the illusion of self- control. In Simondon we also see the necessity of difference even in communication theory, since it presupposes that there is a schematic difference on the side of the receiver when it receives information from the sender.153 Now, a mere difference is not enough, since it is only recursive when this difference is able to make a difference: “Being nor not being information doesn’t depend only on the internal characters of a structure; information is not a thing, but operation of a thing arriv- ing in a system and producing a transformation. Information cannot be defined outside this act of transformative incidence and of the operation of reception.”154 
In Simondon, differences are measured not by quality but by inten- sity. As in the process of individuation, it takes place when a threshold is reached: For example, the supersaturated solution starts crystalizing with a small amount of heat, while when the solution is not saturated, it demands a considerable larger amount of heat (which, first of all, has to increase the concentration by reducing water into vapor) for the crystallization to take place. Simondon didn’t talk about Bateson, and Bateson doesn’t seem to have been aware of Simondon, but it is interesting to see here that a theory of information in communication is extended to a wider perspective, in Simondon concerning individua- tion and in Bateson concerning learning and evolution. Information is a difference that makes difference, only because it is both contingent and recursive. 
Bateson considers both learning and evolution to be fundamentally stochastic processes. This means that learning is both recursive and contingent. The contingency of information is what allows the recur- sive model to develop itself according to an auto-finality. Bateson agrees with Ludwig von Bertalanffy concerning his critique of the separation between the observer and the environment in science, but he disagrees with the concept of spontaneity employed by general sys- tems theorists, for whom spontaneity is what underlies the autonomy and creativity of the system. As the organicists did with regard to the vitalist notion of an élan vital, Bateson criticizes the concept of spontaneity as being too mysterious.155 This is the reason for which learning, as well as evolution, is regarded as stochastic process, which cannot be fully determined by any reductionist program.156 Regard- ing genetic change, Bateson here takes up the concept of epigenesis from his friend Waddington, and elaborates in his own terms: Somatic change precedes the genetic.157 Bateson refuses the idea, widely attrib- uted to Lamarckians, of the “inheritance of acquired modifications,”158 as well as the (neo-)Darwinist notion that evolution depends com- pletely on the gradual change of genetics in geological time, that is, phyletic gradualism. Bateson invokes the experiment of Waddington that contributes to the latter’s theory of genetic assimilation. Wad- dington’s famous 1942 experiment involved inducing an extreme environmental reaction in the developing embryos of Drosophila (a type of fruit fly). Under the gene called a bithorax, the ordinary flies have a pair of wings, but also two little rods at the end that are con- sidered to be reduced wings for balance. Waddington used ether vapor to stimulate the embryo to grow the two rods into a four-wing fly; he repeated the experiment with the selected Drosophila for the bithorax phenotype over twenty generations and observed that after this time some Drosophila had developed bithorax without ether treatment.159 In 1953 Waddington carried out a similar experiment with cross- veinless phenocopy in Drosophila with a heat shock, and obtained a similar result. With the experiment of Waddington, Bateson wants to show that evolution is neither purely Lamarckian nor Darwinian, but rather that the genotype is awoken by the somatic modification in the phenotype: 
This very profound modification of the phenotype, waking up very ancient and now inhibited morphology, could also be produced by a somatic change. When the pupae were intoxicated with ethyl ether in appropriate dosage, the adult flies, when they hatched, had the bithorax appearance. That is, the characteristic, bithorax, was known both as a product of genetics and as the product of violent disturbance of epigenesist.160 
As Susan Oyama noted, for the “program” in the gene to affect behavior, it will have to participate in the phenotype’s form and function, while such participation demands an “insertion of information” from environmental exchange.161 But whether the “insertion of information” will produce a difference or not depends on the intensity of the incoming information and the existing knowledge and experience. Retrospectively, what Bate- son understands about evolution as a stochastic process can be compared with what Stephen Jay Gould and Niles Eldredge called “punctuated equilibrium,”162 which embraces the role of contingency in evolution. Gould defines contingency as “the tendency of complex systems with substantial stochastic components, and intricate nonlinear interactions among components, to be unpredictable in principle from full knowl- edge of antecedent conditions, but fully explainable after time’s actual unfolding.”163 For Bateson the component of contingency is crucial for the conceptualization of evolution, without which there is nothing new: “In each case there is, I believe, a stream of events that is random in certain aspects and in each case there is a nonrandom selective process which causes certain of the random components to “survive” longer than others. Without the random, there can be no new thing.”164 
Recursivity is not only a mechanism that can effectively “domesti- cate” contingency, as we have seen in Hegel; it is also a mechanism that allows novelty to occur, not simply as something coming from outside but also as an internal transformation. Technics in general is that which attempts to eliminate contingency, but in comparison with technical objects based on linear causality and hence susceptible to contingencies, the recursive mode can effectively integrate contingency in order to produce something new; in other words, it demands constant contin- gencies. In machine learning today, we will also find the emphasis on stochastic processes and randomness. Indeed, we may understand that the historical trajectory to modern machine learning comes from the failure of classical AI’s attempt to bolt on logic-based semantics to its model, such as the reaction of the Idealists against the Cartesian mecha- nism (and latter Hubert Dreyfus’s critique of the classical AI based on Heidegger’s critique of Cartesian cognitive theory). It was developed by various scientists such as Arthur Samuel, the PDP research group at the Stanford University, and Geoff Hinton, among others. Insofar as it has been successful, there is a measure of recursion in how deep learning creates its feature vectors. In a neuro-network, every neuron is given a random weight to start with, and training means to reduce error by recursively adjusting the weights until a desirable output is produced. For decades now it has been a common practice to introduce random- ness into algorithms—namely, randomized algorithms, for example, Las Vegas algorithms and Monte Carlo algorithms, which are widely used in sorting and searching. But contingency or randomness is here by no means irrational, since it no longer concerns mere modal logic but rather function and operation. This will be clearer to us when we see how randomness is practiced in programming. First, randomness allows algorithms to save computational cost, since the computer doesn’t have to exhaust all the samples to optimize the output. Second, it destroys “excess information,” meaning that it makes the input data richer and therefore forces the learning algorithm to learn compact representation. Third, by introducing randomness, it allows the learning algorithm to mutate and to jump out of local minima, as the Google search engine is doing. Because the local minima are not optimal in many cases, with algorithms such as stochastic gradient descent, it is possible to jump through those local minima, finally it may not arrive exactly at the global minimum or maximum but only approximate it. 
It is also in this sense (other than statistical mechanics) that we can understand the claim of Wiener that cybernetic machines are able to “live” the Bergsonian time, since recursivity can be implemented in machines. Simondon understands the significance of recursive episte- mology of cybernetics. For him it is possible to combat such a becom- ing with machines, as Simondon says toward the end of the introduction of his On the Mode of Existence of Technical Objects: 
The machine as an element of the technical ensemble, becomes that which increases the quantity of information, increases negentropy, and opposes the degradation of energy: the machine, being a work of organization and information, is, like life itself and together with life, that which is opposed to disorder, to the levelling of all things tending to deprive the universe of the power of change. The machine is that through which man fights against the death of the universe; it slows down the degradation of energy, as life does, and becomes a stabilizer of the world.165 
For Simondon, humans are organizers of machines, and therefore, with the aid of informational machines, he sees the possibility of using them to combat the entropic disintegration of life and the universe. The vio- lation of the second law of thermodynamics points to an interpretation of life as a nonequilibrium thermodynamics, as Dorian Sagan and Eric Schneider suggest. According to this thesis, nature abhors gradients; it immediately crushes a metal can from which the air has been removed, that is, creating an equilibrium. Life, as the manifestation of the second law, on the other hand, moves in the opposite direction.166 However, this human-machine relation, here indicated as a form of “organization,” is exactly what is missing in the discourse of Wiener, according to Simondon, since the former sees an “identity between living beings and self-regulating technical objects,”167 while for Simondon this equiva- lence is nothing but misunderstanding. Simondon understands very well the importance of feedback. He wants to use feedback together with a renewed concept of information to conceive a new program, which we will discuss in the rest of this book. 
§24. INCOMPUTABILITY AND ALGORITHMIC CONTINGENCY 
In chapter 1 we attempted to establish a relation between Schelling and biological organicism; in the current chapter, we moved to Hegel and a mechanical organicism. In order to overcome the threat of contingency, which is also a threat to the essence, it is necessary to render contin- gency necessary: not circumscribing it ontologically by recognizing it as necessity, but putting it to a test, a necessary step of rationalization. Logic is that which eliminates contingency by either excluding it as illogical and therefore absurd or else absorbing it in order to enrich the Notion itself. In the logic of Hegel, as well as that which is turned into recursive theory (or rather a general concept of recursion), a temporal dimension is added to logic, as Gödel rightly noticed. This Heraclitean motif is present in the recursive model in which a judgment is possible only in the course of time—that is to say, genesis—instead of a mere determinate judgment. The domination of reflective judgment has both epistemological and ontological significance. Concerning epistemol- ogy, we can see with cyberneticians like Foerster, Maturana, and Varela that recursivity is the condition of any true scientific knowledge. The path toward scientific truth is always a computation of computation, a thinking of thinking, an observation of observation. We will not be able to know the object unless we evolve with the process of knowing. Con- cerning ontology, we see a new form of Parmenidean rational ontology in which every being is a recursive being, whether it be a crystal or an organism. The question of categories that Aristotle understands to be the vocabulary of ontology, and that Kant believes to be the pure concepts of the understanding, is greatly undermined in this conceptu- alization, since these categories could be derived recursively instead of given as default of the operation. 
For cybernetics, the unknown can be constructed through the already known. For example, a black box can be approximated by trial and error. And this is the reason that this notion of recursivity primarily concerns rationality. However, how do we confront the question of undecidability, something that is not recursively enumerable? This will be the only threat of contingency to any computation system. If a number is recursively enumerable it means it is calculable; other- wise we will have algorithmic contingency. The best of all possible worlds of Leibniz—that is, the “simplest in hypothesis” and “richest in phenomenon”168—is the criterion for algorithmic information theory (e.g., Andreï Kolmogorov and Gregory Chaitin). For example, given a number, the recursive algorithm that is used to express it must be shorter (and the shorter the better) than the number itself. (Here is also the principle of algorithmic compression.) Algorithmic contingency arrives when it is not compressible, or incomputable. Contingency reen- ters the scene as something that is incomputable, or unpredictable. But such contingency can serve as a leap into another loop and consequently be absorbed by it, one that situates on a higher potency (complexity) of the intellectual life: thinking (which is different but not opposed to feedback or self-reference). However, this cannot be reduced to a simple model of positive or negative feedback, since it problematizes the question of the finality of a simple cybernetic machine. Cybernetic feedback allows an “equifinality,” meaning that it allows different paths to arrive at the same end according to individual situations. However, it doesn’t allow a real auto-finality in which will is in tension with ratio- nality. Raymond Ruyer reproached cybernetics for being a reductionist science of machines and organisms that consciously or unconsciously applies a mechanist view in all domains: 
Feedback, the circuits of recurrent effects of mechanist cybernetics, are only secondary products, symbolising (in the sense of Leibniz) and trans- porting in the macroscopic space and time the absolute recurrences, tran- sitions of “initial state final state,” the real choices between the possible events according to an unobservable dynamic, which are not primary proprieties of all the individualized domains.169 
Hans Jonas also reproached cybernetics for a similar reason. For him, cybernetics has misunderstood teleology as “serving a purpose” (which is mechanical) and “having a purpose” (which has to do with the will).170 However, it may make sense to follow what Colin Pittendrigh and Ernst Mayr call teleonomy instead of teleology, with the difference that teleonomy is close to what we above called auto-finality.171 On the one hand, cybernetics is not a mere “reductionist” science, even though it applies the concept of feedback to all domains. It also presents an epistemological shift. It opens up a new operational thinking, which tends to integrate instead of separate.172 It will be truly reductionist if the feedback loop is maintained only on one plane and thinking is nailed to that plane without allowing it to elevate or enter into another recur- sion: for example, reducing every entity to an algorithm.173 That is the reason that the emphasis on recursion in second-order cybernetics can be seen as an advancement of the systematicity of cybernetics, because it extends recursion to all domains. On the other hand, the reality pro- duced by feedback always exceeds its logic, since it also exteriorizes, and it is through the exteriorization that another loop is created. We will take up this critique of cybernetics again in the next chapter, but we will formulate it as a general problem in the philosophy of nature, in which the question of form is privileged and consequently undermines the significance of matter.174 
Regarding incomputability, we may want to consider cases in which contingencies cannot be absorbed by reflections—for example, a great cosmic catastrophe—or cases in which the exteriorized, which exceeds the logic of feedback, produces a contingency that cannot be made use of, meaning that the dialectics leads to a dead end, which is an algo- rithmic catastrophe. Failures and catastrophes direct us to a broader reality, which the previous system cannot integrate, and it enforces the discovery of another system. As when Bateson talks about the theology of Alcoholics Anonymous, he is referring to the “power” that is another even more powerful system; as he says, “[T]he system or ‘power’ must necessarily appear different from where each person sits. . . . The ‘beauty’ of the woods through which I walk is my recognition both of the individual trees and of the total ecology of the woods as systems.”175 The power is analogical to the divine; this divine can be another system, whose rationality may not be readily recognized, but is rationalized through symbols and rituals. 
Here, instead of talking about an apocalyptic event, we may want to raise examples where the exteriorized can no longer be assimilated into the organic totality. Instead, it disrupts this very capacity of assimila- tion, because the former gains power of determination—that is, it is no longer reason that sets the limit, but the other way around. I tend to think that this is the task of twenty-first-century materialism, pre- cisely because the exteriorized ceases to be the servant (Knecht) in the Hegelian dialectics where exteriorization is a means of reason’s self- knowledge, while the completion of the technical system, which is also the accomplishment of metaphysics, is in the process of undoing this dialectic. It also means that a philosophy of limit based on reason gives way to a materialism underestimated by idealists as well as by many so- called materialists. This is the point of departure from which we would like to look into some major interpretations of the relations between human-machine and culture-technics in twentieth-century organologi- cal thinking. In inquiring into such a transition, we are moving from a philosophy of nature into a philosophy of technology, where organicism and organology meet each other halfway. 
Organized Inorganic 
Fire is want and surfeit. 
—Heraclitus, Fragment 65 
If cybernetics attempts to overcome the opposition between mechanism and vitalism, it is because it systematizes the recursive form as a form of reasoning through the process of information. With this general- ized form of thinking, which we reformulated on the basis of Kant’s concept of reflective judgment—namely, recursivity—we can trace a great material transformation in the twentieth century. The cybernetic machines, especially the Turing machine, have a new status, since it is no longer a mere mechanism in the Cartesian sense, nor is it a living being. Instead, it is an organo-mechanical being: a mechanical being implemented in an organic form. The organic form doesn’t just give form as such, but also organizes matter conditioned by a definite end or a relatively open end inscribed in the recursive algorithm. Even if one follows Kant in saying that it is not possible to exhaust the secret of even a single blade of grass or a caterpillar, it is with the realization of cybernetics that we have to analyze the conceptualization of nature and its relation to technology. This does not mean that we are following some conventional readings of John von Neumann’s theory of automata or self-reproduction, visually demonstrated in John Horton Conway’s Game of Life and further developed by the “digital physics” proposed by authors such as Edward Fredkin and Stephen Wolfram, or informa- tion theory in biology, since these readings are often merely reduc- tionist approaches to the question of life. Reductionism is sometimes strategically necessary for comprehension, but it should be seen as a tool instead of an equivalent relation. From Kant, traversing the post- Kantian Idealists, there is an intimacy between the life of the concept and the concept of life, especially in Hegel, where life is identified with the Notion/Concept.1 Not until 1948, with the emergence of cybernet- ics, was the identity between the concept and life reaffirmed, along with the necessity of reconsidering their relation from the perspective of exteriorization. 
§ 25. FROM ORGANICISM TO ORGANOLOGY 
In the previous two chapters we attempted to outline an organicism oscillating between the biological and the mechanical. The task of this chapter is to move from organicism to organology, from the concept to life, since organology presents another form of recursivity and con- tingency. As the first principle of organology, it is important to avoid drawing an equivalence between machine and organism—a common mistake of reductionism—and measuring the progress of technology according to its closeness to “human intelligence,” since this form of thinking is still very much rooted in a separation between form and mat- ter. Each side falls into an extreme of either formal logic or vital matter as an explanation of individuation. Conventional Cartesian mechanism is based on the belief in a linear logic immanent in the living form, thus leading to the mechanization of the organism, and by doing so the analogy between mechanism and organism is taken as an equivalence. There is nothing terribly wrong with mechanization, so long as it is only admitted as one form of knowing rather than its totality. We know that the shift of knowledge from mechanism to organism is an important transition in Western thought, but it would be a mistake to see an equiv- alence between living form and machine form as it appears everywhere in the mass media today regarding algorithms. It is unwise, if not simply stupid, to lament that Alpha Go has defeated the world champions from Korea or China, since it reveals nothing but a contradiction: On the one hand, commentators see an opposition between the mechanical and the organic, since the organic human is deemed superior, so they lament their defeat; on the other hand, they affirm an equivalence between the machine and the organism because machines can now replace humans. It is not productive to claim that mechanism will not surpass the organ- ism, since what we are witnessing today is the very beginning of this tendency enabled by cybernetics. One cannot easily dismiss such a possibility, however; the key question will be how to find a strategy of coexistence. Algorithms—that which is fundamental to machines— should be appropriated as a function of the organism, and serve the spirit to achieve higher aims beyond the utility of the machine, thereby freeing the machine from predetermined rules and functions. 
The assertion of Wiener that cybernetics has resolved the opposition between Newtonian time and Bergsonian time is plausible and deserves much further reflection than it has so far received. However, cybernetics may still follow the wrong path in its search for an equivalence between organism and machine based on imitation, which somehow obscures the distinction between function and operation. This is the reason we must critically assess this assertion regarding the relation between the organic and the inorganic, which cannot be considered only as two potencies, as in Schelling, but should rather be thought of in terms of a hermeneutic circle. This move from organicism to organology suggests that the organic form has to be elevated from a theory of knowledge to a theory of life, and therefore another recursivity. It is not a recursion that progresses toward a total system with numerous feedback loops, but one that reattaches the mechanical recursivity to the ground, which is life itself. The term general organology was coined by Georges Can- guilhem in 1947, a year before the publication of Wiener’s Cybernet- ics. Canguilhem considered Henri Bergson’s 1907 Creative Evolution a precursor to such a project. Organology is not to be understood in terms of cyborg or human enhancement by a “computer-controlled bio- feedback system.”2 This reading of Canguilhem and Bergson is still too close to the Wienerian cybernetic machine and a certain transhuman- ism, without realizing that the task was to prioritize life as the ground of mechanization. The latter, if not critically rethought, tends to separate humans from life. It is difficult to find a philosophy of life in Wiener, though it is possible to identify one in Bateson and second-order cyber- neticians. It is not yet an organology. 
In Canguilhem et la vie humaine, Guillaume le Blanc suggests identifying a trajectory of organological thought that stretches “from Bergson to Simondon passing by Jacques Laffite [and] Canguilhem” and that “corresponds to the event of a biological philosophy of culture and the attempt to elaborate on a biological anthropology.”3 Our inter- est will be to see how organology developed in intimate relationship with organicism, taking its departure from Kant’s third Critique, and to elaborate on the question of recursivity and contingency in organol- ogy as creative evolution and normativity. Before we embark on this journey, it will be necessary to revisit the organic form of Schelling that we elaborated in chapter 1 and, starting from there, to problema- tize the return to a philosophy of nature that was developed at the dawn of the Industrial Revolution. From an epistemological point of view, technology and nature are no longer two starkly differentiated terms since the understanding of nature already reveals a technical form, meaning that it is no longer an innocent and naive first nature but rather a cybernetic nature. This is also why we emphasized the ambiguity between a biological organicism and mechanical organi- cism. The relation between nature and technology has to be reassessed in view of semantic changes, as well as the new epistemological con- ditions brought about by new discoveries in science and technologi- cal innovations. For example, the prominence of force in Schelling’s time is replaced by the concept of information in cybernetics, which was later also adapted in biology via the work of Edwin Schrödinger. Information, which is neither energy nor matter, demands a new place in ontology and reconstitutes the relations between other categories, as well as the foundations of knowledge, just as Bateson and Simondon did by bringing information together with recursion. It reacts against the tendency of separating active form from inert matter in classical metaphysics, where such hylomorphism assigns to intelligence the task of either imposing or extracting form on or from any being as its essence. The fact is that the intuitive example that we have of brick making (in which a mold gives identity to the clay, like form to inert matter) is already a technical one, meaning that it fails to understand what is prior to such imposition of form or abstraction. Instead of thinking of form as the active force or defending the agency of matter, we will need to employ an organological thinking, which is not only an organicist thinking in contradistinction to mechanical reasoning, but also fundamentally a form of synthetic reasoning. 
In order to unfold the argument we will start with Schelling’s read- ing of Plato, a reading that, though already finished in 1794, before the publication of his work on the philosophy of nature, is of great impor- tance in the formation of his mature ideas.4 Retrospectively, the lack of elaboration on the question of technology in Schelling’s thought is surprising when we consider that the first Industrial Revolution had already begun. This neglect might be understood to have originated from his prioritization of form over fire, the two gifts bestowed upon humanity by the gods, according to Socrates in the Philebus. We will start with a discussion on Schelling’s choice between organic form and fire, and from there introduce the concept of organology in Bergson and Canguilhem. Invoking Bergson and Canguilhem does not mean reintroducing distinct forms of vitalism against cybernet- ics, but rather sketching out an organological thinking that seems to me is beyond the mere identification of vitalism with the élan vital.5 As with organicism, the central question is that of “the whole.” In what sense can we talk about the whole? The whole, on the one hand, works against the mechanical construction of the parts, characterizing a progress from homogeneity to heterogeneity, from object to system. On the other hand, it also implies a teleology, in which the parts oper- ate toward a final cause beyond the grasp of each part. Herein lies the central question of the organology of Bergson and Canguilhem: How to conceive of a creative whole? 
§26. FORM AND FIRE, OR LIFE 
We will have to return to Kant’s concept of the organic, and to conceive of an organology shorn of both Bergson’s hatred for and Canguilhem’s love of Kant. Before we go into the concept of organology, however, we need to address the limit of the concept of organic form in Schelling’s Naturphilosophie. It is the question of fire that I found in Bruce Mat- thews’s discussion of organic form in Schelling’s philosophy, and Matthews’s indifference to this subject, that paradoxically exposes the limits of Schelling’s concept of organic form.6 
In the Philebus, Socrates entered into a debate with Philebus on the question of which is most desirable, intelligence or pleasure. The solu- tion that Socrates provides is that it is neither the one nor the other, but a third: the unity of the two. In the Philebus, we recall the famous passage where Socrates proposes that an opposition between finite (??? π?????) and infinite (??? ??π?????) is not sufficient—we need a synthesis in terms of what they have in common (??? ???????).7 This third notion is not a synthesis in the sense of the dialectics of Hegel, but rather, according to Schelling, that of a “unity in multiplicity.” The form composed of three elements, which give us an infinite succession of individuation, is what Schelling also searches for in his “On the Possibility of a Form of Phi- losophy in General” (1794),8 where the philosopher replaces the name Plato with that of Fichte. Later, in “On the World Soul,” the organic form is introduced to designate nature in general. In this affirmation of the wisdom of Plato there is a certain emphasis on paragraph 16c of the Philebus, Schelling’s translation of which I reproduce here, since it deserves our attention: 
This form is a gift of the gods to men, which together with the purest fire was first given to them through Prometheus. Therefore the ancients (greater men and closer to the gods than us) have left the story behind, that everything which has ever emerged out of unity and multiplicity (plu- rality), in that it united within itself the unlimited (apeiron, universal) and the limit (to peras, unity): that thus we too in light of this arrangement of things should presuppose and search for every object one idea. . . . —It was the gods then, who taught us to think, learn and teach like this.9 
Schelling emphasizes “this form,” which is not explicitly expressed in the Greek text, and in doing so marries the Philebus with the Timaeus. But what is important for us here is that, in a certain way, Schelling has already taken a decision by ignoring the question of the fire, which was sent along to humans with “this form.” We can read this passage in the Philebus in parallel with the story told in the Protagoras by the sophist concerning the two titans Prometheus and Epimetheus. In the Protago- ras, the fire is not sent but stolen by Prometheus. There the sophist tells the story of the titan Prometheus, also said to be the creator of human beings, who was asked by Zeus to distribute skills to all living beings. His brother Epimetheus took over the job, but having distributed all the skills found that he had forgotten to provide for human beings. In order to compensate for the fault of his brother Epimetheus, Prometheus stole fire from the god Hephaestus and bestowed it upon humankind: 
Now Epimetheus, being not so wise as he might be, [321c] heedlessly squandered his stock of properties on the brutes; he still had left unequipped the race of men, and was at a loss what to do with it. As he was casting about, Prometheus arrived to examine his distribution, and saw that whereas the other creatures were fully and suitably provided, man was naked, unshod, unbedded, unarmed; and already the destined day was come, whereon man like the rest should emerge from earth to light. Then Prometheus, in his perplexity as to what preservation he could devise for man, stole from Hephaestus and Athena wisdom in the arts [321d] together with fire—since by no means without fire could it be acquired or helpfully used by any—and he handed it there and then as a gift to man. (321c–321d)10 
Hesiod told another, slightly different version of the story in his Theogony, in which the titan challenges the omnipotence of Zeus by playing a trick with a sacrificial offering. Zeus expressed his anger by hiding fire and the means of living from human beings, in revenge for which Prometheus stole fire. Prometheus received his punishment from Zeus: He was chained to the cliff, and an eagle from Hephaestus came to eat his liver during the daytime and allowed it to grow back at night. In Technics and Time, 1: The Fault of Epimetheus, Bernard Stiegler has interpreted this mythology in order to show that the fire as a com- pensation is precisely because of a default. On this interpretation, fire is technics, which allows “the pursuit of life by means other than life,” the organic through the inorganic, and the history of fire also constitutes the already-there in the sense of Heidegger.11 In Schelling’s interpreta- tion of Plato, the form is separated from the fire, from the origin of their coexistence as presence; that is to say, the soul is separated from techn??—the organized inorganic, as Stiegler terms it. If philosophy of nature is identified with the organic form, how can one understand this reality in which the form is no longer separated from the fire, as is so evident to us today in the epoch of the Anthropocene? 
To do justice to Schelling, we must admit that the philosopher has not completely forgotten about fire. Fire becomes a metaphor of the spirit in “On the World Soul,” where he wrote that death in nature is an extinguished (erloschene) life; and in the Stuttgarter Privatvorlesungen (1810), fire became a pseudonym of the sprit, spirit being described as “a soft and muffled flame of life” (sanfte, gedämpfte Lebensflamme).12 
It seems that for Schelling, fire is not something that is completely separable from the form, yet it does not possess the technicity that we want to introduce below. If fire is not exterior to the organic form, it is a central theme to articulate the relation between the organic and the inorganic, which we would like to examine in the next two chapters. It is not our aim here to deconstruct Schelling’s philosophy of nature, but rather to rethink both the thought and the unthought in his philosophy of nature, in order to demonstrate a new condition of philosophizing after Kant’s Critique of Judgment. 
§27. DESCARTES AND THE MECHANICAL ORGANS 
In his 1947 article “Machine and Organism,” Canguilhem proposed to reverse the Cartesian epistemology concerning the mechanization of life based on functional similarities. This critique of Descartes and praise of Kant and Bergson is fundamentally part of his project to intro- duce a biophilosophy that he sees the beginning of in 1910, marked by the publication of Bergson’s Creative Evolution in 1907 and the launch of the journal “Année Biologique.”13 Life in Descartes’s system does not have any “ontological originality,” writes Canguilhem; life is not recognized as a “proper metaphysical object.”14 According to Descartes, the body and its movements are governed by mechanical rules. For him, the death of an organism is no different from the malfunctioning of a clock. The episteme that we name “early modern” consists of the belief in the certainty and perfection of the laws of nature, and sees these as the absolute governing rules of all beings. It is not strange to the reader that Descartes, in a great spirit of self-doubt in part 2 of the Discourse on Method, asked, while looking at the street and the pedestrians out of the window, if these were not robots with clothes and hats governed by mechanical rules.15 
Descartes’s use of mechanical models to explain vital activities can be found in various parts of his work. It will be sufficient for us to understand how such mechanical models seem to him adequate and important for theoretical and practical reasons. The relation between the mechanical model and the living being is not merely functionally ana- logical but also ontological. Descartes rejected the conventional belief that the soul is responsible for the movements of the body. Instead, he restricted the soul to a localization in the pineal gland where “the seat of the imagination and the ‘common’ sense is located.”16 The soul alone does not control the movement of the body, but rather it receives sensations and sends commands through the mechanism of the nerves, which are then connected to other mechanical parts of the body. In his “Description of the Human Body” (La description du corps humain), published in 1662 with the edition of The Treatise on Man, Descartes provides a mechanical model of the human body by mapping the organs onto existing mechanical parts. In Descartes’s “mapping,” the heart is like the spring that drives different parts into motion: In the heart, there is fire that warms up the blood pumping out of it; veins, stomach, intes- tines, and arteries are pipes that allow juices, food, and heated blood to circulate from one place to the other; the blood carries the “animal spirits” or “a kind of air or very fine wind,”17 which dilate the brain and make it ready to receive impressions both from external objects and from the soul.18 These same animal spirits flow from the brain via nerves to muscles and thereby command them to move. Descartes goes on to explain the mechanism of nutrition and the formation of the seminal material. He attempts to provide explanations of various bodily phenomena, though many of them seem outdated if not ridiculous today: for example, the correlation between humors and getting thin.19 
Descartes’s mechanization of bodily organs is the epistemological product of his time; such mechanization can best be demonstrated when Descartes compares the operation of heat and the arteries with the organs of the church. Here we can draw a schematic comparison (see Table 3.1). 
However, we must acknowledge that Canguilhem’s critique of Des- cartes is not entirely negative. Indeed, Canguilhem sees in Descartes a type of technological thinking that is rare in the history of Western philosophy. In an article titled “Descartes et la technique,” published in 1937, Canguilhem systematically demonstrates the important role of technics in Descartes’s discourse on truth, writing at the very begin- ning of the text that “Descartes has very explicitly and very frequently said that the effectiveness of the arts is conditioned by the truth of knowledge.”20 Descartes (as well as Francis Bacon) energetically negates the finality of nature in order to affirm human progress and to demonstrate the continuity between the natural and the artificial. As he writes in the Principles of Philosophy: 
And, to this end, things made by human skill helped me not a little: for I know of no distinction between these things and natural bodies, except that the operations of things made by skill are, for the most part, per- formed by apparatus large enough to be easily perceived by the senses: for this is necessary so that they can be made by men. On the other hand, however, natural effects almost always depend on some devices so min- ute that they escape all senses. And there are absolutely no judgments {or rules} in Mechanics which do not also pertain to Physics, of which Mechanics is a part or type: and it is as natural for a clock, composed of wheels of a certain kind, to indicate the hours, as for a tree, grown from a certain kind of seed, to produce the corresponding fruit.21 
It is in Descartes that the distinction between nature and technics is blurred, not because Descartes wanted to reconcile nature and technics as two opposing realities, as we see them today, but rather because for him they share a common principle: namely, mechanism. This is also the reason why we may say that, having mechanism as the common ground, technics is elevated to be the primacy of knowledge. In other words, technics is not a mere application of scientific knowledge; rather, it is thanks to technics and technical activities that science is made to appear on the occasion of technical breakdown. Descartes, a philoso- pher who reflected often on both the art of making and of medicine, sees a discrepancy between practice and theory in terms of which they are not reducible to one another, but rather together constitute a cycle of knowledge production. In another article titled “Activité technique et creation,” which was originally a talk given in 1938 at the Société Toulousaine de philosophie, Canguilhem writes: “The fact is that the modern world simultaneously presents a multiplication of theories and a multiplication of techniques. But we cannot say if it is the technical boom that depends on the scientific boom or the other way around.”22 
However, this apparent undecidability is only a strategy for affirm- ing the primacy of technics and of creation, which is more primary than scientific knowledge as representation.23 Canguilhem gives various examples—thermodynamics, Pasteur’s theory, the laws of electrostat- ics—that emerged out of practical obstacles in relation to the steam engine, alcohol manufacturing, improving the compass, and so on, and he doesn’t forget to mention that the laws of dioptrics were formulated in relation to the problem of the size of the glass described by Descartes in his Dioptrique. He therefore arrives at the conclusion that “the rise of scientific thought is conditioned by the failure of technical thought.”24 Canguilhem rejects the idea that technology is an application of scien- tific knowledge; rather, he sees science and technology as two regimes of knowledge that inform each other. Retrospectively, we should be able to notice that with the advancement of technology, the dynamic of such circularity is largely altered (see chapter 5). 
Mechanism, understood as a principle of nature, allows Descartes to reverse the intuitive relation between science and technology, which remains an important source of inspiration for Canguilhem. However, the major problem of conceiving life in terms of mechanism is that it attempts to “completely explain life without life.”25 In “Machine and Organism,” Canguilhem wants to question the Cartesian conception of the relation between organism and machine and to show that such a reduction should be reversed by recognizing the preexistence of the organism, meaning that biology is prior to technics. And if biology or organism is prior to technics, then this difference, and the dynam- ics between the two, should be rethought. The central question is not whether mechanism can produce the organism or if mechanism is equivalent to an organism, but rather the relation between the organic and the inorganic (here we can talk about technical objects) has to be systematically articulated. In Descartes there are traces of organologi- cal thinking: For example, the imitation of “a prior organic given”26 is implicit in texts such as The Treatise on Man, and sometimes it is explicit—for example, in the Diotrique, which Canguilhem also cites: “[W]e do not know how to make a new body, we must add to the inter- nal organs organs external to the natural organs, artificial organs.”27 Descartes himself did not develop this organology further. Instead we see in Cartesianism an elimination of the teleology of life through the substitution of mechanism for organism, and of anatomical for dynamic functions, since teleology is closed in the technics of production.28 The effort of Canguilhem, as interpreted here, is one of jumping out of such a reduction and substitution to a renewed relation between organism and machine—namely, a general organology. 
§28. KANT AS PHILOSOPHER OF TECHNOLOGY 
First, concerning the term organology, we should understand the prefix organo- to signify both organs and the organic, which we have dis- cussed in the previous two chapters. It is the double sense of the word organ that allows us to see both a vitalist dimension and a material- ist dimension of Canguilhem’s general organology. It is necessary to distinguish life and living being, for the latter is more or less identified with biology. However, the ambiguity between life and living being also allows Canguilhem to integrate Henri Bergson’s Creative Evolu- tion into his own formulation of the general organology, and it is also precisely for this reason that he terms Bergson’s Creative Evolution the precursor of a general organology. In “Notes sur la situation faite en France à la philosophie biologique,” published in the same year (1947) as “Machine and Organism,” Canguilhem states that the value of Bergsonian philosophy is that it “understood the exact relation between organism and mechanism, being a biophilosophy of mechanism, treat- ing machines as organs of life, and setting up bases for a general organology.”29 Bergson was for him “the first French philosopher who has considered mechanical invention as a biological function, as aspect of the organization of matter by life.”30 Life for Canguilhem is precisely the mediation between the mechanical and value, and such dynamics (in the form of conflict) produce both experience and history.31 Unlike the recursive form in Schelling, in Canguilhem and Bergson we see another recursivity, one that takes up the task of producing and reintegrating the inorganic in order to preserve life, and at the same time exhibits a more advanced form of life. 
The organism is important to Canguilhem since it is that which defines the individuality of a living being. In “Cellular Theory,” pub- lished in the collection of essays Knowledge of Life, Canguilhem rejects the idea that the individual can be understood through an analytic theory based on the atomistic concept of the cell, and proposes instead a synthetic theory that considers individuality from the perspective of a “globality.”32 The organic form is important for Canguilhem’s vital- ism as well as his biophilosophy of culture, which gives a new role to technology. Commenting on the organic form, Le Blanc has remarked: “The organic form is itself a concept belonging to the epistemological field of vitalism and the corresponding political overdetermination, romanticism.”
We have seen in chapter 1 the difference between organicism and vitalism. However, this doesn’t exclude the fact that the organic form is crucial in vitalism, and Canguilhem thanks equally Hans Driesch, Sven Hörstadius, Hans Spemann, and Hilde Mangold for their contri- butions.34 It is also not simply an affirmation of the philosophical con- dition that we have tried to explore that began with Kant, but a further development of such organological thinking, at the same time constitut- ing a new condition of philosophizing in view of the relation between nature and technology. It is in regard to the concept of the organic that Canguilhem suggests reading Kant as a philosopher of technology, especially in his Critique of Judgment: 
Now, contrary to Descartes, one author has affirmed both the irreducibil- ity of the organism to the machine and, symmetrically, the irreducibility of art to science. This is Kant, in the Critique of Judgment. It is true that in France we are not used to looking for a philosophy of techniques in Kant, but German writers who have been interested in these problems, especially from 1870 onward, have not failed to do so. . . . In paragraph 65 of the “Critique of the Teleological Power of Judgment,” Kant uses the example of the watch, so dear to Descartes, to distinguish machine from organism. . . . In paragraph 75, Kant distinguishes man’s intentional technique from life’s unintentional technique. But in paragraph 43 (from the “Critique of the Aesthetic Power of Judgment”), Kant defines the originality of this intentional human technique relative to knowledge.35 
We have already explored in chapter 1 the significance of Kant’s discourse on the concept of organic in §65 of Critique of Judgment, and its development in the work of Schelling and Hegel concerning a philosophy of nature. We should look closely at what Canguilhem wants to say about §75 and §43. In fact, it is not in §75 but rather in §72 that Kant makes a distinction between two forms of procedure or causal operations of nature: One is designed (technica intentionalis), the other undesigned (technica naturalis). The former is related to the final cause of nature or the natural end, which already includes the end of every natural project. We cannot know this natural end objectively. The latter concerns the “natural mechanism,” which may be contingently coinci- dent with our concepts of art, but it is also due to such a coincidence that it is “erroneously interpreted as a special kind of natural generation.”36 In §43 Kant makes an effort to distinguish art from nature, technics from science. For a craftsman like a shoemaker, although he may be able to describe how exactly the best shoes should be made, he “doubt- less, was not able to turn one out himself.”37 Canguilhem is providing a reading (through Paul Krannhals’s Der Weltsinn der Technik) beyond the irreducibility between organism and mechanism, which states that, for Kant, “every technique essentially and positively includes a vital originality irreducible to rationalization.”38 Kant was dealing with the discrepancy between the final cause of nature and the mechanical cause that is presumed to coincide with art. Recognizing this differ- ence doesn’t mean that one has to reject mechanical rules. Kant instead affirms that everything “which is necessary in this nature as an object of the senses we should judge according to mechanical laws,” but he continues that “the accord and unity of the particular laws and of their resulting subordinate forms, which we must deem contingent in respect of mechanical laws—these things which exist in nature as an object of reason, and, indeed, nature in its entirety as a system, we should also consider in the light of teleological laws.”39 Canguilhem’s strategy of reading Kant as a philosopher of technology is to affirm that for him this irreducibility between mechanism and organism actually points in the direction of accommodating the mechanical within the organic, the machine within life. Canguilhem wants to show a more complicit rela- tion between the mechanical and the organic from different sources in ethnology, which we can summarize as follows: 
1. The organic is irreducible to the mechanical. On the contrary, the mechanical can be seen as a particular case of the organic. We may say that mechanization is a kind of rationalization that eliminates certain “useless” and “contingent” features of the organic process. Mechanization entails a simplification and rationalization of the organism. Canguilhem didn’t distinguish the organic from the vital, unlike the other biologists we have discussed—Needham, for example, who promoted a third way, beyond the opposition between mechanism and vitalism. 
2. Technical objects originate from the projection of organs. Canguil- hem takes the point from Alfred Espinas’s Les origins de la tech- nique (1897), who in turn took it from the Hegelian Ernst Kapp’s Grundlinien der Philosophie der Technik (1877).40 In this work Kapp proposes to understand tools as the projection of organs: For example, a hook can be seen as a projection of the hand. Through the exteriorization of the interior, humans acquire knowledge of themselves by recursively coming back to themselves (Cassirer also considers Kapp’s organ projection as a self-knowing). This close relation between organs and technics was further studied by the anthropologist André Leroi-Gourhan in his ethnographies on the evolution of technics, in which technics is at the same time the exte- riorization of memory and the liberation of bodily organs.41 
We must emphasize, however, that Kant was trying to articulate the 
knowledge of “nature as a whole” as a totality of empirical laws (in §75), and this whole cannot be known objectively. The whole remains unknown, but with the idea of reason (or transcendental supposition as a heuristic principle), one can approach it “as if” the whole is present as such.42 The way that reason arrives at the wholeness is reflective judgment, which unifies the laws of nature. The relation between the whole and reflective judgment seems to presuppose a recursivity whose end remains objectively unknown but which can be subjectively thought through reason. It is also the whole that conditions the parts in the recursive process. The speculation of the whole as a method, through Kant and Kurt Goldstein, has a strong influence on Canguil- hem’s general organology. Goldstein’s Der Aufbau des Organismus (1934)43 had a significant impact upon the French philosophy of the time (e.g., Maurice Merleau-Ponty). He also proposes a holistic view of the organism, as well as a theory of the abnormal and the pathological, whose traces are clear in Canguilhem’s own theory. What is holism for Goldstein? The sixth chapter of The Organism provides a clear answer. Without going into the details of the various clinical and laboratory examples that Goldstein employed to demonstrate his thesis, we would like to single out two points. First, Goldstein insists on holism as a method for understanding the behavior of the organism instead of being restricted to local and anatomical explanations. The simplest example is that of putting a starfish in various abnormal positions, where one will observe that the organism soon returns to a normal position,44 or when an organism is injured: for example, when a dung beetle loses one of its legs and will then coordinate its whole body to compensate for the loss.45 Goldstein argued against anatomical evaluation as a basis of localization, as well as against so-called antagonism, which states that every performance results from two opposing forces or operating mechanisms (e.g., the enervation of muscles). Instead, Goldstein wants to interpret these phenomena from a holistic perspective, and he states that, concerning the muscle group, “there are never two antagonistic mechanisms active. We are not even dealing with isolated innerva- tions, but only with one.”46 The second point is that Goldstein was very much influenced by the figure-ground theory of Gestalt psychology, though Goldstein never claimed to be a Gestalt psychologist himself. In the same chapter on holism, Goldstein states that “every reaction is a ‘Gestalt reaction’ of the whole in the form of a figure-ground configuration.”47 After criticizing the inadequacy of antagonism theory, he returns to the figure-ground relation by showing that 
[w]hen a definite movement is intended, then, corresponding to the required distribution in the different sectors of the muscle group, the differential excitation in the spinal apparatus takes place. This pattern of excitation, in the various sectors, forms the “figure process” that stands out as a definite Gestalt of excitation distribution, against the rest of the organism, which forms the background. In the total configuration, the excitation of the agonist, or of the antagonist, represents a part that can only be artificially isolated.48 
This figure-ground relation can also be found in authors like Can- guilhem and, most notably, Simondon, who often employed the figure- ground relation to describe the processes of individuation and genesis of technicity, which we will discuss in chapter 4. Although Canguilhem doesn’t explicitly use the figure-ground trope in this text, his reading of Kant seems to have already applied such a view. We could say that Canguilhem’s general organology is, first, based on the organic whole, and second, a call to bring mechanism back to life. In order to further construct Canguilhem’s general organology, which was only mentioned twice in his writings, we will have to raise the question of why Can- guilhem claims to read Bergson’s Creative Evolution as a precursor to general organology. 
§29. ORGANOLOGY IN CREATIVE EVOLUTION 
Evolution is creative since it is recursive. It consists of repetitive improvements of the means of survival, which open up new perspec- tives of the world through its artificiality and the interobjective relations that it establishes. It also consists of a constant return to the organizing force, which is life itself, in order to undo what is already made and became stratified. Every return is not a return to the same object or the same place, but rather a reorganization of the organic and the inorganic. In Creative Evolution Bergson wants to go beyond both mechanism and finalism. Mechanism is an artificial system that is deprived of “real time,” since the mechanical parts don’t have a history. Every state can be restored according to an external cause. This means that it is revers- ible and it doesn’t endure. Finalism always presupposes a design. Even Kant’s nature end presupposes a sort of design, which makes Kant’s Critique of Judgment close to cybernetics. Unlike the Leibnizian final- ism that subsumes beings to a predefined design, radical finalism breaks such a gigantic program of existence into smaller units, in which every individual has an internal finality. Bergson ridiculed the finalists for having the illusion that such a breaking-down might reduce its surface of blows. The reason is simple: Since the organismic whole is a spatial dimension of life, every “self-contained” organism is always influenced by other organisms, so such an internal finality only affirms the neces- sity of “external finality.” We thus read the famous claim of Bergson, “[F]inality is external or it is nothing at all”: “The truth is, it lays open to them a great deal more radical as our own theory may appear, finality is external or it is nothing at all. . . . The idea of a finality that is always internal is therefore a self-destructive notion.”49 
Bergson’s rejection of finality is subtle, since although he disclaims the finalistic nature of the élan vital, as Ernst Mayr says, it “could not have been anything else, considering its effect.”50 It goes the same when Vladimir Jankelevich points out that Bergsonism presupposes an organic whole; it seems difficult to reject finalism with such a presuppo- sition. When Bergson says that finality is external or it is nothing at all, he is simply refusing a finalism that is in fact a disguised mechanism, meaning that the beginning (the design) already implies the end. Exter- nal finality means an opening to contingency, to creativity, conditioned by the complexity of the organic whole, which cannot be reduced to simple design. How, then, can we articulate the organic whole? As with Kant’s comment on the difference between the clock and the organism in §65 of the third Critique, Bergson sees in the machine a lack of endurance (durer)51—the machine lasts, but it doesn’t endure; a rock may change, but all successive states are external to each other52; or, in Jankelevich’s words, “A material system is entirely what it is at any moment one observes it, and it is nothing but that. Since it does not endure, it is in a way eternally pure because it has no past whatsoever to color and temper its present.”53 If a machine can be comprehended from its different parts, an organism can be comprehended only from its entirety: “[T]he organism is in its entirety or it is not at all.”54 We cannot demand from a machine more than what has been made, while, on the contrary, organisms “are not what they are and are what they are not.”55 
To what extent is the organic whole that Bergson emphasizes differ- ent from the whole in finalism? The whole for Bergson is not that which structurally defines an organism; rather, it is life itself. In agreement with what Kant says in §77 of the third Critique, Bergson writes: “That life is a kind of mechanism I cordially agree. But is it the mechanism of parts artificially isolated within the whole of the universe, or is it the mechanism of the real whole?” Creative Evolution is a treatise that delivers a critical examination of the evolutionist philosophy by positing philosophy as the ground of science and life as the ground of mechanism. Here we will touch upon the question of time, since life endures. This enduring is not only a subsisting, since evolution is “a real persistence of the past in the present, a duration which is, as it were, a hyphen, a connecting link.”56 But it is also creative, because duration “means invention, the creation of forms, the continual elaboration of the absolutely new.”57 We see here that for Bergson, evolution is duration consisting of multiplicity indicated by the “absolutely new,”58 while discontinuities or changes are also that which distinguish it from the mathematized entity or objectification. Life means the persistence of the past and the consistency between the whole and its parts in duration. Bergsonian dualism, which has been reproached by many authors, is not a real dualism in the sense that it does not posit two unsurmount- able and unreconcilable realities. Dualism is comprehended in duration and resolved by intuition. By the same token, the dualism between mechanisms and organisms is not a confrontation of the two, but rather precedes an integration according to the spatial (parts-whole relations) and temporal (duration) dimensions of creative evolution. Bergson’s general organology is indeed an attempt to reconcile the inert (matter) and the living (life), the inorganic and the organic. We know that it is in Creative Evolution that Bergson developed the concept of élan vital, which we can understand as an impetus of life that acts on the orga- nization of matter, or as creativity irreducible to any mysterious vital force.59 Bergson distinguishes an indirect and direct way of acting of the élan vital, which also distinguishes intelligence from instinct: “It can either effect this action directly by creating an organised instrument to work with; or else it can effect it indirectly through an organism which, instead of possessing the required instrument naturally, will itself con- 
struct it by fashioning inorganic matter.”60
The élan vital, therefore, is an organizing force, which either directly 
affects the organism or indirectly organizes the inorganic through the organism to make it part of the organism. To reconstruct a Bergsonian organology, we will focus on the third chapter of his Creative Evolu- tion, a chapter to which Canguilhem devoted a long commentary.61 We will do so by taking up the dualities that Bergson sets up: life versus matter and intuition versus intelligence. If intelligence, by geometrizing matter, recursively constructs an artificial system, then by undoing the relation between matter and intelligence, intuition returns mechanism to its ground, which is life. Bergson’s definition of intelligence here is certainly a narrow one, but it is very pertinent to what is called arti- ficial intelligence today because the latter is fundamentally based on geometrical time and space. Bergson’s critique of intelligence can also be applied to the fantasy of artificial intelligence that finds the Absolute in itself. To act is a key word for acquiring a true understanding of intelligence and its potential, since intelligence, as Bergson argued, is not what Plato describes in the allegory of the cave, where intelligence contemplates either the sun or the shadow. To act is at the same time to bathe in the ocean of life, the élan vital. We can thus see that Bergson attempts to account for the genesis of intelligence instead of simply presupposing it. For Bergson, explaining the genesis of intelligence and dissolving its geometric form in order to unveil its primordial reality comprises the primary task of philosophy: 
Yet a beneficent fluid bathes us, whence we draw the very force to labor and to live. From this ocean of life, in which we are immersed, we are continually drawing something, and we feel that our being, or at least the intellect that guides it, has been formed therein by a kind of local concentration. Philosophy can only be an effort to dissolve again into the Whole. Intelligence, reabsorbed into its principle, may thus live back again its own genesis.62 
Bergson immediately suggests an objection to his theory, which is conceived as a positive feedback loop—for if one wants to go beyond the intelligence, what can one start with if not intelligence itself? If this were true, wouldn’t we be locked within this circle? Bergson introduces “action” to break out of this circle, since action is a kind of information that triggers a Gestalt switch. For example, we cannot learn to swim simply with our intelligence, but must first get in the water, and it is by acting that intelligence is brought back to a reality from which it is detached, if not yet completely so.63 Canguilhem sees here a parallel between the Spinozist natura naturans and natura naturata and the Bergsonian flux of consciousness and intelligence,64 though Bergson used this Spinozist vocabulary only in Two Sources of Morality and Religion.65 If intelligence is characterized by a geometrism, as Bergson claimed, it is necessary to negate such a geometrism (it is considered as a natural logic, such as induction and deduction, which the philosopher criticized) in order to arrive at the primary reality of consciousness.66 Intelligence acts on matter in order to see in it space, and the latter also aids the former to schematize so that it continues on its own. We can thus see that intelligence and matter are two sides of the same genesis: 
Then in saying that neither does matter determine the form of the intel- ligence, nor does the intelligence impose its form on matter, nor have matter and intellect been regulated in regard to one another by we know not what pre-established harmony, but that intellect and matter have pro- gressively adapted themselves one to the other in order to attain at last a common form. This adaptation has, moreover, been brought about quite naturally, because it is the same inversion of the same movement which creates at once the intellectuality of mind and the materiality of things.67 
That there are two sides to this genesis does not mean that matter is intelligence. Rather, they are two realities correlated with one another. Bergson takes intelligence away from the contemplating whole, as in Plato’s allegory of the cave, to the ground of labor—as Canguilhem says, like a plowing ox (bœuf de labor).68 In the reciprocity of the two terms, Bergson reintroduces a ground against which the genesis of matter and intelligence emerge as a reciprocal process. Action, like a contingent event, which breaks away from the recursive form, takes the intelligence out of its routine and reveals to it an ocean of which it is only a surface: “[Yet] the state of consciousness overflows the intelligence; it is indeed incommensurable with the intellect, being itself indivisible and new.”69 Or even more beautifully, as Canguilhem puts it: “It is by reflect- ing its character of progressive rectification that the intelligence will find the mobility and the fluidity of the current of which it is the deposit, the solidification. Intelligence, reinterpreting itself in life, revives its gen- esis, it becomes dynamic from static, agile from frozen again.”7 Finality is constrained by tendencies—a word that Bergson employs in both Creative Evolution and Two Sources (elementary tendency, preliminary tendency, natural tendency, original tendency, etc.). Ten- dencies are not ends, but they propel the movement of the flow—these could be habits, will, or the direction of a certain future. All these influences presuppose “an organic whole that manifests a deep-living principle of which they are the expression.”71 The question of finality comes in here again, since these tendencies are like valves, participating in the becoming and leading to what is called a “creative finalism.”72 However, we may want to ask, isn’t this current the one in the sense that Schelling gives to nature? And isn’t the genesis of both matter and intelligence a constant contact with hindrances (Hemmungen), which in turn determine the tendency of the movement of consciousness? Is dualism therefore only a fabulation to be unmade in order to go back to the current? 
In order to arrive at a better understanding of these questions—even if we will probably never have any satisfactory answers—we have to be precise about the meaning of matter and intelligence.73 Canguil- hem proposes to understand the question of materiality in two stages: first, according to its relation to duration, and second, in its relation to extension (étendue). In relation to duration, matter is presented as the “incapacity of linking the present to the past, of contracting habitudes, of utilizing the present by pushing it into the future.”74 Matter can be considered as a fall (chute); it is a product of oblivion (oubli), as in Félix Ravaisson’s On Habit. Canguilhem suggests that Bergson resonates here with his master Ravaisson concerning the role of matter. Like Ravaisson, who thinks that “the body exists without becoming anything [sans rien devenir],” Bergson holds the same view, seeing matter as “a limit, the ideal term of an inclination.”75 In relation to extension, matter is presented as the permanent threat of an exteriorization that retards the spiritual tension or the personality in the form of a deficit. As Bergson says: “[A]ll our analyses show us, in life, an effort to re-mount the incline that matter descends.”76 Toward the end of chapter 3 of Creative Evolution, Bergson delivers an explication of the relation between life and matter through the famous example of steam. The example consists of imaging a vessel full of steam. There is a crack in the vessel, so the steam escapes toward the opening. After having contact with the out- side air, the steam condenses and therefore falls back into the vessel. At that moment the escaping steam is trying to raise and retard the falling water drops. With this image, Bergson claims that “from an immense reservoir of life, jets must be gushing out unceasingly, of which each, falling back, is a world.”77 This recursive form, which is ended by its own exhaustion, is not an accurate comparison, as Bergson reminds the readers that the creation of the world is a free act of which life takes part, and so is unlike the jet of steam trying to escape. Nevertheless, this image illustrates the creativity of life, “making itself in a reality which is unmaking itself.”78 Matter, as we can see from this image, is a move- ment that inverts the movement of life. However, it is not opposed to life, but is rather the necessary condition of evolution: 
The impetus of life, of which we are speaking, consists in a need of cre- ation. It cannot create absolutely, because it is confronted with matter, that is to say with the movement that is the inverse of its own. But it seizes upon this matter, which is necessity itself, and strives to introduce into it the largest possible amount of indetermination and liberty.79 
In relation to extension, tension is present in the spirit like the back- ground against which extension emerges, like freedom in relation to mechanical necessity.80 The passage from tension to extension is an inversion. Canguilhem brings deficiency, inversion, and de-tension together in the same group.81 To detend, says Bergson, is to let go of tension, in order to extend.82 Canguilhem distinguishes extension from space. The former is the content of perception, while the latter is the pure exteriority of homogeneous parts; therefore, “matter is a possible direction of the consciousness and as such it is more extension [éten- due] than spatial.”83 We can recapture the relations between these terms with the aid of a precise summary from Canguilhem: 
In short, we obtain, in the decreasing order of spiritual reality de-tension (by which matter has a private relationship with the mind/spirit), exten- sion (order properly material where the relation to the spiritual order is progressively forgotten), extended (the past participle signifying here the oblivion of past spiritual participation), space (externality of parts to each other and from the whole to the spirit). Space is the achievement of the intelligence out of a sketch. Here we recognize the brutality of the action, the abusive nature of logical thinking.84 
Canguilhem thus suggests that matter has “more natural reality” in Creative Evolution than in Matter and Memory, since in the form of extension matter is “offered and lends itself to the habitude of the intelligence.”85 If action is what triggers the reflection of the intelligence to bring it back to creativity, then this reflection can take place only when the intelligence is forced to go back to and beyond itself. This force acting against it can only be matter itself. It is through contact with matter that the immense virtual tendencies of life are actualized or individualized. However, the individualities are always reassociated and reintegrated into an organic whole. 
If we refer back to Bergson’s example of learning to swim, it is evident that one cannot learn how to swim by just watching a video, through which one schematizes matter in geometrical form, for exam- ple, the movements of the arms and the kicks. Neither can one learn to swim by repeating these spatialized movements on the bank of the pool without really interacting with the water, which is matter (as extension instead of space). As with the example of swimming, water in terms of matter is not that which limits movement, like a static form, only by act- ing against which the swimmer can advance. To think of it in terms of organology, water is neither only physical and static nor an ensemble of geometrical and physical attributes; rather, it is that which facilitates the body to let go of its weight and push itself in the desired direction. To learn to swim is to invent new gestures that integrate water as a function of the man-water unity. 
One can make a more general claim in relation to Bergson: namely, that organology is a practice that infinitizes the finite, liberates the determined by de-geometricizing the object in order to create. Berg- son’s theory of the genesis of intelligence implies also a genesis of matter and an attempt to dissolve the rigid spatiality produced by the intelligence. It is in his commentary that Canguilhem formulates what he calls a Bergsonian general organology later in “Machine and Organism” and “Notes sur la situation faite en France à la philosophie biologique”: 
The formation of form would be correlative to the materialization of matter. In the description of the spatial form, Kant is interested in its use. Bergson thinks that one cannot understand the use if one does not understand its formation, because here too it is the function that creates the organ. Earlier on (E.C., 175) Bergson described matter as organ.86 
What is meant by saying that matter is organ here? This passage is directed against Kant’s understanding of intelligence and matter in terms of form and thing. Rather, they are to be understood as move- ments acting on each other. If intelligence and matter are decomposed into movements, then we will see that the formation of form is no differ- ent from the materialization of matter. It is thus not form alone whose functionality is at the center in such a genesis, but rather the unity of usage and formation. Matter is internalized organs: “[Instrument] reacts on the nature of the being that constructs it; for in calling on him to exercise a new function, it confers on him, so to speak, a richer orga- nization, being an artificial organ by which the natural organization is extended.”87 
We may want to go a step further here and claim that the enterprise of Bergsonian organology consists in a philosophical attempt to recur- sively undo the stratified view of science, thus refusing positive science as the starting point of philosophizing by reconstructing a genesis of the intelligence and matter from the point of view of interaction and creation. Evolution is creative only because it is not confined to the geo- metrical order that is science, and, by struggling to return to that from whence it emerged, it constructs through the matter upon which it acts a new organ corresponding to a new function. For a swimmer, water is not the matter that he or she wants to geometrize and to overcome, but rather water becomes part of his or her body, without which there will be no advancement in movement. A good swimmer is not necessarily physically stronger than a bad swimmer, but he or she knows better how to organize him- or herself and the water in order to set both of them into effective movement. In Creative Evolution Bergson states that “instinct perfected is a faculty of using and even of constructing organized instruments; intelligence perfected is the faculty of making and using unorganized instruments.”88 Matter in relation to intelligence is inorganic instrument spatially schematized—namely, the organized inorganic. This is even more clearly pronounced later, in the Two Sources of Morality and Religion,89 where Bergson writes: 
Let us merely recall the fact that life is a certain effort to obtain certain things from raw matter, and that instinct and intelligence, taken in their finished state, are two distinct means of utilizing a tool for this object; in the first case, the tool is part of the living creature; in the other, it is an inorganic instrument which man has had to invent, make and learn to handle.90 
Instinct and intelligence stand here as two modes. One is ready-to- hand, the other present-at-hand, but the invention and apprenticeship of tools leads back to an embodied and naturalized use. Thus, when Bergson talks about order and disorder, he tries to demonstrate that order and disorder are not two absolute terms, but rather relative. What is disordered is called such just because it is not confined to a specific order, but this doesn’t mean that it is lacking in order. If order and dis- order are relative to each other, then Bergson brings evolution back to the infinite possibility of life from the destiny of the intelligence. This could be seen as a fundamental difference between organicism and organology; that is, whereas organicism studies the relations between different parts in the system—for example, an organism—organol- ogy extends beyond organic form to reintegrate the inorganic into an organized whole; we may call this constant integration of the organized inorganic evolution. In order to integrate them, it will be necessary to free them from being present-at-hand, and this is the reason for which it is creative. Life is fundamentally artificial. This artificiality must be thought in terms of organology, in which creativity is released from the rigidity imposed by the intelligence. Such liberation is possible only through a doing-undoing process, since there is no undoing if there is no doing, and there is no doing where life is not already presupposed. Like intelligence, a machine-learning algorithm may allow us to turn an ordinary photograph into one that resembles the style of a Klee or a Van Gogh, but it is nothing creative in the sense that what it does is a geometrical mapping of pixels. We have to admit that it is a useful function that can be integrated organologically, but it is not one that could replace creativity. 
§30. NORMS AND ACCIDENTS 
Like Bergson’s organology, which seeks an ontological ground of life by rethinking the movement of matter and intelligence by situating them in the reality of the élan vital, Canguilhem also sees that the ques- tion of technics in life is beyond any scientific conceptualization. As he says toward the end of “Machine and Organism,” organology has the advantage of showing “man in continuity with life through technique prior to insisting on the rupture for which he assumes responsibility through science.”91 The same thesis is repeated in a different way in “Aspects of Vitalism,” where Canguilhem invokes Hegel’s cunning of reason (List der Vernunft) to describe this strategy. For Hegel, reason employs the ruse of taking a detour through other objects in order to achieve its aim. This detour seems at first glance contingent and ines- sential, whereas it is in fact necessary: 
Mechanism, as is well known, comes from “m??chan??,” whose meaning, “engine,” contains two senses: that of ruse and stratagem, on the one hand, and that of machine, on the other. One could ask whether the two meanings do not amount to just one. Is not man’s invention and utilization of machines, and technical activity in general, what Hegel called the ruse of reason? The ruse of reason consists in reason’s accomplishing its ends through the intermediary of objects acting upon one another in conformity with their nature. Essentially, a machine is a mediation or, as mechanists say, a relay. A mechanism does not create anything—and therein lies its merit (in-ars)—but it can be constructed only through art, and it is a ruse. Thus mechanism, as a scientific method and as a philosophy, is the implicit postulate of all usage of machines.92 
The mechanist philosophers want to explain life through mechanism, that is to say, to explain life without life.93 The vitalists respond by showing that life cannot be reduced to any physico-chemical and cel- lular principles. For Bergson, artificial systems are mechanical but not real. Science, when it becomes mechanistic, prevents us from compre- hending the creativity which is life itself. Life is a recursive process of making in the unmaking, in which science is necessary in the sense that 
it is a making to be unmade in order to make. In contrast to the presen- tation of a negative necessity of science in the current of the élan vital, Canguilhem gives a more positive role to science and proposes that the task of vitalism is to search for the meaning of the relation between life and science.94 In “Le concept et la vie” (1966), Canguilhem also addresses the genetic program and information theory of biology, which he refuses to see as a mere “imported metaphor”; rather, he says, it is important to seize the “development and progress of knowledge that it offers.”95 Canguilhem wants to emphasize, in line with the vitalists of the Renaissance, that life seeks “to put mechanism back into its place within life”96—or, in his own words, to “rejoin life through science.”97 This is the spirit of general organology. 
A theory of form, which is also metaphysics, attempts to compre- hend the one and the all, and neglects the constant materialization of fire, which is technics. A theory of life attempts to address the dynamic between the organic and the inorganic through the notion of creation, motivated by the whole, which is life itself. For this, we must pay atten- tion to technical reality, which, like the human reality, has a dynamics of its own. The schemes and forms inside the mind of the engineers are exteriorized in material terms, thus effectively detaching themselves from their confinement to mind and released into the world of which they are going to take part. These material beings are no longer schemes, and therefore it makes almost no sense to criticize technical objects as merely mathematics, without understanding that these are also material beings. In other words, by reducing technical objects to their schemes, one returns to idealism and unconsciously denies materialism. The cre- ated is never equal to the scheme that created it, nor is it equal to the élan vital that runs through the process of creation. At stake here is not only the question of form, of artificial organs, but also the question of life in light of the fire. Canguilhem agrees with Marxist philosophers in interpreting biological phenomena in terms of a dialectics,98 but such an interpretation is valid only since life struggles against its mechanization as a permanent demand (exigence).99 
Life in the philosophy of Bergson and Canguilhem is creative, but this creativity takes different forms. Guillaume le Blanc has pro- posed understanding this difference between Canguilhem and Bergson through Canguilhem’s writing on Alain’s work on art.100 Both Alain and Bergson move away from the Platonic conception of art, meaning the imitation of the ideal in the real, since they regard the realization of art as an encounter with contingency—not the least that painting, writing (poetry), and composition (music) are open to the contingency of the muscles.101 But therein also lies a fundamental difference. For Alain, art tends to arrest a movement and to fix a form, since immobility is the basis of representation. For Bergson, however, it is the opposite, and such a fix is rather a constraint on becoming. Even in the case of con- templating a work of art, maintains Bergson, a prolongation of move- ment is demanded, since art reveals the reality, the “universal reactive duration.”102 However, Canguilhem thinks that both Alain and Bergson are still unwittingly repeating a form of Platonism. Having explicitly disavowed Platonism, Alain proposes to fix the form, and Bergson also refuses error and nothing (néant), because nothing is an error. As Le Blanc points out, Alain and Canguilhem want to understand force “in the risk of inadaptation, of error and errancy,”103 while, on the contrary, it carries little importance in Bergson’s thought: “By forging the con- cept of force (élan vital), both in life and in art, its ontology of the real, excluding both non-being and the void, makes impossible the misuse of force. If there is good production and invention in life as in art, there is no such place, and it is the whole problem, for the loss, the error or the failure.”104 
This may remind us of Bachelard’s criticism that “while composing his epic of evolution, Bergson no doubt had to disregard accidents.” Bachelard proposes to create a doctrine of the “accident as principle,” as he says that there is only one general law in truly creative evolu- tion—“the law that an accident lies at the root of every evolutionary attempt.”105 This doesn’t mean that there is no contingency in the thought of Bergson. Indeed, Bergson emphasizes that contingency has an important role in evolution, since it is contingency that introduces divergence into the path of evolution. It is also true that Bergson didn’t thematize the question of accident, since he prioritizes duration as multiplicity in unity. If “loss, error, and failure” don’t pose problems for Bergson, it is because Bergson also rejected adaptation. By adapta- tion here we refer to the organism’s capacity to coordinate its organs in response to changes of its milieu.106 Bergson criticized adaptation as a manifestation of mechanism and finalism,107 so there is nothing called inadaptation for him. Nor is there an inadaptation for Canguilhem from the perspective of biology, since inadaptation means simply elimina- tion. However, it is possible to talk about social inadaptation.108 
If we follow Le Blanc in saying that Bergson didn’t go far enough to take up the necessity of failure and error, in Canguilhem we find error, as Michel Foucault says, as the “permanent contingency [aléa] around which history of life and becoming of humans entwine.”109 Error or contingency is addressed in his understanding of the patho- logical, since the abnormal, or irregularity, is a necessary part of life. As Canguilhem states in The Normal and the Pathological, “irregular- ity and anomaly are conceived not as accidents affecting an individual but as its very existence”110—or, in the words that he borrowed from Gabriel Tarde, “the normal is the zero of monstrosity.”111 August Comte and Claude Bernard112 maintain an identity between physiol- ogy and pathology; or, in other words, the pathological is that which exceeds the upper and longer range of the normal, the latter being the subject of physiology.113 Canguilhem questions the identity between physiology and pathology, maintaining that the latter is derived from the former, by giving a new definition to the pathological. 
The pathological is not the lack of norm or order, but rather a norm that deviates from those of health, so there is no opposition between the normal and the pathological, but only between pathology and health115: 
What characterises health is a capacity to tolerate variations in norms on which only the stability of situations and milieus—seemingly guaranteed yet in fact always necessarily precarious—confers a deceptive value of definitive normalcy. Man is truly healthy only when he is capable of several norms, when he is more than normal. The measure of health is a certain capacity to overcome organic crises and to establish a new physi- ological order, different from the old. Health is the luxury of being able to fall ill and recover.116 
As some authors have proposed, the influence of Kurt Goldstein on Canguilhem is evident. Goldstein refuses any statistical definition of normality and health, but rather considers what he calls the “individual norm”: “[T]here is only one relevant norm; that which includes the total concrete individuality; that which takes the individual as its measure; it is therefore a personal individual norm.”117 Every illness means a new process of adapting to the milieu, which will result in a new individual norm. In the same spirit, Canguilhem claims that “the pathological can be distinguished as such, that is, as an alteration of the normal state, only at the level of organic totality.”118 On the case of diabetes, Can- guilhem questions Bernard, who claims that glycosuria is only a func- tion of glycaemia and that the kidney functions by means of a constant threshold, by referring to newer researches that showed that the renal threshold is not fixed and static but rather mobile, depending on the specific condition of the individual.119 
§31. THE UNCANNY FIRE 
Pathology is defined in terms of the weakening of the organism’s capac- ity to adapt to its milieu. A healthy organism is able to adapt itself to a variety of milieus and adopt certain elements of the milieu to empower itself, while a pathological organism can adapt itself only to a specific milieu in order to maintain its interior milieu. Inspired by Goldstein and the German zoologist Jacob von Uexküll, on the other hand, Canguil- hem maintained that one should consider the organism and its milieu as a whole. Canguilhem cites Goldstein’s view that biology “has to do with individuals that exist and tend to exist, that is to say, seek to realize their capacities as best they can in a given environment.”120 The ques- tion of milieu or environment is in between the two dimensions of the organology (here we follow André Leroi-Gouhran): on the one hand, the exteriorization and liberation of the organ, and on the other hand, the internalization (interiorization) of the environment as an organic whole: “An environment is normal because a living being lives out its life better there, maintains its own norm better there. An environment can be called normal with reference to the living species using it to its advantage. It is normal only in terms of a morphological and functional norm.”121 
Recursivity is the mechanism of the norm established between the living being and its milieu. It is not a mere imposition, like mechanical laws, but rather a Spinozist “immanent causation.” We can therefore understand why, commenting on Canguilhem, Pierre Macherey claims that there is an immanent causality of the norm, which emerges from the subjects subjected to it.122 Norms change when an event occurs that exceeds the normalizing capacity of the norms. Disease, when seen as this type of contingent event, leads to the establishment of a new norm, which is also a new relation between the organism and its milieu. Certainly it may take time for a symptom to appear, but to the patient it appears as a sudden event interrupting the previous routine and harmony. Disease, both physiological and psychological, arises out an inability to adapt to and adopt the existing milieu.123 The launch of a new round of adaptation through technical aides, as a recursive process, is a necessary condition of knowing life: 
Disease reveals normal functions to us at the precise moment when it deprives us of their exercise. Disease is the source of the speculative attention which life attaches to life by means of man. If health is life in the silence of the organs, then, strictly speaking, there is no science of health. Health is organic innocence. It must be lost, like all innocence, so that knowledge may be possible. Physiology is like all science, which, as Aristotle says, proceeds from wonder. But the truly vital wonder is the anguish caused by disease.124 
If in philosophy of biology the organism presents itself as an organic form that includes the organism, its tools, and the environment, the organic is always already outside of itself with other organic and inorganic beings, while it depends on the capacity of the organism to define the boundary of such an organic totality, like a tick defines the dynamic of its Umwelt (internalized outer environment) by filtering its Umgebung (surroundings). Humans are not ticks, however; they also organize their environment with their tools—which is also the com- mencement of a pathological journey. Indeed, humans not only change their environment but also build completely new environments—for example, the emergence of factories during the Industrial Revolution. The submission of the organic body to mechanical machines produces a malaise that originates from the enforced adaptation to the rhythm and operation of the machines, since division of labor according to machineries is also a fragmentation of knowledge and the organicity of the body. Will it be possible to reverse the situation by turning it around—that is, by submitting machines to the organic body? Berg- son thinks that the consciousness in humans enables them to escape the captivation of mechanism, so they propose to “create with matter, which is necessity itself, an instruction of freedom, to make a machine which should triumph mechanism.”125 In the last remark of “Machine and Organism,” Canguilhem cites the sociologist Georges Friedmann’s critique of industrial mechanism and his idea of developing a new mode 
of work in which machines are adapted to human organisms: 
Friedmann sees the development of a technique for adapting machines to the human organism as an ineluctable revolution. This technique seems to him a scientific rediscovery of the empirical processes by which primitive peoples have always sought to adapt their tools to the organic norms of an efficient and biologically economical action—that is to say, an action that situates positive value in the evaluation of technical norms within the organism at work, which spontaneously defends itself against any exclu- sive subordination of the biological to the mechanical.126 
Friedmann sees the man-machine adaptation brought about by Tay- lorism as a reification of workers, and argues that the workers’ reac- tions against Taylorism should be understood as both a biological and a social defense—or, in other words, the defense of health. This situation thus demands a new relation between human and machine to liberate workers from the subordination of the biological to the mechanical. It is not our priority to assess the judgment of Friedmann in detail and to situate it in the history of labor; it is worth noting that his proposal resonates with many other theorists of technology of his generation, including Lewis Mumford, mostly influenced by Whitehead. However, today we know that working conditions do not resemble the adaptation of instruments to the organic, but rather that machines are themselves becoming organic—a new dialectics toward a techno-systemic totality. Wiener’s claim concerning the obsolescence of the opposition between Newtonian mechanism and Bergsonian vitalism marks the beginning of an epistemological revolution that effectively integrates human and machine into numerous feedback loops. The form of automation that Marx described in Das Capital is not what we normally see in factories today. Automatism is no longer about repetition but rather recursion. Recursive operations are exemplified in the imagination of future smart cities, artificial intelligence, machine learning, nanotechnology, biotechnology, and the like. We have pointed out the immanence of recursivity in the process of adaptation, which is ontological, but we also witness the recursivity in the process becoming a modus operandi that is social and technological. Like philosophy of nature, philosophy of biology confronts its limit in light of the recursive algorithms imple- mented as a dominating form of adaptation, which is what Deleuze calls societies of control.127 At the same time, there is an artificial selection (instead of natural selection) enforced by the politics of transhumanism (e.g., human enhancement, genetic engineering). 
If we take seriously Wiener’s argument that the opposition between mechanism and vitalism is dissolved in cybernetics, and we see it as a milestone of the completion of metaphysics begun in Hegel’s philoso- phy, then in what way can philosophy—the highest recursive form of thinking—still think? For sure, one can follow Heidegger by saying that it is the end of philosophy, but in order to think with and beyond Heidegger’s concept of philosophy, thinking must identify a new con- dition under which a transformation is possible in order to escape the enclosure of feedback loops. Canguilhem’s concept of life could be understood as a thinking that transcends the Hegelian identification of the notion of philosophy with that of the system by putting the system back into life.128 If through Kant the concept of the organic has been conceived to be the weapon of philosophy against mechanism and determinism, as well as an aspiration for a cosmopolitan politics, today we are encountering a rather different situation, in which the mechani- cal is, first, in the process of taking the form of the organic, and second, overriding biological evolution. In this situation we will have to trans- value thinking against any tendency toward closure—a task we will undertake in the next chapter on the organizing inorganic. 
Organizing Inorganic 
If you do not expect the unexpected, you will not find it. . . . —Heraclitus, Fragment 18 
We are more than ever living in an epoch of cybernetics, since the apparatus and environment are becoming organismic. The environment actively engages with our everyday activities, and the advent of plan- etary smartification means precisely that recursivity will constitute the major mode of computation and operation of our future environment. The recursivity of algorithms equipped with big data will penetrate into every facet of human organs and social organs. The mode of participa- tion of technology is fundamentally environmental while at the same time transforming the environment. This chapter is dedicated to a more elaborated organology and the necessity to push it further. I will argue that it is necessary to look into the question of recursivity and contin- gency in human-machine relations before we can fully elaborate on the significance of the becoming organic of the environment—which, ironi- cally, has been organic since the very beginning. Milieu-technics is the general operation of cultural-technics, as Simondon has rightly observed. Simondon’s concept of cultural-technics can be understood with the aid of the example of livestock farming, in which technologies act on the milieu instead of acting directly on living beings.1 We can understand the posthuman condition as a large-scale domestication of human beings through the manipulation of the technical system as milieu-technics, as was done in the ancient livestock farming described by Peter Sloterdijk in his article “Rules for the Human Zoo.”2 The move from self-domesti- cation through the invention of tools to the large-scale determination of the masses through the modulation of the exterior technical milieu marks the moment of the current work. This planetary domestication goes far beyond the effects of previous communication technologies—telecom- munications, radio, television, and even the World Wide Web of the twentieth century. The smartification and systematization of the environ- ment is a more advanced milieu-technic, which is closely related to the silent continuation of the cybernetic movement.3 
In the example of the livestock farming, through the application of technologies to primary nature, we have developed a second nature that envelops the earth like the noosphere of Teilhard de Chardin. Livestock farming exhausts the potential of animals, plants, and the environment, while cultural-technics sets a second nature into perpetual genesis, as Simondon observes: “[O]ne could say that culture, by managing the milieu, gives rise to the genesis of a second nature, while livestock farming is detached from all nature, detours the nature to a hypertelic dead-end for the deviated species.”4 
Organicism and organology in the twentieth century could be seen in part as an effort to incorporate this second nature into the organicity of body and culture. Oddly, this includes National Socialism in Germany, which attempted to marry Romanticism with industrialism in order to construct a nationalist ideology.5 Simondon proposes a different approach to overcoming the antagonism between culture and technics through a reinvestigation of the evolution of technical objects. Simon- don has never used the word organology, but as we saw in the last chapter, Guillaume le Blanc has suggested that organology is a common theme of Canguilhem and Simondon. We also have to remind ourselves that Simondon was greatly influenced by cybernetics, especially the concept of feedback, and also by Canguilhem, who was the supervi- sor of his supplementary thesis On the Mode of Existence of Technical Objects. Recursivity is thus at center of Simondon’s approach to techni- cal objects. We will see how Simondon’s somewhat implicit organology is further extended and complemented in the work of Bernard Stiegler. Between Simondon and Stiegler we can also find two different ways of approaching contingency. The question of contingency for Simondon remains close to a philosophy of nature, in which a contingent encoun- ter is a source of information that may trigger further individuation. For Stiegler, on the other hand, the question of contingency is closer to artistic creativity. As we might expect, the difference is due to the fact that nature remains an important element in Simondon’s theory of individuation, while for Stiegler the word nature is overcharged with too many meanings. By abandoning the word nature, Stiegler has abandoned pure first nature in favor of a second nature, which is a tech- nicized nature, a nature in which nothing is natural.6 
§32. UNIVERSAL CYBERNETICS, GENERAL ALLAGMATIC 
In chapter 3 we saw how the concept of life is closely related to that of adaptation, which is central in the theory of evolution and animal behavior. Simondon has reproached the concept of adaptation as being analogical to hylomorphism in the sense that the subject has to submit itself to the environment as the ancients did with matter and form: The inert matter gains its identity by being subsumed under a form. In L’Individuation à la lumière des notions de forme et d’information, Simondon criticizes sociologist Kurt Lewin’s theory of group dynamics, which is based on an appropriation of field theory in physics and topol- ogy in mathematics. Simondon criticizes Lewin for several weaknesses in his theory. We won’t be able to enter into all the details here, but let’s try to understand it through three major points. First, Lewin’s analysis rests on the paradigm of adaptation: that is to say, that the individual should adapt itself to the group, which is presented as a field of forces. Second, this emphasis on adaptation based on the field of force prevents us from understanding being as the operation of successive individu- ations.7 Third, the life space (locus of a person’s experiences and needs) is already individuated: that is to say, it is already a solution, because this representation, being the field of forces, doesn’t consider the dispa- rations (or asymmetry, incompatibility) according to different orders of magnitude. To sum up Simondon’s critique, he maintains that Lewin reduces individuation to a social adaptation, and that the method he uses, based on topology and field theory, is not sufficient for expressing the dynamic of individuation. On the contrary, Simondon proposes that we understand the process of individuation by approaching the notion of information in cybernetics. 
We would like to see more precisely how, with the notion of infor- mation, Simondon refuses both hylomorphism and substantialism. By doing so he replaces substance with relations and introduces contin- gency into the structure and operation of relations. If we follow Simon- don, these two ancient theories of individuation presuppose a principle of individuation (which is based on already individuated elements, for example, form and atom) anterior to individuation. It fails to explain individuation since the individuation of the presupposed elements of such a principle has to be explained. Information provides a conceptual tool applicable to all orders of magnitude, whether it be the micro-level of electrons or the macro-level of steam engines. A contingent event brings information into the system and produces a signification to certain elements in the system or the system as a whole, which in turn triggers a new process of individuation. This is the reason that, toward the end of his book on individuation, Simondon claims that “the task of discovery of significations and collective is submitted to chance [hasard].”8 This submission to contingency would remain a vague philosophical proposition if there were no further elaboration. This gesture of positioning individuation on the concept of contingency or chance may risk obscuring the understanding of individuation. 
I would like to propose that it is in On the Mode of Existence that Simondon attempts to integrate such a notion of contingency into technical organizations. As there are good and bad infinities, there are also good and bad contingencies: luck or catastrophe. The organiza- tion of a machine should be valued by its capacity to deal with these different notions of contingency and their classification, instead of mere automatism. One of the most significant forms of organization is what Simondon calls an open machine. An open machine is one that possesses a margin of indetermination, meaning that it is not insen- sitive to “pure chance,” for example, noise. An amplifier without a margin of indetermination is not able to deal with noise, since it will amplify noises as if they are desirable signals. Contrariwise, an ampli- fier with a margin of indetermination is able to tolerate these noises by undermining them so that they will not be amplified. A machine that is sensitive to information is able to distinguish different patterns of sound waves, so that the irregular ones can be eliminated and the regular ones will go through. There are different ways that this can be achieved. The most straightforward way will be to have a mechanism that filters noises in consideration of their irregularity. However, this may also eliminate desirable waves; therefore, the most ideal case will be to have a feedback mechanism that governs this learning process. It was too early for Simondon to anticipate machine learning, which arrived only toward the end of the last century. Apparently he was very much nurtured by cybernetics, especially the notion of feedback. Simondon sees in cybernetics a correction of both positivism (Comte) and criticism (Kant), which he calls phenomenalist objectivism. Positivism tends to privilege structure over operation, and excludes the study of operations from the realm of science. Criticism, on the other hand—especially Kant’s first two Critiques—separates knowl- edge from action.9 The separation between structure and operation in phenomenalist objectivism is a preparation for their unification through reflective thinking. In positivism the synthesis takes place in humanity, which becomes “the absolute principle of normativity.
in criticism, it is manifested in respect [Achtung].”11 It is clear that Simondon was referring here to the third Critique, in which reflective judgment is fully elaborated, and which is attempted as a unification of the first two Critiques. Since this reflective thinking has its suc- cessor in cybernetics, Simondon claims that Kant could deal with cybernetics only by situating it in the Critique of Judgment.12 Cyber- netics, for Simondon, designates a new epistemology that has to be distinguished from Cartesian epistemology. As he repeated on several occasions, “automatism is not cybernetics,” “a robot has nothing to do with cybernetics.”13 He calls the Cartesian and cybernetic forms of reasoning two different cognitive schemes: one linear and based on logical propositions, the other based on recurrent causality. 
Recurrent causality (causalité récurrente) is a causality that comes back to itself to act on itself—a translation of feedback that Simondon sometimes also calls internal resonance.14 Recurrent causality is recur- sive. It is in this sense that Simondon calls technical objects that possess an associated milieu constituted by recurrent causality technical indi- viduals—in contradistinction to technical elements (e.g., a gear, a diode, or a triode) and technical ensembles (e.g., a laboratory or a factory). This capacity of coming back to itself in order to determine its next move is the criterion of an “individual” in the sense of a “living being.” For sure, recurrent causality is a reference to feedback, but doesn’t this also resonate with Wiener’s critique of the opposition between Newto- nian and Bergsonian time that we looked at in the previous chapter? If the opposition between Newtonian mechanism and Bergsonian vitalism is sublated, it is only because a third, organicist form of organization is identified that preserves both of them and elevates them to a higher potency. 
The organ is that which is able to condition itself. In the genesis of organs there is an effort toward convergence that systematically binds the organ with the other organs, as well as the body as a whole: 
[A] vital work [une œuvre de vie] is required to take the leap beyond a given reality and its current systematization, toward new forms that only maintain themselves because they exist all together as a constituted sys- tem; when a new organ appears in the evolving series, it maintains itself only if it realizes a systematic and pluri-functional convergence. The organ is its own condition. It is in a similar manner that the geographic world and the world of ready existing technical objects enter into a rela- tion in which concretization is organic, which defines itself through this relational function.15 
This is the only passage in On the Mode of Existence where Simondon discusses “organic” concretization. What does it mean that the organ becomes its own condition? It means that it is situated in a system and in reciprocal relations with other parts; it adapts itself to the system while at the same time modifying the system, which in turn conditions its further mode of operation; it becomes its own condition through the feedback of the whole organic system. In a preparatory text titled “Cybernétique et philosophie” (1953), Simondon used the term holistic (holique) to describe this form of organization of cybernetics.16 It is pos- sible to generalize this “organic” thinking of technology through one of his favorite examples, the Guimbal engine, which he mentions toward the end of the above citation on the relation between the geographical world and the world of technical objects. A Guimbal engine is a turbine that uses the river as both a driving force and cooling agent to reduce overheating caused by the Joule effect, which may burn the engine. The river and the turbine become an organologically functional unity. The river in this case is the associated milieu of the turbine. The associated milieu is central to the organic structure of the turbine since it allows such a recurrent causality to take place: When the current is stronger, the turbine produces more heat, while because the water flows faster it also takes away heat more efficiently. 
Like Wiener, Simondon recognizes the “becoming organic” of technical objects: As he says, “[S]ince the mode of existence of the concretized technical object is analogous to that of natural spon- taneously produced objects, one can legitimately consider them as one would natural objects; in other words, one can submit them to inductive study.”17 The Guimbal engine is an excellent example of the holistic organization facilitated by recurrent causality. Simondon and Wiener speculate on the possibility of the becoming organic of machines, which was later elaborated further by Günther as self- consciousness. However, we should do justice to Simondon’s effort to go beyond Wiener and Ashby’s cybernetics. Cybernetics desig- nates a new epoch for science, but Wiener’s 1948 book Cybernetics, for Simondon comparable to Descartes’s Discourse on Method, does not yet fully define the cybernetic method. Simondon therefore sees that the most urgent task is to reclaim a cybernetic thinking, which he calls general allagmatic18: 
This third discipline, synthesis of cybernetics and positivism, will not only be an axiology of knowledge but also a knowledge of being: it will define the real relation of the operation and the structure, the pos- sible conversions of the operation in structure and structure in operation and structure in the same system. Such will be the scope of the disci- pline; indissolubly scientific and philosophical, which we have named allagmatic.19 
Simondon understands Wiener’s cybernetics—and we can also add to it the general systems theory of Bertalanffy and the organicism of Needham et al.—as a mathematics of operation. In the above quote, he seems to suggest that cybernetics prioritizes operation over struc- ture. However, this doesn’t seem to be appropriate, since all systems presuppose structures. It also doesn’t make much sense to say that cybernetics, which was largely influenced by Bertalanffy, ignores structure. Simondon’s real contribution beyond cybernetics can be grasped only toward the end of this chapter. For now we can say that the core of the general allagmatic constitutes a theory of conver- sions between structure and operation. The equivalence between the machine and the organism that Wiener was hinting at is problematic for Simondon. The cybernetic machine that is invented according to the study of certain behaviors of organisms has only a functionalequivalence with them but not necessarily an operational equivalence. For example, Alpha Go may have the same functions as the Go world champion, but they don’t necessarily have the same operations. Func- tional equivalence is closely related to its economic value. Marx does not make any distinction between operation and function in his eco- nomic analysis of machinery, and the general intellect (allgemeiner Verstand) remains a functional equivalence of categorical analysis. This is the reason that the functional equivalence that we find in arti- ficial intelligence poses the threat of mass unemployment. The general allagmatic is a universal cybernetics in the sense that it aims to go beyond the particular or specific cybernetics (for example, psychology and sociology) in order to conceive of a genesis in which operation and structure are constantly interacting. It is not only an axiology of knowledge but also a knowledge of being—namely, ontogenesis or onto-axiology—meaning that it unifies action and value, operation and structure. The emergence of new values depends on the existence of a problematic, for it is the problematic that presents an incompat- ibility that has to be resolved in order to arrive at a new metastabil- ity qua compatibility. Simondon thus claims that “the axiological function is therefore aspect of structural modification of a holistic system.”20 The axiological function is both exterior and interior, like the citizens and the civic regime in the Greek polis, constituting a holistic structure and operation.21 
§33. RECURSIVITY IN PSYCHIC AND COLLECTIVE INDIVIDUATION 
General allagmatic is a thinking that is central to Simondon’s theory of individuation. The conversion between operation and structure demands a genesis in which they take part. Individuation is a recursive process whose dynamic is reciprocal (between parts) and holistic (as a whole). The questions we would like to raise here are the following: 
What is the role of technology in this allagmatic thinking? What is the relation between contingency and technology? L’Individuation is an attempt to reconsider the question of being and becoming according to a new epistemology suggested by cybernetics. It is still not a treatise on organology but on philosophy of nature; that is to say, it doesn’t yet address the relation between human and machines, machines and world. We would like to suggest instead that Simondon proposed an organo- logical thinking in On the Mode of Existence, but that this organological thinking was almost invisible in his principal thesis since technology was not yet thematized in his theory of individuation (of physical being, living being, and psychical being). It is organological in two senses. First, technology is the posteriori becoming a priori: For example, memory is empirical, hence posteriori, but once it is recorded it is then a priori since it becomes the condition of new experiences.22 Second, there is an organic whole between the individual and the collective that cannot be separated like fish and water; this organic part-whole relation is the condition of the theory of individuation. 
We will attempt now to identify the role of recursivity in Simondon’s allagmatic thinking of which individuation is an exemplar. We can understand the individuation of human being as an operation (com- munication and conversion) between two realities or two structures, the psychical and the collective, initiated by a problematic. We may want to understand the resolution of the problematic as occupying the place of the end or the telos here. Simondon suggests that both psy- chologism and sociologism make a fatal mistake, since they attempt to substantialize these two realities and confront them as oppositions. An analysis starting with either of these two polarities confronts its own limit. This tendency of substantializing comes out of the desire to seize the essence of the human.23 Psychologism holds the view that the social is a projection of the interior activities of the individual without tak- ing into account the tensions within the individual, while sociologism sees the individual as a product from the point of view of exteriority, without taking into account the agency of the individual. For Simondon, the individual and the social are not substantial realities but rather ensembles of relations. 
Simondon holds the view that individuation is at the same time psy- chic and collective. This means that we cannot separate the psychical from the collective, since the psychical is always already transindivid- ual. This transindividual relation of the living being is characterized by an interaction between perception and action in which the problematic is normally resolved by the intervention of affectivity.24 The psychical intervenes when affectivity cannot succeed in resolving the problematic and is therefore obliged to relinquish its central role in individuation.25 According to Simondon the psychic problematic cannot be resolved on the “infra-individual level” since “the psychic life goes from the preindividual to the collective.”26 In order to understand what he means by this, let’s take solitude as an example. Solitude is not a rupture from all relations with the world; on the contrary, it is transindividual in the sense that it is always in search for an outside, without which there is only isolation: “The veritable individual is that which has passed through solitude; what he discovers beyond solitude is the presence of a transindividual relation. The individual finds the universality of rela- tion by means of the test [épreuve] that is imposed on him, and it is a test of isolation.”27 
Simondon cites the example of the tightrope walker in Nietzsche’s Also Sprach Zarathustra, who was abandoned by the crowd after hav- ing fallen down to the ground. Zarathustra feels affection toward him and takes the cadaver on his shoulders to bury it. Of this Simondon writes: “[T]his is through the solitude, in Zarathustra’s presence to a dead friend abandoned by the crowd, that the test of transindividual- ity starts.”28 This encounter with the tightrope walker—or better, this exceptional event—is the beginning of a discovery of the transindividu- ality that leads to a new individuation. This transindividuality is consti- tuted by the two poles of interiority and exteriority, which consist of a recursive movement: the interiorization of the exterior and the exterior- ization of the interior.29 This means precisely that, as in memory, what is posteriori becomes a priori—not a priori in the strict sense of being transcendental, but rather in the sense that it becomes the condition or the criteria of selection. 
Psychical and collective individuation is carried out through such a recursivity, and it is in this sense that we understand that the collective cannot be separated from the psychical (or vice versa). A recursive model expresses psychical and collective individuation better than the process of crystallization that Simondon uses to describe physical individuation. There is a limit to the analogy of crystallization precisely because physical being such as crystals and supersaturated solutions have a lower potency, which means its capacity to act is rather limited, while action or movement is primordial to the psychical and collective individuation. A psychic being will not wait, like the solution, until it becomes supersaturated. Instead, it is in constant search of informa- tion to maintain or decrease its entropy, while such a search originates from an excess, or ekstasis, in the sense of Heidegger. The elements that Simondon names in crystallization as a paradigm of individuation remain valid, however—for example, the preindividual (the reality in which individuation takes place), disparation (tension within the indi- viduating system), and metastability (indicator of the resolution of the problematic). 
The preindividual is the reality that cannot be exhausted and always remains as the background against which individuation takes place. Like Schelling’s natura naturans, which designates the infinite pro- ductive force of nature, for Simondon nature is the a priori of indi- viduation, meaning that it offers a preindividual reality to individuation. Individuation for Simondon always presupposes the preindividual, meaning a reality that is given and carried in the individual as potential- ity. Individuation cannot exhaust this preindividual reality. Instead, it is conserved in the individuated being and becomes the primary condition for the next process of individuation. Simondon identifies the concept of the preindividual with the Ionian physiologists, especially Anaxi- mander’s apeiron. In chapter 3 we saw how this apeiron is inscribed in the peras, and such inscription constitutes the dynamic of the individ- ual. The preindividual is not a cause, but rather the potential or resource through which the path from cause to effect is actualized. For example, the supersaturated solution, which is charged of potential and hence in this case a preindividual reality, doesn’t lead directly to crystallization; rather, crystallization is realized by a contingent event, for instance, the supply of heat. The preindividual is more than unity as well as more 
than identity30; in other words, it is the hidden excess: 
One could name nature this preindividual reality that the individual car- ries with it, in order to find again in the word of nature the signification that the pre-Socratic philosophers have put: the Ionian physiologists found the origin of all the species of being, anterior to individuation; nature is reality of the possible, under the species of this ??π????? by which Anaximander produces all individuated forms: Nature is not the contrary of man, but the first phase of being, the second being the opposition of the individual and the milieu, compliment of the individual in related to all.31 
The preindividual belongs to the ground that gives form. That which plays the role of the energetic is not forms but rather the ground, which carries forms. The ground is the system of virtualities, potentials, and forces, while the form is the system of actualities.32 The concept of metastability is a counterconcept of equilibrium that we can find in the homeostasis of Ashby, because homeostasis is determined by the search for equilibrium that for Simondon is the impasse of individuation: death. Disparation—incompatibility or asymmetry—is the motor of individu- ation, which obliges the being in question to pass into act in order to resolve the tensions thus generated. To act means to elevate and to meta- stabilize. Every successful individuation is like a quantum leap in the sense that there is an elevation from one discrete energy level to another, and precisely due to the discreteness of energy levels, it also gives a metastability, meaning that the next phase of individuation will take place when further conditions (material, energetic, and informational) are met to overcome the threshold of individuation. Metastability is like the effect of the Hemmung that we find in Schelling, since it is a transition- ary product of the recursive movement that characterizes the movement of the spirit, with the difference that Simondon’s model is more subtle thanks to the discovery of the notion of information. The recursivity of psychic and collective individuation is even more evident when we consider that toward the end of the introduction to L’Individuation, Simondon says that “the knowledge of individuation is the individuation of knowledge,” which means that there is a coupling (in the sense of the autopoeisis of Varela and Maturana) between the knowing subject and the totality, including the subject and its environment. One can also see it as a reformulation of what Bergson says about the theory of life and the theory of knowledge. How are Simondon’s and Schelling’s models different from each other—or, more precisely, in what sense is Simondon not an idealist? 
Unlike Schelling, who proposes a construction of matter from force, Simondon does not suggest the construction of matter through infor- mation. Information is the extra of the energetic and material process: “extra” in the sense that it is not reducible either to energy or to matter, but is omnipresent in the process. Information is the disparity that carries in itself a signification: signification in the sense that it carries meaning which the system cannot ignore. Information is only one of the condi- tions of individuation, along with which we can also find material and energetic conditions. If force can be reduced to an effect of energy (for example, that resulting from the transformation from potential to kinetic energy in the case of the pendulum), information cannot be reduced to either matter or energy. As a signification, information differs from noise precisely because it carries meaning while noise doesn’t necessarily carry meaning; when noise carries meaning, it is already information. Information is not pure chance, and as we have already seen in the intro- duction and chapter 2, it is both recursive and contingent, as in Bateson’s definition of it as “a difference that makes a difference.” 
The concept of information originates in communication theory and mathematics, so it is first of all a technical concept, and has its reality in technical objects. A technical object that carries information is that which allows transindividual relations to be established between psy- chic individuals: a book, for example, which can be shared by multiple readers and form a collective (e.g., a reading group or a fan club). A book, when it is used inappropriately (for example, if it is used as a prop for an unstable desk), loses its information and ceases to possess transindividual relations, which also means it loses its signification. In other words, there is a reality in symbols and technical objects that is already a synthesis.33 As Simondon writes: “[T]he technical object understood according to its essence, that is to say technical object as long as it is invented, thought and wanted, assumed by a human subject, becomes the support and the symbol of this relation that we would name transindividual.”34 Simondon therefore endows technical objects with the role of facilitating the process of individuation: 
By the intermediary of the technical object an interhuman relation is cre- ated. That is the model of transindividuality. . . . The relation to technical objects cannot become adequate individual by individual, except in some very rare and isolated cases; [the relation] can only be instituted under the condition that it succeeds in bringing this collective inter-individual reality into existence, which we call transindividual, because it creates a coupling between the inventive and organizing capacities of multiple subjects.35 
If we follow what Simondon says concerning transindividual rela- tions, it is possible to conceive an organological thinking that is beyond artifacts (for example, the Guimbal engine), and that extends toward psychic and collective individuation. Transindividual relations are embedded in technical objects and modulated according to their opera- tional and organizational schemes. The evolution of technical objects thus constantly shifts the theater of individuation by reconstructing the stage with new forms of transindividual relations and new dynamics. Cybernetics, with the notion of feedback and information, has intro- duced a new cognitive scheme, and consequently a new organization of human-machine relations and sociality. Simondon relates his inter- pretation of the technical lineage—from “elements” to “individuals” and “ensembles”—to specific historical epochs. He sees that techni- cal elements represent the optimism of the eighteenth century, which longed for infinite progress and the constant amelioration of human life. Technical individuals, which appeared in the nineteenth century as automated machines in factories, displaced human beings from the cen- ter of production. And in the twentieth century Simondon understands technical ensembles, with the emergence of information machines and cybernetics, as a new project of organizing transindividual relations in order to resolve the problem of alienation. 
Simondon calls this way of studying and engaging with technology mechanology. This discipline, which aims to overcome the antagonism between culture and technics and between technics and nature, remains an unfinished project. The organological thinking that he opens up through the process of individuation (understood as an operation that consists of several phases, from the intensification of tensions to the resolution of these tensions in the form of metastability) and individu- alization (understood as psychosomatic schematization and physical concretization) provides a theoretical framework for understanding the relations between humans and machines, and this framework has to some extents already moved beyond that of Bergson and Canguil- hem. This is because, unlike the latter two thinkers, Simondon did not pretend to develop a philosophy of biology, but rather a philosophy of individuation, in conjunction with a philosophy of technology. How- ever, this conjunction remains most of the time implicit in Simondon, since he did not elaborate on the role of technical objects in the process of psychic and collective individuation, and therefore the dialogue he had with cybernetics must be taken much further. 
§34. AN ORGANOLOGY OF CONTINGENCY 
Bernard Stiegler’s general organology could be seen as an extension of Simondon’s analysis that reinterprets the major terms in Simondon’s theory of individuation and individualization. Stiegler started elabo- rating on the concept of general organology since 2003.36 In contrast to the organology of Canguilhem, which was closely related to the organic whole proposed by authors such as Kurt Goldstein, in Stiegler we find less emphasis on the concept of the organic whole and more upon the functional organs. This is because the term organology for him is derived less from Bergson and Canguilhem’s philosophy of life and more from musicology. It is not so much Stiegler’s resonance with Bergson and Canguilhem as his deviation from them that interests us here. Stiegler rarely refers to Canguilhem’s “Machine and Organism,” in which Canguilhem attempts to sketch his concept of organology. We can reconstruct the concept of recursivity and contingency, since both 
of them are of extreme importance in Stiegler’s thought: Contingency for him is the quasi-cause, while recursivity often takes the name of repetition. 
If we say that the concept of recursivity exists in Stiegler’s think- ing, it is because this recursivity is inscribed in the circuit of retention and protention. These are two terms borrowed from Husserl’s theory of inner time-consciousness, where retention names the capacity of remembering or retaining, and protention the capacity of anticipation. Basing himself on Husserl’s concept of primary and secondary reten- tion/protention, Stiegler develops the concept of tertiary retention. To provide an example, when we are listening for the first time to Johann Strauss’s An der schönen blauen Donau, we will retain every now of the melody. Since every now is always already no longer, this reten- tion of the melody is called primary retention. At the same time, since I also anticipate the coming melody—since without this I won’t be able to comprehend the phrasing, and there will be no music but only sound—this anticipation of the coming now—the not-yet—is called primary protention. If tomorrow I remember the Blauen Donau, it is no longer a temporary retained now, but rather recollection: namely, memory or secondary retention. And since I already have memory of the music, I am able to anticipate the end of every phrasing and the end of the piece, which is called secondary protention. On the basis of these concepts of primary and secondary retention and protention, Stiegler suggests what he calls tertiary retention: namely, artificial memories. For example, my secondary retention of the Strauss piece is not reliable, as it is blurred over time, but a CD may help me to recover my memory. Now the gramophone (analog) or the CD or MP3 (digital) are tertiary retentions that in some way invoke the primary and secondary reten- tions and protentions, like Proust’s madeleine, but it is more than the madeleine since it is characterized by an exactitude that Stiegler calls “orthothetic,” a neologism from the Greek orthot??s meaning “exacti- tude” and thesis meaning “position.”37 Primary, secondary, and tertiary retention together with primary and secondary protention thus form a circuit in which the soul is no longer simply a movement that returns to itself to determine itself, but rather the soul, whose activity is the noesis, is also a tekhnesis, whose organization depends on the third memory. 
The third memory is a compensation to the retentional finitude of the organism, since an organism cannot retain all its experience and cannot transfer this experience to the next generation without having exterior- ized them as symbols and tools. Furthermore, the secondary retention, which we call memory, can be effectively activated only through the tertiary retention, since it is the tertiary retention (for example, writings or images) that provides the force of synchronization and the diachro- nization of memory. 
Like Simondon, we find in Stiegler’s organology a recursive form of interiorization and exteriorization. The exteriorization of memory in technical objects is also when the posteriori becomes a priori. This becoming is a transition from the empirical toward the a-transcen- dental—a-transcendental because it is neither purely transcendental nor empirical. It is only based on this circuit that Stiegler attempts to rearticulate what Simondon calls psychic and collective individuation. However, this interpretation of Husserl also leads to Stiegler’s critique of Simondon. Just as Stiegler finds fault with Husserl for neglecting technical objects in his phenomenology of time-consciousness, he criticizes Simondon for failing to see that the anticipation of operation depends on technical objects.38 By anticipation, Stiegler means that technical objects are the supports of both memory and anticipation, since if anticipation demands an organization of memory, then such an organization depends more and more on artificial memories. For Stiegler, Simondon ignores the role of technical objects in temporaliza- tion because he depends too much on Bergson’s concept of time and therefore opposes living time to geometrization, which makes tertiary retention possible: 
Simondon’s relation to the question of time is too inhabited by its intimate penetration of Bergsonian thought in order for it to be able to escape both the metaphysics of vitalism that denounces the geometrization of time, which is to say, its spatialization, which is precisely that in which every tertiary retention consists, and the Bergsonian ignorance of the crucial difference brought about by Husserl between primary and secondary retention.39 
Stiegler thus argues that although Simondon attempts to rethink individuation in light of the notion of information, he fails to see that information demands a material support, which is precisely the techni- cal object. This critique is probably a little bit too quick and harsh, but it deserves our attention since it also helps to reflect on what we have discussed since chapter 3. We have tried to show above that, for Simon- don, there is a recursive process of exteriorization and interiorization, and for him, technical objects carry what he called “transindividual rela- tions,” which are sources of information—though it is true that Simon- don doesn’t approach the question of temporality according to the same order of magnitude as Stiegler does. Stiegler’s critique of Simondon (as well as Bergson) points to another dimension of organology. As we have seen, in Creative Evolution Bergson opposes intelligence/matter and intuition/life and wants to decompose matter from geometry into movement. Stiegler’s organology, on the other hand, reposes largely if not entirely on geometrization, since it allows the spatialization of time, and such a schematization is indispensable for a creative evolu- tion. Like André Leroi-Gourhan, who has shown that hominization can be understood in terms of two parallel processes—namely, the exteri- orization of memory and the liberation of organs—Stiegler understands exteriorization as the spatialization of time: for example, writing is that which discretizes and spatializes speech into symbols. Tools, languages, rituals, writings, and so on are forms of exteriorization that distinguish the human species from the other animals: “Like tools, human memory is a product of exteriorization, and it is stored within the ethnic group. This is what distinguishes it from animal memory, of which we know little except that it is stored within the species.”40 
We know that Lamarck’s mistake was due to his belief that pheno- types produced during adaptation to the environment would be passed to the genotype—like the famous giraffe stretching its neck in order to eat the leaves on the tree, which in turn explains its long neck. August Weismann’s discovery of the fact that the soma cell is different from gene cell, and that the former cannot be inherited, renders Lamarck’s example of the giraffe laughable. However, the environmental factor exists and is theorized by Conrad Waddington—a dear colleague of Joseph Needham and Gregory Bateson—as epigenetic. Waddington’s notion of the epigenetic landscape is the best way to visualize what he means by epigenetics in evolution: In a landscape consisting of differ- ent slopes or valleys, the movement of a ball becomes dependent on the particular configuration and contingent encounters.41 He also coins the word canalization, which is the measure of the ability of a species to produce the same phenotype regardless of changes in the genotype and environment. Waddington’s epigenetics and canalization form a coupled function between gene and environment, a theory of develop- mental systems biology.42 
A question we raised at the opening of this chapter was: What is the relation between technology and the environment? From the perspec- tive of animal behavior one observes a mode of operation between organism and environment, which is an adaptation in order to produce coupling. But with technology there is also the effect of will. Instead of merely adapting themselves to the environment, and unlike animals that consume the environment (e.g., eating up all the grasses), human beings change the environment in favor of their survival. With tech- nology, human beings are able to pass their memory from generation to generation, without affecting the soma and gene cell. In contrast to adaptation, Stiegler uses the word adoption. Human beings adopt the milieu, but not only by adapting to it. To adopt is to affirm what acci- dently arrived and integrate it into the whole. Adoption is different from adaptation but it is not opposed to it, as if they were two incompatible processes. Rather, there is a dynamic between the two. What constitutes an organology is the strategy of adapting and adopting. Like the Guim- bal turbine described by Simondon, it adapts to the geographical milieu of the river (e.g., through isolation with oil) by adopting it as part of its functioning (cooling mechanism). In other words, the river creates an associated techno-geographical milieu. The Mongolian mobile tent is a way to adopt the ever-changing environment; medication is a way to help the patient to adopt the narrowed milieu caused by virus; the will is a way of overcoming suffering and self-pity (e.g., Django Reinhardt’s becoming a legendary guitar player after having lost the use of two fin- gers of his left hand in a fire). In other words, to adopt is to overcome a deficiency of the milieu by creating an associated milieu. 
It is also on the question of the associated milieu that we can trace an implicit organic form in Stiegler’s thought. Stiegler calls technol- ogy “epiphylogenetic memory.” In his own words, this refers to “the past that I never lived but that is nevertheless my past, without which I would never have had a past of my own.”43 Epiphylogenetic memories constitute the prosthesis of individual and collective memory. They also largely consist of the world into which we are thrown (Geworfenheit) in the sense of Heidegger, meaning the already there—that which I have inherited and which shapes my tendency of identification (instead of a static identity). In the past thirty years Stiegler has from time to time changed his formulation of these terms from tertiary retention to epiphylogenesis, and more recently to exosomatization, as responses to specific questions, yet all of these terms concern the same subject— namely, technics as “the pursuit of life by means other than life.”44 
The accident of Epimetheus is not only a fault but also a default—as it is said in the French idiom, le défaut qu’il faut. It is the fault as default constituting the question of the will and necessity. As someone who became a philosopher by accident, Stiegler’s own biography vividly demonstrates such a passage from contingency to necessity. Stiegler had done many things before he started studying philosophy in prison, having been sentenced to five years of imprisonment for armed robbery. Due to the encouragement of the phenomenologist Gerald Granel (some- one with whom he had become acquainted while he was still owner of a jazz bar in Toulouse), Stiegler registered in the philosophy program at the University of Toulouse II–Le Mirail. After his incarceration he met Jacques Derrida, and through the latter’s recommendation wrote his memoir under the supervision of Jean-François Lyotard. He later wrote his PhD under the direction of Derrida at the École des hautes études des sciences humaines, with a thesis that he later developed into what became the multiple volumes of La technique et le temps. This personal biography is not without significance, since it is only through the question of the accident that he opens a new way to think and to act—that is to say, to become by suspending the end. The prison is an accident to Stiegler, as is the loss of fingers to Reinhardt, but these accidents or traumas are causes—or, more precisely, quasi-causes—of their singularity, since not every prisoner can become a philosopher just as not all musicians can become better musicians after losing their fingers (Django Reinhardt and Paul Wittgenstein being exceptions). The accidents are normally considered to be fateful tragedies, but it is possible that these causes transcend normativity (e.g., a guitarist who has lost some fingers may have to pursue another career in the music industry that requires less manual dexterity) and become the condition of a new associated milieu. By suspending finality one suspends also the relation between the origin and fate that is at the heart of Aristotle’s definition of essence, according to which the origin already contains its end. It is in this attempt to reject a sort of Aristotelian final causal- ity that Stiegler adds to hominization and individuation the concept of quasi-causality: 
We can argue, like Aristotle, that the end is already contained in the origin. In this sense, conducting a radical analysis of what constitutes the origin of a being also tells us something about its end, since, after all, both the origin and the end make up the essence of this being—what makes it identical across time. To use reason this way is to miss the problem of time and becoming (besides, time does not only equate to becoming). Contrary to Aristotle and to all metaphysics—to all “onto-theology,” as we call it after Heidegger—I believe that an accidental process takes place between the origin and the end; we cannot speak only of an essen- tial process for there are occurrences that disturb the metaphysical illu- sion that the end is already there in the origin. Philosophy should learn how to think this accidentality (together with its genealogy).45 
We can even go a step further. Stiegler not only attempts to break this essential relation between the origin and the end in Aristotle, he even demonstrates that it is not legitimate to assert the question of the origin. Indeed, one can talk only about a lack of origin as necessity, a défaut qu’il faut. Or, to put it in other words, technics is accidental, and it is also the origin. If the origin is accidental, then the origin can no longer be seized as essence. In this case, the accidental becomes necessary since it is that which defines human being. Instead of claiming that there is an original essence of human, one should analyze the history of technicity. 
Let’s come back to the question of contingency again, and take up the meaning of Stiegler’s motif of “philosophizing by accident.” At the beginning of the chapter we proposed that Simondon’s concept of contingency is close to a philosophy of nature, while Stiegler’s is closer to artistic practice. For Stiegler, the task of the artist consists in modulating the process of individuation to construct the plan of consis- tency—that is to say, to render the unexpected necessary, to expect the unexpected, as Heraclitus says in our epigraph. In order to engage with the problem of sensibility today—or what Stiegler calls “the catastro- phe of the sensible”—it is necessary that one seizes the accidentality of the epoch, the catastrophic becoming of it as the Ereignis, and renders such accidentality—which is at the first glance improbable—necessary; that is to say, to see it not as a mere matter of fact or a destiny to be followed, but rather a necessary condition for psychic and collective individuation. 
§35. NATURE OR ART 
The object of art is the sensible, and the organization of the sensible through artifices is techn??. Stiegler sees an opening to both the organic and the social through the exteriorization of the sensible. By such an exteriorization the noetic soul is no longer an individual and isolated soul, but rather one that has a history and is in history as a historical being. For Simondon, anterior to the process of individuation there is a preindividual reality. Such a reality is the reservoir of potential, which will not be exhausted, meaning that once conditions are met, it will allow another process of individuation to take place. While Simondon tends to consider this preindividual reality as nature, as we saw earlier, Stiegler refuses to use the word nature but rather understands the pre- individual as an assemblage of technical beings, history, and psychical apparatus. If in Schelling we see that nature possesses the technique of an artist who renders contingency into necessity and inscribes the infinite within the finite, in Stiegler it is not the figure of nature that corresponds to individuation but rather the artist whose task is not only to render contingency necessary as its operation, but also aim for an elevation of the audience as a form of revelation. The artist is he who opens up, through his or her work, a process of transindividuation, meaning a psychical and collective individuation. If the work of art is the consequence of the techn?? of the sensible, and if such a techn?? opens the noetic soul to a circuit of the social, then the artist takes up the role of a facilitator of individuation. In the second volume of De la misière symbolique, Stiegler asked the question, what is an artist? He answers as follows: 
The artist is an exemplary figure of psychic and collective individuation, where an I is to be found only within a we and where a we is consti- tuted simultaneously by the strained and oversaturated potential of the pre-individual ground presupposed by this process, and by the diachron- ies constitutive of the Is through which it is formed. These Is, or psychic individuals, are the inheritors of and strained by this pre-individual poten- tial which connects them, each in their own way, to the we composed by they.46 
The term artist here can be replaced with philosopher, educator, or engineer, as you wish, since the artist is only an exemplary figure. The artist is not someone who produces a work of taste, but rather someone who is capable of and responsible for creating a circuit that allows a transindividuation between the I and the we through the sensible exte- riorized in the form of an artwork (or, in the case of writing a book, a computer program). Recursivity is established through the work of art, constituting a self-knowing toward an end, while this end remains mys- terious, as in mystagogy, a purposiveness without purpose. What is cru- cial to such an individuation is the tension between the I and the we, and it is through the movement necessitated by tensions and the resolution of them that a metastable state is finally achieved, while such a meta- stability can only be a multiplicity because the end remains mysterious and singular to each recipient. A metastable state is stable, but it is not an equilibrium. Rather, it means that such an end is also not a finality of utility but rather a process. Metastability designates a transitional status that may shift into another phase when a new individuation process is triggered. The Simondonian concept of individuation and the individu- alization of technical objects are unified in the thinking of Stiegler, and hence technical objects, here in the case of the artwork, become an indispensable dimension of psychic and collective individuation. 
The artist is he or she who is able to modulate the essential sensible and the accidental sensible, and this modulation is also an act, which renders the accidents (in both senses of the word—namely, inessential and contingent) necessary. An artistic creation is a process through which the unexpected is expected, meaning that the accidents are conceived as necessary in the sense that they are now condition for a possible transformation. It is accidental insofar as it is informational, since it is that which deviates from the regular, the norm, or the already expected, and in doing so it opens a new circuit of individuation in order to arrive at a new normativity. This individuation is presented as a noetic “acting out,” in the sense that a threshold is transgressed, like a quantum leap, elevating one energy level to another energy level; or, in other words, it is a mystagogy. Without recurring to Joseph Beuys, often cited by Stiegler, one can refer to practices from the Dadaists and surrealists, such as Marcel Duchamp, Max Ernst, André Breton, and others. We could say that this motif of artistic creation is rather explicit among the surrealists, for whom the accident is present in all forms of artistic creation: collage, exquisite corpse, automatic writing, and so on. Max Ernst, in “Traité de la peinture surréaliste,” has proposed that 
[a]sked to characterize here the process which first came to surprise us and put us on the path of several others, I am tempted to see the devel- opment [exploitation] of the fortuitous encounter of two distant realities on a non-suitable plane (this paraphrases and generalizes Lautreamont’s famous dictum: beautiful like the fortuitous meeting of a sewing machine and an umbrella on a dissection table) or, to use a shorter term, the culture of effects of a systematic disorientation according to André Breton.47 
It is contingency, in the sense of the improbable, that is the mechanism of the encounter of two distant realities, and this encounter, no mat- ter now improbable, has to become art. Obligation means precisely necessity. The surrealists are only an exemplary figure of such an act of rendering accident necessary. There are, for sure, other practices pivoted on the thinking of the accident. Stiegler adds to this artistic practice a new dimension, which is the theory of individuation. What underlies this process is the will and the creativity to search for truth, to allow the spec- tators to experience the participation in the divine, in the sense that they desire inexistence through technical objects (be it painting, photography, or video), not from the essential, but from the accident. The artist or phi- losopher is he or she who elevates contingency to the plane of necessity, where elevation means seizing it as the element of an associated milieu. In contrast to the natura naturans and natura naturata—which exhibit the general activity of the spirit or invisible nature and in which contin- gency is no longer opposed to necessity, since every contingent event is actualized as necessary—Stiegler’s concept of contingency in the recursivity of psychical and collective individuation designates another dimension of the spirit, one in which the spirit possesses a will that not only actualizes the virtual but also gives new meaning to contingency by seizing it as the opportunity to become, to singularize. 
§36. TERTIARY PROTENTION AND PREEMPTION 
If we return to Simondon’s and Stiegler’s understanding of the primacy of contingency, for the former contingency signifies incoming infor- mation, while for the latter it signifies the moment of appropriation or transformation, which Heidegger calls the Ereignis. This event already presupposes signification, since without signification there is no becom- ing and no appropriation. However, the questions that remain are how this event can happen in the time of technical systematization, and indeed whether it will happen at all, as the philosophers of technology expect. In On the Existence of Digital Objects I have noticed that, in the retentional and protentional circuit that Stiegler has proposed, there is a missing element.48 This missing element—tertiary protention—is crucial to understanding the evolution of technical systems, meaning their becoming organic. I have tried on my part to propose the concept of tertiary protention in order to sketch what I call computational herme- neutics, based on an investigation into the history of Gödel’s recursive function, which was further discussed in chapter 2. I have to admit that, at the time of writing the aforementioned work, I was not yet able to relate it to the subject that we are trying to approach in the current work. The concept of tertiary protention seems to me crucial in order to inves- tigate temporal structure, reconstituted by digital technology, which also presents itself as a new form of determination. Such a determination is always preemptive, in the sense that the machine has already anticipated what the options will be: In this case, freedom means choice. This pre- cisely means that it is a reduction of the contingent to the most probable. The contingent is possible, but it is not the most or highly probable. The contingent is the least probable, or even the improbable. The lack of discussion of tertiary protention in Stiegler’s work seems very paradoxi- cal. On the one hand, the retention and protention of difference is that which gives rise to the concept of différance in Derrida,49 and it is based on such a différance that tertiary retention as differing and deferring is articulated by Stiegler. On the other hand, this silence on tertiary pro- tention seems to suggest that tertiary protention is reducible to tertiary retention, which would be contradictory. We will have to return to Hus- serl’s writings on time-consciousness in order to clarify this question. 
In Husserl’s 1905–1906 course, the relationship between reten- tion and protention was not fully elaborated, and in fact only a few pages were dedicated to protention. In the later Bernau manuscript (1917–1918) and C-manuscript (1929–1934), Husserl provides a more integrated model of the relationship between retention and protention. In these different versions Husserl’s focus and approach noticeably shift.50 Protention is a key element that distinguishes Husserl from his teacher Franz Brentano. Brentano’s model has two main elements: namely, ordinary sensations of the now moment and representations of the past. While the former is real, the latter is unreal, since we can never have direct experience of the past.51 That is to say, in Brentano’s model of consciousness, the question of protention is almost nonexistent. The other main difference between Husserl and Brentano, which is also central to Husserl’s triple intentionality model, is that for Brentano what is perceived is actual, a correspondence between a real and a mental content, while for Husserl the psychic present is not an instantaneous moment but a temporal extension.52 Such an extension—or, more pre- cisely, individuation—consists of multiple successive phases mediated by the intertwining of retention, primary presentation, and protention. Husserl, unlike Brentano, intends to inquire into this time of conscious- ness, the individuation of consciousness, for which time is the principle. While Husserl effectively adopts some elements taken from Brentano (for example, consciousness as continuous flux), he at the same time elaborates his own model of triple intentionality. 
The clearest explanation of the relation between retention and proten- tion is to be found in articles 1 and 2 of the Bernau manuscript. There the relation between the two, which is only loosely articulated in the 1905–1906 course, is further elaborated, and greater prominence is given to the role of protention, where Husserl proposes what he calls a primary or original process (Urprocess), and shows how protention can be understood from within retention and retention from within proten- tion, a complex model through which the dynamic modifications of both are explained. Here it is helpful to refer to Husserl’s own diagram (see Figure 4.1). At this point it may be worth emphasizing that there is a functional difference between protention and retention, and that protention is not purely a product of retention, though it definitely relies on the latter. It has been noted that in §18 of Hua XI55 Husserl proposes an intrinsic dif- ference between retention and protention: directedness (Gerichtet-sein) belongs intrinsically to protention. The kind of directedness referred to here does not belong to the act of the ego but is rather a “passive direct- edness” in which the ego does not actively participate. In this section, entitled “Description of the Possible Types of Empty Representation,” the word Leervorstellungen, or empty representation, refers to the moment of the primal impression of the stream of consciousness, from where consciousness is continuously constituted. Husserl proposes that, even when both protention and retention are empty representa- tions, an immeasurable (gewaltig) difference must still remain between them: First, retention lacks directedness, since it does nothing but push back to the past, whereas protention continuously directs atten- tion (Gewahren).56 Second, Husserl reproaches Brentano for seeing the lawful connection between retention and impression as an original association, Husserl proposing instead that association takes place only in protention.57 Passive directedness seems to be fundamental, since it also directs active directedness, meaning that it is already a selection: 
When the grasping egoic regard is directed toward what is to come, this active directedness follows the passive direction that accrues to percep- tion on the basis of protention. When the grasping regard is directed toward what has just been perceived, in other words, when the grasping regard goes through the retentional continuum, then this directedness goes against the passive direction that belongs to perception itself.58 
We cannot fully separate protention and retention as two operations, and in fact they form a necessary circuit: Retention, though being pas- sive, motivates protention; protention, being active, enriches the reten- tion according to the structure that is coherent to the experience of the individual. However, we must emphasize here that protention cannot be reduced to either primary or secondary retention, otherwise there will be no différance, as Derrida has already shown: rather, it is the demar- cation between passivity and activity, whose dynamic is fundamental to Husserl’s transcendental phenomenology. If we further consider the dynamic between passive and active directedness against the backdrop of a history of tertiary retention, we can see that such an evolution of tertiary retention entails at the same time an evolution of protention, which can neither be contained within a human subject or conscious- ness nor reduced to any reified retention. 
Debt is a primitive form of tertiary protention; it is also retention. With the digital form of tertiary retention we are all in debt: not only in debt to the past that we haven’t lived but which belongs to us, but also in debt to the future, to which we will have to return by acting in accor- dance with its predictions. Preemption is a term that is used to describe the delegation of decision making to algorithms, since in our day it is evident that this is the major issue of the automatized society, in which every possible encounter can be the result of calculation. The use of stochastic algorithms is not only an introduction of contingency to user experience, but is also meant to improve the accuracy of calculation. A consumerism that was based on the manipulation of psycho-power, as Stiegler has described, is stretching to calculation based on data. It seems that marketing based on psycho-power (or even psychoanalysis) is losing its central role in the current form of consumerism (even if it remains fundamental), as marketing strategies move from the manipu- lation of the unconscious to the analysis of big data: that is to say, the manipulation of consciousness (as we are already witnessing, for example, in the Cambridge Analytics affair). It is also the question of protention that confronts us with the question of freedom. 
§37. INORGANIC ORGANICITY OR ECOLOGY 
With the becoming organic of machines and technical systems today, how should we think about these organological attempts to comprehend human-machine relations? The new form of industrialization, sometimes referred to as “Industry 4.0,” has proposed rendering the milieu automatic and smart.59 What is meant by smart here? Smart means the capacity to anticipate, to expect what is most optimal. Since the twentieth century we have witnessed the maturation of the Third Industrial Revolution (information machines) toward what we know as full-scale automation, enabled by artificial intelligence and digital networks. Since the introduc- tion of computers in the second half of the twentieth century, mechani- cal and sequential control of automation has been slowly replaced with digital feedback systems. The power of digitization lies in the capacity to effectively create interobjective relations between different technical ensembles,60 compressing time and space, and hence also systematize these ensembles in such a way that they can be easily submitted to calculation. The proliferation of smart objects and smart environments characterize this revolution today. A challenge is thus imposed on us that doesn’t just demand a new critique of industrialization and the alienation brought about by it, but also reflection on the very possibility and impos- sibility of the tertiary protention implemented in systems of all scales. It is in tertiary protention that the question of contingency is important, since it has to be anticipated in order to optimize. Hence the evolution of tertiary retention is not based on the refusal of contingency but rather depends on it; this is also one of the meanings of being smart. 
Smart objects take part in the milieu by playing the role of organs of motor-perception (converting approximate sense perceptions to signals), organs of analysis (based on calculation and existing data), and organs of synthesis (constitution of the tertiary protention). These devices are connected together through networks, in contrast to the phenomenon of adaption, which is based on the negotiation between living beings and the environment. The technical system anticipates and modulates, and a social phenomenon can now be induced through sig- nal manipulations and across different networks. Profiling through sen- sors and different forms of capture constitutes a new type of individual norm in the sense of Goldstein, but this individual norm is not limited to pathological and clinical analysis, since the concept of the milieu is extended beyond human organs to analog and digital organs, constitut- ing a new milieu characterized by an inorganic organicity. The differ- ence between the inorganic organicity and inorganic mechanicity is that the latter demands a total synchronization without which the system will fail to function, while the former (although it still needs a common temporal axis) allows diversity to appear: for example, personalization, bottom-up movements. Indeed, diversity is necessary for an organic technical system to optimize its performativity. Therefore, one may argue that a superintelligence is one that is in favor of diversity but not delimiting it; however, this notion of diversity is still limited since it is possible only within this system. 
The recursive models that define the individual norm through an organic configuration of relations between perception, retention (data capturing), and protention become the source of pathologies. The clos- est example that we can relate it to is internet and video game addic- tion, and, like addiction to drugs, these addictions demand a milieu that makes them comfortable while rendering them too feeble to adopt and adapt to other milieus. It would be too simple to think that it is possible to escape by changing the way of dealing with tools like what is called the Gelassenheit, since what is at stake is a systemic determination through modification of the milieu, analogic to how humans domes- ticate animals. The technical infrastructures stack on one another like building blocks,61 from censors to data, from data to software, and from software to system, which then operate according to a recursive feed- back loop. The technical system far exceeds the operational capacity of any individual technical object, or the cognitive capacity of any human individual. The recursive circuit, based on the three forms of protention and retention, constitutes a new dynamic of the soul that indicates the evolution of technical objects and the inhumanization in which the soul is rendered fragile by the product of the spirit. 
What we are witnessing today, since the advent of cybernetics, is the development of an inorganic organicity stretching through every smart device and multiple levels of systemic organization. They cease to be the merely organized inorganic but rather are becoming the organizing inorganic, which functions recursively to produce its own structures and patterns. Recursivity is a thinking that enters into all orders of magnitude—for example, synthetic biology. Recursive algorithms are employed to conceptualize the reproduction of DNA-RNA-protein. Or as a researcher in synthetic biology has put it, after having referred to Schelling’s Naturphilosophie: “Synthetic biologists aim to enhance and transgress nature by using nature’s self-organization principles, in short: transgressing nature by harnessing nature! They conceptualize nature as a kind of technology or, more specifically: as a universal engineer.”62 
Nature is recursive; in it one can find reciprocity between parts and the whole. However, recursivity is not only a natural phenomenon; it is also a technical thinking, or what Douglas Hofstadter calls a “strange loop.”63 With recursivity, algorithms are able to domesticate differ- ent forms of contingency in order to render them useful. The relation between technics and contingency must be analyzed materially and historically; these relations reflect dominant scientific epistemologies. Capital in the digital age takes up a recursive form enabled by algo- rithms and digital networks, since it is capable of regeneration/repro- duction. This regeneration is not a linear accumulation, however; it recursively overcomes (whether by integrating or eliminating) the con- tingency on its way toward the infinite: the ultimate goal of accumula- tion and development. This is not merely ideological, since technology is not an ideology and critique of capital is fundamentally a critique of technology. We can find many concrete examples: Google is a gigantic recursive machine that reproduces itself by integrating all the data of its users, updating them and parsing them into useful information for other services. Google is of course only one example, but when our milieu is surrounded by censors and interactive machines, real subsumption adopts a new mechanism in which the user is treated as a recursive algo- rithm and becomes part of another recursive algorithm. Deleuze, taking up the vocabulary of Simondon, calls this process modulation instead of molding.64 Maybe it is still not evident, since the concretization process of technical systems takes time. It always starts with bugs and errors, but technology is driven by failure and limits. It is not encouraged by perfection, because perfection means no more progress. 
What is called a system is supposed to be a self-dependent ideality, realized by an organic (recursive and contingent) form that is math- ematically grounded. Retrospectively we can say that the systems of Schelling and Hegel, as well as those of Bertalanffy, Luhmann, and von Foerster, have longed for this inorganic organicity of society, though without thinking about its realization through digital technologies. In the social system of Luhmann, for example, society is maintained by performativity, which functions like feedback, or an Anstoß, deciding if the next cycle should proceed. This self-reference is conceptualized according to the double contingency of communication between two black boxes, and it is different self-reference subsystems that constitute the complexity of the social system. The development of a social sys- tem can be realized through the implementation of such recursive forms on all levels (languages, individuals, families, institutions, states) and the connections between these levels. In comparison to Schelling’s ide- alist conceptualization of nature as general organism, or visible mind, human beings are in the process of realizing it as a cybernetic system. 
A similar line of thought can be found in Pierre Teilhard de Char- din’s noosphere, which he describes as an “envelope of thinking sub- stance.” The noosphere is an “added planetary layer”65 produced by the process of hominization. This “added planetary layer” is leading to a convergence, whose evidence could be found in the “universalization” of technology. As Teilhard observed, the invention of tools may come from individuals, but the spread of such technology is planetary. Since the beginning, when tools were used as a biological function to liberate the limbs and other organs, the constant evolution of technology has produced a rather different scene: When Homo faber came into being the first rudimentary tool was born as an appendage of the human body. Today the tool has been transformed into a mechanized envelope (coherent within itself and immensely var- ied) appertaining to all mankind. From being somatic it has become “noospheric.”66 
It is through the universalization of the Noospheric technologies that Teilhard sees a convergence, which is the convergence of all brains to a Brain, or the creation of a superorganism. The modern earth—which, following Margulis and Lovelock, we can call an artificial earth or Gaia—is the realization of a superorganism, a super Brain that is the collectivity of the individual brains. This superorganism is in great contrast to the general organism of Schelling precisely because, for Schelling, general organism is the name given to nature, while this superorganism that Teilhard is proposing is no longer about nature, but rather a system realized by technology, which is capable of reflection and anticipation. 
We can raise the following speculative question: If we follow the logic of Teilhard, will it be justified to say that, toward the maturity of the noosphere, technology, and the environment, the organic and the inorganic will together constitute an organismic system in which a perpetual peace in the sense of Kant can be guaranteed, since such a self-organization of nature based on reciprocity and community is what Kant projected as a political ideal? That is to say, with the technologi- cal acceleration toward a singularity and intelligence explosion, will it be possible to realize an artificial earth that is capable of self-planning and self-organization, like a true “organism”? But if Teilhard is able to propose such an eschatology, it is because he didn’t take power and capital into account. The superorganism will remain a theological ideal, but it ignores the struggles present within different levels. It is also the reason that organology after Canguilhem must also be a political proj- ect, which is cosmotechnical. 
§38. THE PRINCIPLE OF GROUND 
How is it possible to think of indeterminacy when the determination of the technical system is now a general tendency? This determination consists of speculating about future actions and events. This is what I suggested constitutes a new condition of philosophizing after Kant’s formulation in the Critique of Judgment. However, the problem is not about the loss of superiority of the organic, but rather the failure to recognize the becoming organic of the inorganic, as Simondon has clearly noticed. Simondon sees it further as a possibility, since it is easier to create and organize associated milieus within an ensemble of information machines, considering that the geographical milieu of the Guimbal engine is not transferable. The struggle is therefore not one that attempts to restore the superiority of the organicity of human being, but rather a struggle that attempts to resituate technology— that is to say, a cybernetic or allagmatic thinking—which is beyond technical concretization. (This is also why it is a general allagmatic instead of cybernetics in the sense of Wiener, and is closer to Bateson than to Wiener.) This “going beyond” is not a detachment but rather a recontextualization of technological concretization, not unlike what the vitalists wanted to do with mechanism by putting “mechanism back into its place within life.”67 By mechanizing the organism, argues Canguilhem in “Machine and Mechanism,” Descartes made teleology disappear, since nature is reduced to mechanism and teleol- ogy is enclosed in technical activities.68 The distinction between the reflective judgment and determinative judgment in Kant’s Critique of Judgment, as well as the distinction between life and mechanism in Bergson, allows Canguilhem to conceive a reality that is far beyond science and technology, like Kant’s natural end69 and Bergson’s élan vital. In the West one can see the shift from myths to mechanism, and later to organicism and cybernetics, as progress in science, while mathematical formalization is that which qualifies what can be called scientific. Wiener claims that a Bergsonian machine is possible pre- cisely because, like the organicists, he rejects the concept of élan vital, and an organism could be studied mathematically. Wiener’s cybernetics suggests a new epistemology based on feedback and information for constructing non-Cartesian machines. By doing so, Wiener also escalates the territory of rationalism. Both Bergson and Canguilhem see that technical rationalization always carries an irre- ducible other: “The rationalization of techniques makes one forget the irrational origin of machines. And it seems that in this area, as in any other, one must know how to cede a place to the irrational, even and especially when one wants to defend rationalism.”70 
I tend to think that the limit of technical rationality is a central theme of part 3 of Simondon’s On the Mode of Existence, and this remains of ultimate importance since it is necessary to understand technology from its genesis, as well as a crucial element of hominization. If we are right (as stated in the introduction) that the term technical reality (réalité technique), which Simondon proposes to us to reflect on the existence of technical objects, is a response to Heidegger’s Dasein, which was rendered into French as “human reality” (réalité humaine) or “being there” (être là), then, like the human reality, the technical reality also points to a “there.” This means that the technical real- ity cannot be studied as an isolated entity that is a mere product of rationality, but rather it has to be approached from its historicity and locality. Therefore we understand that Simondon sees the necessity of accounting for a genesis of technicity of which the physical concreti- zation of technical objects is only one part. Simondon wants to move away from the physical concretization to the genesis of technicity, and this is also the point of departure for my own concept of cosmotech- nics. There are multiple geneses as there are multiple cosmotechnics. If general allagmatic merits the name of a universal cybernetics, it is because it seeks a genesis as a resolution of problematics that are also necessities, since systems saturate in time, meaning that the structure no longer sustains the dynamic of its operation. The operation there- fore has to search for another structure, and this is also a process of transevaluation. In chapter 2 we briefly discussed Bateson’s proposal to end the vicious circle of the positive feedback, which he demon- strates with the example of the alcoholic. This leap is possible when a certain threshold has been reached—for example, a fatal disease or serious accident—and another reality is presented, like the “power” among the members of Alcoholics Anonymous. The power refers to the experience of the divine, the moment when one is out of oneself and the self is reconfigured by an ungraspable external force. This divine takes the name of magic in Simondon and the Unknown and incalculable in Heidegger. In Simondon’s speculative history of technicity, the magic phase is a moment when the figure and ground (in the sense of Gestalt psychology) are separated, while there is no sharp distinction between subject and object. We can understand this indifference between subject and object in terms of a continu- ity between the interior (the human subject) and the exterior (natural phenomenon), in which interiority is reflected in exteriority and vice versa.71 The magical phase bifurcates into technics and religion, and each of them bifurcates further into theoretical and practical parts. In this constant bifurcation in time, Simondon envisages the task of philosophical thinking (in view of the insufficiency of aesthetic think- ing) as one that seeks convergence against constant divergence. 
There is no doubt that technology is indispensable for hominization, but in the development of civilization, technology is only one part of cosmic life. Technology is not equivalent to culture. Indeed, technol- ogy is always contested by culture, and at the same time motivated and constrained by culture, as Bertrand Gille has described in view of the conflict between the human systems (juridical, political, economical, etc.) and the technical system.72 However, when technology detaches itself from this balance of figure/ground and becomes its own ground, as well as the ground of other domains, we will have to resituate it in a new episteme and transform it from within according to different epistemologies. This is also the reason for which we must search for the ground of technology. This was also my motivation in developing the concept of cosmotechnics as an attempt to open up the question of technology: We don’t have only one technology (as figure) and one cosmology (as ground), but rather multiple cosmotechnics containing different dynamics between the moral and the cosmos. Borrowing from Gestalt psychology, Simondon comments on this relation between fig- 
ure and ground: 
Gestalt psychology, while recognizing the function of totalities, attributed force to form; a deeper analysis of the inventive process would no doubt show that what is determinant and plays energetic role are not forms but that which carries the forms, which is to say their ground; the ground, while perpetually marginal with respect to attention, is what harbors the dynamisms; it is that through which the system of forms exists; forms do not participate in forms, but in the ground, which is the system of all forms or rather the common reservoir of the formers’ tendencies, well before they exist separately and constitute themselves an explicit system. The relation of participation that links forms to ground is a relation that bestrides the present and diffuses an influence of the future onto the pres- ent, of the virtual onto the actual. For the ground is the system of virtuali- ties, of potentials, forces that carve out their path, whereas forms are the system of actuality.73 
The ground functions as the support of all the forms. Simondon pro- poses to resituate technicity within a cosmic reality, in which the concretization of technical objects (or what he calls physical concreti- zation) should be guided by this search for convergence, which pres- ents itself as an individuation facilitated by an internal resonance. The allusion to Gestalt psychology is very significant when we associate it with what we have seen in the previous chapter concerning the organic totality in Canguilhem and Goldstein (not to mention that Simondon’s master Maurice Merleau-Ponty was very much influenced by Goldstein and Gestalt psychology). The subversion between ground and form pro- duces what Deleuze calls “transcendental stupidity” (bêtise) and what Schelling simply calls “evil”: 
[I]ndividuation as such, as it operates beneath all forms, is inseparable from a pure ground that it brings to the surface and trails with it. It is dif- ficult to describe this ground, or the terror and attraction it excites. Turn- ing over the ground is the most dangerous occupation, but also the most tempting in the stupefied moments of an obtuse will. For this ground, along with the individual, rises to the surface yet assumes neither form 
nor figure. . . . It is the indeterminate, but the indeterminate in so far as it continues to embrace determination, as the ground does the shoe.74 
In his seminar on transcendental stupidity, Derrida has noticed that, in order to understand what Deleuze has written about the individua- tion of the be?tise, it is necessary to engage with Schelling’s concept of the ground both as the original ground (Urgrund) and the groundless ground (Ungrund). Deleuze’s footnote referring to Schelling’s Frei- heitsschrift is one reason for making this claim.75 Derrida was probably right to state the importance of Schelling’s concept of the ground here. However, it is crucial to note that Deleuze is also referring to Simon- don’s concept of individuation, as well as the ground-figure metaphor that Simondon had borrowed from Gestalt psychology. It has been well noted that Difference and Repetition is a work highly influenced by Simondon, and it is even more evident when we consider that Schelling did not use the term figure when he talked about the ground. Neither did Heidegger use the term in his seminar on Schelling’s Treaties on Human Freedom. Schelling wants to show that freedom is the condi- tion of both good and evil, and that evil is therefore inevitable even in God. This may lead to the suspicion that God is evil, since according to the conventional understanding God and evil, freedom and system, are incompatible. Schelling’s argument shows that evil cannot be other than God, since if evil is not absolutely nothing, then it is being, and if God is the Being of all being, then evil is within God. In the becoming there is a separation between ground and existence, like two forces: gravity and light, universal will and self-will. Every being in its becoming is separating from God, while such separation is a contradiction since it is and can only become in God. The individuation (Vereinzelung) of evil comes out of the striving of the self-will to take the place of the universal will: 
Being spiritual, self-will can strive to be that which it is merely by remaining in the divine ground also as creature. As separated selfhood it can will to be the ground of the whole. Self-will can elevate itself above everything and only will to determine the unity of the principles in terms of itself. This ability is the faculty of evil.76 
If we claim that the technical concretization is obscuring the cosmic real- ity, it is because technical and digital objects are becoming the ground of their own movements instead of the figure. When technology itself becomes the ground, then the cosmic reality is obscured, and the techno- logical acceleration becomes the value of all values. This is why I have attempted to show that we must not conceive modern technology as the totality or the ground, but rather that it is necessary and urgent to con- ceive different cosmotechnics in which technology is reconnected with the cosmos and the moral. This will demand a rethinking of the transfor- mation of the episteme, which in turns conditions a different political, social, and aesthetic experience. We can say here that it is based on this cosmotechnical reconfiguration that Simondon proposes what he calls co-naturality, which means coexistence between nature and technology; or, more precisely in this case, technical infrastructures complement the magic phase (i.e., the unity between ground and figure), becoming the key points such as peaks, giant trees, and streams in this phase: 
Look at this TV antenna of television as it is. . . . [I]t is rigid but it is oriented; we see that it looks into the distance, and that it can receive [signals] from an emitter far away. For me, it appears to be more than a symbol; it seems to represent a gesture of sorts, an almost magical power of intentionality, a contemporary form of magic. In this encounter between the highest place and the nodal point, which is the point of trans- mission of hyperfrequencies, there is a sort of “co-naturality” between the human network and the natural geography of the region. It has a poetic dimension, as well as a dimension having to do with signification and the encounter between significations.77 
This can be called a cosmopoiesis in which coexistence is favored instead of the domination of humankind and technology over nature. 
This organicity in Simondon moves away from a functional neces- sity (for example, in the Guimbal turbine) to both an aesthetic and philosophical necessity, which is fundamentally intuitive. Intuition for Simondon is “the relation of figure and ground in itself.”78 Instead of reason, Simondon moves back to intuition. Like Bergson, he sees intuition as the primordial faculty that is not yet exhausted by the intel- ligence. Simondon wants to generalize Bergson’s concept of intuition as a form of knowing that resolves the problematic produced by the inversion of figure and ground,79 but he also wants to overcome Berg- son’s opposition between a “pure operation,” which is disinterested, and a “utilitarian operation”80 (as we have seen in the last chapter). For Simondon operation and structure cannot be isolated; a veritable analysis of individual being can rather be understood as an encounter between structure and operation, which he calls allagmatic. There are three types of intuition for Simondon: the magical, the aesthetic, and the philosophical. Magical intuition maintains the coherence between ground and figure, but after its bifurcation into technics and religion it was aesthetic intuition that tried to produce a convergence between the two. However, it can only do so by indicating a relational necessity without going further, and finally it is philosophical intuition that must resume this task. This philosophical intuition has to be distinguished from idea and concept, Kant’s intuition, as well as intellectual intuition: 
Intuition is neither sensible nor intellectual; it is the analogy between the becoming [devenir] of known being and the becoming of the subject, the coincidence of two becomings: intuition is not merely the grasping of figural realities, like the concept, nor a reference to the totality of the ground of the real in its unity, like the idea; it aims at the real insofar as it forms systems in which a genesis occurs; it is the knowledge proper to genetic processes.81 
We can infer here that philosophical intuition is neither purely contin- gent nor primitive, but rather an aesthetic and philosophical education, or education of sensibility. This is not epistemology in the strict sense that we use it today in science, but what I prefer to call episteme, the sensible condition of the production of knowledge. Since Simondon claims that intuition is neither sensible nor intellectual, it is also hard to demonstrate that it has nothing to do with the two pure intuitions that characterize the Critique of Pure Reason. What could be justified is that philosophical intuition here is less about the representation of a reality but rather that it first operates in an analogical way.82 Analogical here means that there is a relation between the two becomings of the known being and the subject, but it is not yet a synthesis. Philosophical intuition searches for a coupling between religion and technics, concept and idea, and in this sense we can say that it is in philosophical intuition that we find again an allagmatic that is beyond Simondon’s discussion regarding cybernetics.83 
Allagmatic, which is at the heart of philosophical intuition, seeks a genesis. But what exactly is this ground that Simondon is talking about? What kind of reality does it have? We actually couldn’t find an explicit meaning in Simondon’s writing. We associate the ground with a cos- mic reality, but this cosmic reality, as the “becoming” of the “known being,” carries in itself something unknowable. It is the Unknown and the most contingent. Kant grants to intellectual intuition the capacity to know the noumenon—including freedom, the divine, and the immor- tal soul—in order to compensate for the limits of sensible intuition. Simondon’s philosophical intuition is that which produces a coupling between sensible and intellectual intuition (if we assume that it exists and is accessible to human beings, as it is argued by philosophers after Kant: for example, Fichte, Schelling, and Mou Zongsan, among others), just as it does regarding concepts and ideas. The paradox is that this 
Unknown can never be known objectively, since when it is known it is no longer unknown and so no longer remains the absent other of the system, but rather becomes part of techno-science. We must emphasize that the Unknown is an epistemological category, not the mysterious ineffable thus named out of mere “laziness” or “irrationality.” If we put the divine, the Unknown, absolute contingency, incalculability, and even Dao into this category, it is not simply a gesture to affirm the irre- ducibility of life to physico-chemical activities, or of spirituality to mat- ter, but also to suggest that it is necessary to rationalize the Unknown, which remains necessary for any system of knowledge in order to reframe the question of technology, so that technology will have a final- ity that is not a finality of use but rather a finality beyond usage. At this point, Simondon is close to Heidegger, and indeed invokes Heidegger: “[T]hought that recognizes the nature of technical reality is that which, going beyond separate objects—utensils—according to Heidegger’s expression, discovers the essence and reach of technical organisation, beyond separate objects and specialized occupations.”84 
We are not, of course, equating Simondon with Heidegger here. How- ever, this is also the precise moment when Simondon wants to construct a thought that recognizes technical reality, as when Heidegger wants to deviate from the Gestell of modern technology. A dialogue between Simondon and Heidegger is therefore revealing for a future philosophy of technology. Now, how is an epistemology of the Unknown possible at all? At first glance the question itself may appear contradictory. After all, the Unknown is unknown precisely because there is no way of knowing it, and if there is a way of knowing it, how can it still be Unknown? However, we remind ourselves here that it is the attempt to know the Unknown without really knowing it that constitutes the spirit of organic thinking from Kant to cybernetics. The difference is that in cybernetics the Unknown is ignored on the level of functioning, mean- ing that the Unknown is absent of function, while we want to take the Unknown as functional, which not only imposes constrains and limits on our comportment in the world, but also allows us to develop a non- exhaustive relation with the world and with technology. When I say that it is necessary to “rationalize” the Unknown, I don’t mean to suggest turning the Unknown into something ready-to-hand that we can grasp, like a glass of water in front of us, but rather constructing a plane of consistency that allows us to access the Unknown through the symbolic world that we have inherited and within which we live. The question of spirituality is always a question of symbols. It is frustrating when cog- nitivists want to demonstrate that it is possible to reach the spiritual by modifying neural activity, without realizing that it is the symbols that construct the cathedral rather than the mind alone. Noetic recursivity, through which the subject-object distinction is obliterated, is fundamen- tally a technical recursivity, which complexifies through the course of history and the evolution of symbolic systems. 
The seemingly mysterious Unknown or the Unknowable may be associated with what Heidegger calls Being.85 It is a response to the exhaustion raised by the constant conquest for the why of the sci- ences. In his seminar The Principle of Reason, Heidegger proposes two interpretations of Leibniz’s nihil est sine ratione (nothing is without reason), which Heidegger translates as Nihits ist ohne Grund. The first is logical: For any proposition—for example, “stupidity is evil”—to be true, there must be a necessary connection between the subject and the predicate, that is, a foundation of judgment. According to this reading we come to the first interpretation of the ground as ratio or account (Rechenschaft).86 The account is given to the human and to the human as a judging subject. In this judgment, the object has become a Gegen- stand, which positions itself in front of the human and his or her point of view. The principle of reason now carries the following meaning, quot- ing Heidegger: “[E]very thing counts as existing when and only when it has been securely established as a calculable object for cognition.”87 This interpretation of the principle comes with modern technology because the latter is based on the calculability (Berechenbarkeit) of objects, and therefore the domination of the principle of reason deter- mines the essence of the era of modern technology.88 
If the first interpretation totalizes the ground as the account for the representation of beings, it exhausts being by rendering it calculable. And even if Heidegger did not talk about the figure, we can neverthe- less see that calculability is another name for Gestell. It is the moment when the figure becomes its own ground, whereas this self-constitution is primarily a mode of exclusion.89 Heidegger also proposes a second reading of Leibniz’s principle. If in the first interpretation the empha- sis is on nihil and sine; in the second interpretation it is the question of est and ratione that concerns us, that is, the question of Being. In this second interpretation the ground is—that is, the ground belongs to Being. The principle of reason thus evoked is no longer what reigns in the representation of beings, but a term of Being. What is this ground as Being? The ground is what answers the question why (warum; in German, weil), but it does not mean because, but whereas (dieweilen): “so long as . . . while.” Heidegger concludes that “whiling (weilen), tarrying (währen), perpetuating (immerwähren) is the old sense of the word ‘Being.’”90 This conclusion echoes his 1949 lecture The Question Concerning Technology, where the word essence is reinterpreted as what remains permanently (das Fortwährende).91 The central question of the ground for Heidegger is the preservation of Being. 
This ground is groundless, the Urgrund is the Ungrund or Abgrund, since it cannot be reached and seized as present-at-hand. One would be overwhelmed to realize that what one believed to be the ground turns out to be nothing but an abyss. Can humans believe in what cannot be verified while still being guided by it? Isn’t this a return to religion and to poetry, in the case of Heidegger, since the poet should anticipate the arrival of the Unknown (in a way, to expect the unexpected)? And isn’t this a conservative or a romantic rejection of what modern science suc- ceeded in obliterating in order to liberate itself from the oppressions of illusory transcendence? Doesn’t the question of the Unknown lead us astray from the ground that modern science has promised us, in which what is free is rational and what is rational is free? 
However, what I mean by the rationalization of the Unknown or the unknowable carries a different meaning. It doesn’t mean reinstalling a Godlike transcendence, but rather preserving the instrumentality of technology and unifying it with the spirit, and at the same time going beyond technological instrumentality so that new forms of life and happiness can be perceived through new symbolic systems that allow the Unknown to be welcomed not only in the form of a sect, a religion, or New Age practices, but also manifest in scientific research and technological development, which no longer carry the name “modern science and technology.” Modern science and technology sees only the standing-reserve of the universe, and the possibility of exploring the secrets of the universe according to a materialist doctrine. This ground- less ground, because of its virtuality, will be revealed, in one form or another, after a “hitting bottom” of the alcoholic moderns: Only a God can save us. 
The Inhuman That Remains 
Lightning steers the universe. 
—Heraclitus, Fragment 64 
Writing in the immediate aftermath of the Fukushima catastrophe of 2011, the French philosopher Jean-Luc Nancy already saw the end of tragedy, the end of the longing for a postindustrial apocalypse, as well as the end of any wishful hope for a unified faith and wisdom: “We live no longer either in tragic meaning nor in what, with Christianity, was supposed to transport and elevate tragedy to divine salvation. Nor can we take refuge in any sort of Confucian, Taoist, or Buddhist wisdom: Equivalence does not allow it, despite all our good intentions.”1 
The end arrives as an event and discloses the monstrosity of meta- physics. It is not that the earth is under attack. Whether by increasing its strength or mitigating its damages, it will be able to absorb these catas- trophes—this is referred to as the resilience of the ecosystem. Rather, the gigantic force of technology is autosystematizing at all orders of magni- tude—artificial intelligence, the internet of things, nanotechnology, bio- technology, drones, driver-free cars, smart cities, SpaceX—in the hope that one day, even if it exhausts the productivity of nature, it will enable itself to be self-fueled. The planetarization (in the sense of Heidegger) means the invasion of technology into all beings, rendering them stand- ing reserves, like the general equivalence (it is also in this sense that we understand what Nancy calls equivalence). This technological globaliza- tion, first of all, had to conquest the other systems of knowledge. The non-European cultures have to adopt the modern episteme, including its technological apparatus, so that such a gigantic system will be possible. As Heidegger has famously said in “The End of Philosophy and the Task of Thinking” (1964): “[T]he end of philosophy proves to be the triumph of the manipulable arrangement of a scientific-technological world and of the social order proper to this world. The end of philosophy means: the beginning of the world-civilization based upon Western European thinking.”2 This “manipulable arrangement” (steuerbare Einrichtung) of the scientific-technological world means cybernetics. This age of ours has to be fundamentally distinguished from the tragic age and the apocalyptic age, which we shall identify with Greco-German and Judeo- Christian thought, respectively. The catastrophic age, instead, is global, artificial—maybe we can also say superficial. 
As we know, now it is the time that algorithmic catastrophes will come one by one: climate change, flash crashes of the global financial markets, mass unemployment, and imminent cyber- and robot warfare. We are no longer dealing with a specific geographical culture; rather, we are now confronting a global culture brought about by technolo- gies, which I described as the accomplishment of a global axis of time. However, it has not and probably will not realize the “hidden plan of nature” as perpetual peace. Reason arrives at a confused moment of its existence, and a nihilism emerges out of this context as the crisis of the Übermensch. Hence it is also a time when reason wants to seek its saving power in contingency, something that is not yet determined and that always remains indeterminable. At the same time, it is recognized that catastrophes can also be transitions toward a better future, since the perfection of technology depends largely on malfunctions and catastrophes: Without the shipwrecks there wouldn’t be better naviga- tion technology; in other words, without failure and obstacles, there wouldn’t be any invention, and probably no science. The planetary convergence that we are witnessing today, and the governmentality that relies on recursive modeling, is no longer a metaphor, but is in the process of completing a superorganism in the sense of Teilhard de Chardin and Lovelock. Is this gigantic system going to impose more violent determinations on us, or will it offer a promise of freedom, as the transhumanists claim?3 We would like to come back to the question of determination after having discussed the concept of the margin of indetermination in Simondon’s thought, since in Simondon the margin of indetermination is a principle of machine design as well as of un- predetermined relations between human and machine. How can we approach the question of indetermination in the time of the organizing inorganic of technical systems? 
§39. POSTMODERNITY AND RECURSIVITY 
We will have to return to Kant’s third antinomy, already discussed at the beginning of chapter 1, concerning the tension between the laws of nature and freedom. Friedrich Schiller responds to Kant’s antinomy in his On the Aesthetic Education of Man,4 in which Schiller addresses not only the conflict between the sensual and the rational but also politi- cal faculties in view of the French Revolution5—namely, the terror of reason over individual freedom. These oppositions take another form in our time—that is to say, governance through algorithms and big data in opposition to individual freedom and desire. Schiller and Kant, of course, did not dream about the reign of machines in our time. Hegel probably has a sense of this, regarding the realization of the State; however, Hegel may not have imagined that the exteriorization of the spirit, in the name of reason, would be actualized in the algorithmic mode of governmentality today. For sure, one can argue about whether this form of reasoning in machines merits the name of reason, but the fact is that recursive algorithms today have a flexibility and a capacity for self-organization and self-improvement that may have overwhelmed thinkers who still naively identified technology with the machines of the seventeenth and eighteenth centuries. 
In view of the above-mentioned antinomy, Schiller’s response is to construct a third. Schiller reformulates the opposition as an opposition between the material drive (Stufftribe), which is sensual, and the formal drive (Formtrieb), which is rational, and suggests a synthesis that he calls play drive (Spieltrieb). The drive of play is a synthesis that, rather than attempting to eliminate any of these two opposing drives, instead renders them contingent, and later neces- sary, by preserving both. We know that it is this double movement of preservation and elevation that inspires Hegel’s Aufhebung. The play drive, as I would like to argue, is organological, since it attempts to preserve and integrate the two opposing forces into a synthesized force. Schiller’s aesthetic education is still significant today, since his questioning and argumentation could be restaged in our time by con- sidering the displacement of the concept of nature by the concept of the technical system. Schiller proposed to overcome this opposition through aesthetic education, which is also the formation or education of sensibility (Ausbildung des Empfindungsvermögens). Aesthetic education allows the reconciliation between necessity and contin- gency, the inscription of the infinite in the finite, which has the aim of fully realizing humanity. As Schiller says in Letter 25, “[S]o it is through the unification of both natures, the possibility of inscribing the infinite in the finite, and therefore the possibility of the sublime humanity is demonstrated.”6 
Whether Schiller is a philosopher is not our question; however, his attempt deserves to be reformulated and recontextualized. First, it could be read in parallel with Schelling’s system of human freedom, which cannot be completely separated from evil. Second, the realization of humanity could be read in parallel with Lyotard’s concept of the inhu- man. Retrospectively, we would like to reconstruct Lyotard’s critique of the concept of system as an attempt to address our concern here, not only because Lyotard has been largely underestimated but also because he remains a prophet of our time. 
It seems to me that Lyotard’s writing between his 1979 The Post- modern Condition7 and his 1988 The Inhuman: Reflections on Time— passing by his Le Different in 1983 and notably his exhibition Les Immatériaux in 1985—could be read in terms of the philosopher’s constant reflections on the promise and problematic of the postmodern characterized by the dominance of system. The postmodern embodies an irreducible duality. Like the sublime, it has its positive side—namely, the liberation from the modern concept of order and hierarchy—yet on the other hand it is also characterized by a certain autonomous opera- tion or self-legitimation that constitutes a new paradigm of knowledge production, which Lyotard refers to as system. One may have to admit that the actual status of the production of knowledge and technological development has not yet surpassed what Lyotard envisaged in the first pages of The Postmodern Condition. For sure, it will also be problem- atic if we believe that we remain in the historical moment that was called the postmodern forty years ago. This demands further reflection on Lyotard’s fundamental critique of technology, in order to understand what is at stake today. 
Regarding Lyotard’s critique of system, we must first of all clarify an ambiguity in Lyotard’s interpretation of Kant’s reflective judgment. Lyotard is a very careful reader of Kant, especially the Critique of Judgment. It is no exaggeration to say that reflective judgment is the key to Lyotard’s discourse on the postmodern and on art in general. As Lyotard claims, reflective judgment is that which allows Kant to unify the field of philosophy (natural and moral) by “making manifest, in the name of aesthetic, the reflective manner of thinking that is at work in the critical text as a whole.”8 Lyotard calls this reflective judgment, the pas- sage from the particular to the universal, heuristics—a term now often used technically in artificial intelligence, designating a technique for solving a problem when classical methods are too slow. This heuristic process is controlled by the state indicated by sensation. Lyotard takes a rather machinic metaphor here: Heuristic is “the transcendental activity of thought” and sensation is that which “informs thought of its state.” 
The use of terms such as heuristic, state, and information resembles a modern recursive machine. In chapter 1 we attempted to character- ize Kantian reflective judgment as a preliminary model of recursivity, which is the foundation of both aesthetic and teleological judgment. The further development of recursivity led us away from the historical debate between mechanism and vitalism, which we can formulate in the following way: Mechanism implies a repetition of the same, which reduces life to physico-chemical equations. Vitalism implies a repeti- tion of difference, while this repetition is the expression of a vital force that, whether it be it an entelechy or élan vital, remains mysterious. These two are, as Scott Gilbert mocks, the bad companions of organi- cism.10 We have also seen why the notion of recursivity in modern computational machines is what gives rise to the fantasy of machine intelligence or machine consciousness, since recursion functions like a soul, which comes back to itself in order to know itself, while in every moment of reaching out it encounters contingencies. 
In the 1950s, Simondon already observed the emergence of a differ- ent cognitive scheme from the Cartesian one. The Cartesian cognitive scheme is mechanical, presupposing linear causal relations—“the ‘long chains of reasons’ carry out a ‘transport of evidence’ from the premises to the conclusion, just like a chain carries out a transfer of forces from the anchoring point to the last link”11—while the concept of feedback in cybernetics introduced a new temporal structure, one that was no longer based on a linear form but was rather more like that of a spiral. In this schema the path toward the telos is no longer static but rather a constant self-regulatory process, which Simondon himself described as “an active adaptation to a spontaneous finality.”12 
When we said that there is an ambiguity, this is because Kant pres- ents reflective judgment as a heuristic that leads to a unification of the field of philosophy, similar to what Wiener’s “feedback” wants to achieve for all scientific disciplines. In other words, it is the manner through which a systems theory is possible, as we demonstrated earlier. At the same time, reflective judgment seems to be a mechanism of anti- system for Lyotard. This is where the ambiguity lies: It is what makes the system possible, while it is at the same time antisystemic. How can this be possible at all? 
We won’t be able to answer this question immediately, but it is nec- essary to point out the paradox here. When we say that reflective judg- ment is antisystemic, it is for two reasons. First of all, reflective logic is superior to categorical logic. As Kant already said in the appendix to the Critique of Pure Reason titled “Amphiboly of Concept of Reflec- tion,” reflection (deliberation, reflexio) doesn’t mean deriving concepts from objects directly but is rather the state of mind in which we dis- cover the “subjective conditions under which [alone] we can arrive at concepts.”13 Reflection is not limited by the categories or pure concepts. It is rather that which supplements the transcendental categories. Kant in the above-mentioned appendix shows that given two drops of water with the same properties, one will not be able to discern them from each other as discrete entities just according to the function of the categories, since the two drops of water are logically the same. It is only with tran- scendental reflection that the difference is shown, under four headings: identity/difference, agreement/opposition, inner/outer, determinable/ determination. For Lyotard, therefore, reflection pushes the determina- tion of the categories aside without completely negating them: 
If reflection thus supplements the category, it must have at its disposal a principle of subjective discrimination that belongs to no faculty, but that enables to restore the legitimate limits of the faculty by exploring the con- fines they dispute. . . . Thus refection retains only the notion of an empty legality of the theoretical use of pure concepts (“type”: KRV, 70–74; 79–84) by prohibiting the content of what reflection brings together from being determined.14 
It may seem that Lyotard still sees the system as a mechanical one, like that which is composed of mere determining phrases. As we know, however, that is not the case. Lyotard was aware of cybernet- ics, especially second-order cybernetics. Throughout his later writings he was very critical of the concept of system, as well as the systems theory fashioned in sociology by Luhmann. Luhmann’s systems theory presents a different cognitive scheme from the mechanical system, and it is exactly this “advancement” that is the target of Lyotard’s critique: 
In the work of contemporary German theorists, systemtheorie is techno- cratic, even cynical, not to mention despairing: the harmony between the needs and hopes of individuals or groups and the functions guaranteed by the system is now only a secondary component of its functioning. The true goal of the system, the reason it programs itself like a computer, is the optimization of the global relationship between input and output—in other words, performativity.15 
The system is a self-organizing totality recursively optimizing the global and local relations between the parts and the whole. Luhmann himself also uses the recursive model to describe the postmodern: As he says toward the end of his The Society of Society—the title is already recursive—“[I]f one conceives of postmodern descriptions as operating within realms of self-produced indeterminacy, then one immediately sees parallels to other trends in science that deal with mathematics, cybernetics, systems theory, or with the characteristics of self-referen- tially and recursively operating machines.”16 Complexity is produced by “repetitive operations that follow on from a self-generated initial state and continue it with every operation as a starting point for further operations.”17 How, then, did Lyotard see in reflective judgment an antisystemic potential? The most straightforward answer is that reflec- tion demonstrates a critique against mechanical systems. However, this would remain an outdated critique that Lyotard himself wants to reproach. A more subtle answer, according to Lyotard, is the sublime. The sublime, as we will discuss later in this chapter, is the grounding sentiment of the postmodern. The sublime is like the incomputable of the Kantian machine: When the recursive algorithm is no longer able to arrive at a halting state, it instead triggers a violent reaction. In other words, it is a failure of the understanding and the imagination alone to produce a representation of the sublime object. Such a failure requires the intervention of reason to stop the process by imposing violence on the imagination. We may refer to the example, given by Kant himself, of encountering an Egyptian pyramid: When we are too close to it our gaze always rests on successive apprehensions (Auffassung) without being able to comprehend (Zusammenfassen) the pyramid as a unity.18 Lyotard identifies the sublime with the avant-garde when he says that “for the last century, the arts have not had the beautiful as their main concern, but something which has to do with the sublime.”19 The sub- lime is, for Lyotard, that which is not representable—the unrepresent- able, or, in machine language, the incomputable. But again, it is not the ineffable, since there is an interest in the sublime in Kant, as Lyotard discovered. Kant says that the sublime “indicates in general nothing purposive in nature itself, but merely in that possible use [Gebrauch] of our intuitions of it by which there is produced in us a feeling of a purposiveness quite independent of nature.”20 This feeling of a pur- posiveness Kant calls respect (Achtung). The term Gebrauch is often translated as use or usage, but Lyotard says that it also means abuse and subreption.21 We will come back to the question of sensibility and the sublime later. At this point we can say that the interest in the sublime is an appropriation of the conflict between faculties—the appropriation of the failure, the impossible. 
We will now return to the question of the system. Lyotard sees systems theory coming from cybernetics as a powerful thinking of governance and social regulation, but he refuses to see it as a philo- sophical system: “[S]ystems theory is not a philosophical system but a description of reality, a so-called reality [die sogenannte Wirklichkeit] that has become entirely describable in terms of general physics, which stretches from astrophysics to particle physics . . . and of course also in economic terms.”22 However, it remains doubtful how this is not a philosophical system, as I want to show in this work, when later we will see that Lyotard himself, like Heidegger, admits the close relation- ship between cybernetics and metaphysics. I would like to address this commentary from Lyotard and to extend it to our discussion on the human-machine condition concerning both the legitimation of knowl- edge and the emergence of technical systems. Lyotard sees clearly a shift from de jure to de facto: namely, that the normativity of laws is replaced by the performativity of procedures. Facts, instead of laws, define norms: 
This led Luhmann to hypothesize that in postindustrial societies the nor- mativity of laws is replaced by the performativity of procedures. “Context control,” in other words, performance improvement won at the expense of the partner or partners constituting that context (be they “nature” or men), can pass for a kind of legitimation. De facto legitimation.23 
This critique of the replacement of de jure by de facto consists in the shift of the production of knowledge in the sense that knowledge—or, in a rather narrow sense, truth—is no longer produced by authorities but rather by the induction of facts. In an article published in 2008 titled “The End of Theory: The Data Deluge Makes the Scientific Method Obsolete,”24 Chris Anderson, editor of the magazine Wired, has already proposed that in the age of big data there is no need of theory since algorithms and big data will be able to generate theoretical models, thereby announcing the end of theory. More than ten years later, with the proliferation of deep learning, this tends to be more and more the case concerning the use of big data in both the natural and social sci- ences. This raises precisely the question of knowledge today, since we all know that if knowledge were reducible to calculability and to fact, this would only be a very limited conception. As Lyotard already says: “Knowledge [savoir] in general cannot be reduced to science, nor even to learning [connaissance]. Learning is the set of statements which, to the exclusion of all other statements, denote or describe objects and may be declared true or false. 
The rejection of the notion of knowledge as reducible to science and calculation is an attempt to show that there is knowledge such as savoir faire, savoir vivre, and savoir-écouter that are beyond the realm of scientific knowledge. Stiegler seems to have gone a step further in this respect by showing that savoir faire is necessary for savoir vivre, and that therefore the deprivation of savoir faire is a form of proletari- anization, which problematizes existence by undermining the means of subsistence. The shift in the sense of knowledge in the era of digital automation underlines a delegation of knowledge production and deci- sion making to machines. Lyotard says that “the question of knowledge is now more than ever a question of government.”26 The organic totality of the system that is based on recursivity is realized through different technological schemes (such as the smart city, the internet of things, etc.), which characterize a planetary computation. In chapter 4 we named the new faculty of the machine to anticipate tertiary protention. The preemption of the tertiary protention is possible only because of the computational hermeneutics, which is essentially recursive: It con- stantly evaluates the past in order to anticipate the future, which in turn determines the present. Human beings are reintegrated into the tempo- rality of machines, not only as individuals but also as collectives and communities. This is precisely what is called algorithmic governmen- tality.27 It seems to me that in order to intervene into this new temporal structure today, it is necessary (while we have to do it elsewhere) to redefine savoir faire—or, more precisely, savoir technique—in today’s system of knowledge production. 
Besides theoretical discourse on the question of recursivity and social systems in the work of Niklas Luhmann and Heinz von Foerster, who also gives another name to recursion—“non-trivial machines” (ironi- cally, Lyotard also calls cybernetic information theory trivial)28—we have attempted to show that such discourses are gradually materialized through the implementation and distribution of smart objects, neuro- networks in urbanism, and the use of smart devices in order to access infrastructures. In this process of the totalization of the technological system, the immediate effect is the process of desymbolization through the establishment of inter-objective relations and resymbolization within the technical system, as Jacques Ellul pointed out in his Le système technicien.29 When Ellul says that “on the one hand, man’s inherent power of symbolizing is excluded; on the other hand, all con- sumption is symbolic,”30 he means that symbols that connect humans to nature and allow them to master nature in a nonviolent way slowly give way to technology, and finally lead to a resymbolization of the technical system in which symbols are no longer linked to nature but to commodities. 
The social system is not separable from the technical system. Indeed the technical system is the support of the social system, not only in terms of communication but also in terms of organization. The social system is not reducible to the technical system, though this reduction is taking place rapidly at the moment. Written two years after Ellul’s Technological System, Lyotard’s The Postmodern Condition also hints at the realization of totalized technical systems when he says that “the growth of power, and its self-legitimation, are now taking the route of data storage and accessibility, and the operativity of information.”31 The realization of this system is for Lyotard the continuation of the concept of development, which is a metaphysics without finality: “[W]e are in an Umwelt that is the realization of metaphysics as a gen- eral physics under the name of cybernetics.”32 Lyotard here takes up both Heidegger’s verdict on the status of cybernetics and Luhmann’s systems theory, which is the avatar of a “general physics.”33 Cybernet- ics realizes metaphysics, makes metaphysics reality, and establishes its right over thinking.34 What is left for metaphysics is to incorporate the outside as its inside, to exclude itself as the dominant thinking. For systems theorists the task is not to get out of the system but rather how to optimize the system by modulating its performativity and increasing its capacity for resilience based on feedback.35 Therefore, when modern leftists lament that there is no longer any outside, they become the true metaphysicians. The opposition between engineering and the humani- ties can be caricatured as an opposition between positivism and herme- neutics, or between efficiency and reflexivity, but such a distinction, as Lyotard already noted while commenting on the Frankfurt School, is not acceptable,36 for the solution proposed by the latter “is no longer relevant” in postmodern societies, since this opposition ceases to func- tion as a critical apparatus.37 
§40. TECHNOSPHERE OR CHRISTOGENESIS 
Instead of seeing cybernetics as a nonphilosophical system, in this work I have been attempting to show that cybernetics is fundamen- tally a metaphysical project. The passage from nature to logic, from the organized inorganic to the organizing inorganic, is staged as a conceptual conflict between form and matter. However, this has not been presented as the triumph of one over the other, since they are not separable. It is only in our archaic epistemology that form and matter are separated, and this conflict is interpreted as a philosophical melo- drama. With the organizing inorganic, where are the humans heading with their technologies? For decades we have been talking about an intelligence explosion, superintelligence, the technological singularity, a foreseeable technological utopia in which genetic engineering, human enhancement, and immortality are promised. Speculation concerning what comes after the human has been widely discussed. The emergence of x-humanisms, whether this x be post- or trans-, attempts to point to a definite future in which humans can be either saved by a posthuman ethics or by advanced technologies. On the one hand, the posthuman gives us an impression of liberation, of freeing ourselves from the older category of the human. On the other hand, this “being liberated” is noth- ing other than the fact that humans have become obsolete in relation to their own products, as Günther Anders has described in his The Obso- lescence of Man.38 I am very sympathetic with posthuman discourse and with the idea that the humanities must fight against any anthropocen- trism for what the Italian theorist Rosi Braidotti calls posthumanities.39 However, certain forms of posthuman discourse also betray a naive attitude toward technology, seeing it simply as secondary to a “true” and “good” posthuman ontology, as if all oppositions can be neatly resolved by a theoretical canon, whether this be a process philosophy or a relational ontology, while completely ignoring the transformation of machine-organism relations that we have endeavored to illuminate. 
Transhumanists, on the other hand, take an opposite position and exploit technology to an extreme. They embrace functionalism (see- ing the human as composed of functions that can be improved indi- vidually) and an interdisciplinary program for human enhancement, including information technology, computer science, cognitive sci- ence and neuroscience, neural-computer interface research, materials science, artificial intelligence, regenerative medicine and life exten- sion, genetic engineering, and nanotechnology.40 They emphasize the importance of technology as a means to extropia (as opposed to the “static utopia”), an open-ended perfection of the human species.41 There is an ambiguity between the terms transhuman and posthuman. For example, transhumanists like Nick Bostrom see the transhuman as a form of the posthuman, which possesses some posthuman capacities transcending the limits of the human (for example, life span, cognition, and emotion).42 We may recognize that the transhuman sounds like a typical “scientific humanism,”43 and indeed it is a humanism under the guise of a posthumanism. We want to point out that the posthuman cannot be defined according to a simple divide between a clean post- humanities and an outdated humanism,44 so the transhuman cannot be seen as an enthusiastic open transhumanism opposing a closed dualist humanism. 
However, here we would like to question the notion of humanity before we justify distancing ourselves from it. Carl Schmitt, in his The Concept of the Political, claims that “the concept of humanity is an especially useful ideological instrument of imperialist expansion, and in its ethical-humanitarian form it is a specific vehicle of economic imperialism. Here one is reminded of a somewhat modified expression of Proudhon’s: whoever invokes humanity wants to cheat.”45 What Schmidt is saying here deserves our attention, since the term humanity itself is problematic and any attempt to define a new future for human- ity seems to be a form of cheating. Schiller was able to talk about the realization of humanity since Enlightenment humanism was necessary for his epoch, while after more than two hundred years we will have to face a new politics announced as the end of the Enlightenment.46 This politics concerns not so much human nature as inhumanity. Human- ist discourse continues, and it is indicated by a political theology that entails a rather simplistic conception of world history, which we can analyze as a linear progress from premodern ? modern ? postmodern ? apocalypse. This Judeo-Christian eschatology seems to be a domi- nating discourse in which science and technology will bring forward a system that is more and more favorable for human existence while it will finally confront self-destruction, and what remains is the salva- tion or the completion of world history as theodicy. It is not without surprise to see that this end of history resonates with the concept of the Homo deus, since by then theodicy will be indicated by transforma- tion of humanity into a kingdom of gods. The author of Homo Deus introduces dataism, a human-algorithm reduction; in the name of “life science,” dataism claims that: 
1. Organisms are algorithms, and humans are not individuals—they are “dividuals”; that is, humans are an assemblage of many different algorithms lacking a single inner voice or a single self. 
2. The algorithms constituting a human are not free. They are shaped by genes and environmental pressures, and take decisions either deterministically or randomly—but not freely. 
3. It follows that an external algorithm could theoretically know me much better than I can ever know myself. . . . Once developed, such an algorithm could replace the voter, the customer, and the beholder. Then the algorithm will know best, the algorithm will always be right, and beauty will be in the calculations of the algorithm.47 
The transhumanist tone, claiming insight from “life science,” has 
already pointed to the future of humanity, which can be reduced to arti- ficial intelligences governed by a superintelligence that knows anything and everything. We can find a similar argument in Teilhard’s concept of the noosphere. The noosphere will finally lead to the realization of a superorganism: the Brain of all brains. Through the systematization and planetarization of tools—especially automation—it will finally lead to the complete liberation of human beings from production—or, in economic terms, mass unemployment. Teilhard doesn’t see this mass unemployment as a danger but rather as the possibility of the realization of humanity. Like Schiller, who was concerned by the determination and domination of reason, Teilhard was also obliged to address the question of freedom. He distinguishes two types of freedom: individual freedom and collective freedom. The realization of the technical system as a superorganism may undermine individual freedom, but it also real- izes collective freedom: “One might put it that determinism appears at either end of the process of cosmic evolution, but in antithetically opposed forms: at the lower end it is forced along the line of the most probable for lack of freedom; at the upper end it is an ascent into the improbable through the triumph of freedom.”48 
Teilhard avoids a crucial problem here: What really is “collective freedom,” and how can it justify the sacrifice of individual freedom? 
Is it not similar to the argument of “collectivism” that we have seen in former communist regimes? And further, what precisely is meant by “convergence”? We have seen in chapter 4 what Simondon calls convergence, which is not the convergence facilitated by transport and communications network, but rather to reattach the figure to the ground. But maybe at the end, for Teilhard, the question of the future of humankind is fundamentally a theological one, as he indicated in a note in the text we cited above titled “The Formation of the Noosphere,” published in Revue des Questions Scientifiques: “The description of the Noosphere and its attendant biology, as here propounded, is no more opposed to the Divine Transcendence, to Grace, to the Incarnation or to the ultimate Parousia, than is the science of paleontology to the Cre- ation, or of embryology to the First Cause. The reverse is true.”49 
What Teilhard said concerning the process of evolution and the realization of the superorganism is now much easier to imagine than when he wrote it in the first half of the twentieth century. Today this image is reinforced by the fantasy of the technological singularity, in which the speed of technological development will be indicated by a vertical acceleration. We may say then say that this is a true completion of humanity, since there will no longer be a sharp distinction between the finite and the infinite. In a review of The Phenomenon of Man titled “Cosmologist of the future,” Joseph Needham called Teilhard “the greatest prophet of this age.”50 He admired Teilhard’s work and regarded the “convergent integration” (a term that was employed by Julian Huxley in his preface to The Phenomenon of Man) of the super- organism or superbrain as the most original point of the book, which could be seen as a “Christogenesis”: 
[T]ime is also of the essence; there was a time when there were atoms but no molecules, later on there were nucleoprotein molecules but no living cells, later fishes but no mammals, later man but no co-operative commonwealth. What are these propositions? Simply, the view of the universe held by the overwhelming majority of working scientists in our age. Implicit in it is the conviction that social evolution is continuous with biological evolution, and therefore that what materialist theologians have 
called the kingdom of God on Earth is not a desperate hope but a sure development with all the authority of evolution behind it.51 
However, in the spirit of eschatology one may ask: Is this completion of humanity a revelation or a catastrophic becoming? We are asking this question, as most of the sci-fi movies do, since we are living in an epoch of technological uncertainty and instability. Cybernetics, the accomplishment of metaphysics, is the force unifying “humanity” through globalization and neocolonization. In other words, we can use the vocabulary of Gestalt psychology in claiming that technology becomes the ground instead of the figure. The noosphere becomes the most dominating sphere on earth, overriding the biosphere. The system is an indication (or the Absolute in the Hegelian sense) of the evolution of science and humanity,52 but it doesn’t necessarily take the form that Schiller envisaged in terms of artistic creation. Any future philosophy that ignores the question of system is fundamentally deficient. 
§41. INHUMAN CONTRA SYSTEM 
We would now like to return to Lyotard’s critique of system and to offer a reinterpretation of his concept of the inhuman. It is important to bear in mind that the form of resistance Lyotard is talking about here is not a humanist critique but rather inhumanist. The concept of system poses a major problem to Lyotard and is one of the main fea- tures of postmodern society. If systemic thinking becomes dominant, it is because it shows itself to be a better explanation of the efficient cause and the final cause. It is against system that Lyotard proposes the concept of the inhuman. The inhuman is the leading concept of his collection of essays and conference presentations that he delivered to a general audience: The Inhuman: Reflections on Time. Although it is not written for specialists, The Inhuman remains one of the most important publications of Lyotard, since it also allows him the free- dom to speculate on some themes that appear “too dialectical to take seriously.”53 The systemic becoming is the inhuman, since it owes its metaphysical root to development; it is the mastery of human being over all beings: 
The striking thing about this metaphysics of development is that it needs no finality. Development is not attached to an Idea, like that of the eman- cipation of reason and of human freedoms. It is reproduced by accel- erating and extending itself according to its internal dynamic alone. It assimilates contingencies [hasards], memorizes their informational value and uses this as a new mediation necessary to its functioning. It has no other necessity than a cosmological chance [hasard].54 
How should we understand the two occurrences of the word hasard in this passage? We rendered the first as contingency, because becoming system means precisely the capacity to assimilate contingencies into its operation. That is to say, contingency is not something destructive that interrupts the causalities of the system, but rather that which allows the system to empower its internal dynamic. We render the second occur- rence of hasard as chance or accident, since in such a system there is no longer any difference between necessity and contingency, as what we tried to demonstrate with Schelling’s concept of nature. Recursion extends from the mechanism of nature to the mechanism of the machine, the mechanism of capital and now the mechanism of globalized culture. Development, as Lyotard continues, “has thus no end, but it does have a limit, the expectation of the life of the sun.”55 What is meant by an endlessness with a limit, an affirmative negation? This brings us to the famous essay in the collection, “Can Thought Go On without a Body?” This essay is a conversation between a female interlocutor and a male philosopher. It starts with an event, which is the explosion of the sun in 4.5 billion years’ time that will put an end to all organic life, an event after which nothing is thinkable—an event that Lyotard himself coins 

solar catastrophe and Ray Brassier considers to be the ultimate chal- lenge to what Quentin Meillassoux calls “correlationism.”56 
The destruction of all organic life points to the only possibility for the survival of the human, which is the separation between body and mind, between hardware and software. This metaphor of software and hardware is technological, but it is also not a metaphor because it is a research agenda that covers everything from dietetics, neurophysiology, genetics, and tissue synthesis to particle physics, astrophysics, electron- ics, information science, and nuclear physics.57 The search for the sepa- ration between thinking and organic life is a response to the prospect of solar catastrophe, since the central question is, how is it possible to survive without an organic form of life? Or, as Lyotard puts it: “[H]ow to provide this software with a hardware that is independent of the con- ditions of life on earth?”58 This is a negative organology, or an extreme humanism. It is negative since it is based on a total negation of the organic and on the belief that there is a possibility, no matter how small it might be, of replacing the organic body with an inorganic artifice for the survival of thinking. Lyotard, through the incarnation of a female interrogator called Him, implicitly goes back to the recursive structure of organization and the possibility that such a recursive algorithm could be independent from the organic body: 
Most of all: [human]’s equipped with a symbolic system that’s both arbitrary (in semantics and syntax), letting it be less dependent on an immediate environment, and also “recursive” (Hofstadter), allowing it to take into account (above and beyond raw data) the way it has of process- ing such data. . . . Isn’t that exactly what constitutes the basis of your transcendence in immanence?59 
The notion of recursivity is raised here, but Lyotard does not explore the relation between recursivity and reflective judgment further. He did not understand the concept of recursion, just as he had already dismissed information theory in cybernetics for its “triviality” earlier in his The Postmodern Condition. Here he is prepared to reject this thesis 

by invoking Hubert Dreyfus, whose What Computers Cannot Do? A Critique of Artificial Reason (1972) challenged the research in artificial intelligence (AI) of that time as being too Cartesian in the sense that AI reduces intelligence to a very limited way of knowing. This could be briefly explained with what in classical AI or “Good Old-Fashioned AI” (GOFAI) is called the frame problem, which is about the AI’s description of the world. In order to know an event or an environment, the AI will have to produce a huge amount of descriptions. However, it remains very difficult to contextualize these descriptions. It is Cartesian because, in this form of knowing, everything is merely present-at-hand in the sense of Heidegger, while it ignores the fact that in the preoc- cupations of everyday life Dasein encounters situations that are ready- to-hand and have to do with embodiment and intuition. The rejection of reducing thinking to a binary form is also a rejection of the separation between body and mind. The philosopher, who is challenged in this dialogue, is also a phenomenologist. He has to defend the importance of the body and of sexuality, since without the body and without sexuality, can thinking exist at all? Brassier has nicely summarized the perspec- tives of the two interrogators: 
one for which the inseparability between thought and its material sub- strate necessitates separating thought from its rootedness in organic life in general, and the human organism in particular; another accord- ing to which it is the irreducible separation of the sexes that renders thought inseparable from organic embodiment, and human embodiment specifically.60 
If becoming system presents a negativity for Lyotard, this is because it is based on a negative organology, which ignores the question of life and existence. And if Lyotard here invokes this negativity, it is because he wants to think through the question of resistance, as he asks in his introduction: “[W]hat else remains as ‘politics’ except resistance to this inhuman?” This resistance is also inhuman since the negative inhu- man doesn’t occupy the totality of this concept. Like the sublime, the inhuman also has its double, as Lyotard emphasizes: “The inhumanity of the system which is currently being consolidated under the name of 

development (among others) must not be confused with the infinitely secret one of which the soul is hostage.”61 
The inhuman is truly posthuman in the sense that it considers the dissolution of the human as messages, waves, particles, and cells. How- ever, the inhuman is not transhuman. Although the inhuman shares the negativity of the transhuman—that is to say, it is imprisoned by the fanaticism of development or technological singularity—at the same time it resists such negativity not by rejecting a human-machine hybrid- ity but by rejecting the tendency imposed by a transhumanist ideology that is motivated by the anticipation of the solar catastrophe and desire of inorganic immortality. What is meant by “the infinitely secret one of which the soul is hostage”? Ashley Woodward identifies the double of the inhuman by suggesting that the negative inhuman can be identified with nihilism, and further that art is the second sense of the inhuman.62 However, I have strong reservations about this second observation since this is too narrow and it does not seem to be what Lyotard was refer- ring to, though it is interesting here to consider in art the potential of overcoming the determination of the system. If the soul is the hostage of the inhuman, it is because the inhuman is like its preindividual reality as well as its call. It is like water to fish: Even though the latter live in the former, it remains transparent to it. This inhuman cannot be reduced to calculation and to representation. The possible explanation of seeing an intimacy between art and the inhuman is that art sends the system back to a primordial creativity in order to undo the totalization of the system. It is clearer when we refer to Lyotard’s reading of Augustine. However, instead of discussing his The Confession of Augustine, I will instead make a short-cut by referring to an episode of a TV program called Apostrophes that was broadcast on the January 9, 1981. I transcribe part of the lengthy conversation below. 
JFL: You remember that in the eleventh book that you cited, and that you remember, those confessions, there is this formula, it is a god more interior in myself than me, that is what I make allusion to, what Wilson searches, it is that, isn’t it? There is something in me which is more interior in myself than me, well, this what I call the inhuman, I have 
The Inhuman That Remains 255 the right, it is perfectly clear, in fact, because it is just something with 
which I will never arrive at having . . . 
Interrogator: Vulgarly, when we employ the word inhuman, we think about the horrible, appalling, cruel, and detestable, we don’t think about interior being which unfolds . . . 
JFL: You do it on purpose! 
Interrogator: But I am not philosopher, I am journalist, I am a bit flat.63 
Lyotard sometimes refers to this inhuman “which is more interior in myself than me,” as la chose or the child, which carries within it the antidote to the negative inhuman. However, these two inhumans are not completely separate, since the latter is also partially a condition for the former, without which the positive inhuman remains merely an element of theology, meaning that there is only one mode of rationalization of the Unknown through God. The logical sense of the inhuman is exem- plified in Ludwig Wittgenstein and Gödel, since both logicians refused the subordination to positivism. Like Gödel, who shows the incom- pleteness of any logical system in terms of proof, for his part Wittgen- stein “did not opt for the positivism that was being developed by the Vienna Circle, but outlined in his investigation of language games a kind of legitimation not based on performativity.”64 The positive inhu- man is that which resists systematization and reduction to calculation. The question is, how can we articulate the question of the inhuman, which is not hermeneutic, and not reflexive, without returning to theol- ogy or mysticism? 
The concept of the inhuman (like the Unknown) should be consid- ered an organological concept rather than a theological one since it is not necessarily the transcendent God. Lyotard rejects the reduction of thinking to algorithms or to the determination of any technological system, but he doesn’t explicitly reject technology. In some places the intimacy between technology and culture as modes of inscription is the 

condition under which thinking is possible, and this condition always 
carries a negative dimension such as incompleteness, lack, or obstacle: 
[W]e think in a world of inscriptions already there. Call this culture if you like. And if we think, this is because there’s still something missing in this plenitude and room has to be made for this lack by making the mind a blank, which allows the something else remaining to be thought to hap- pen. But this can only “emerge” as already inscribed in its turn.65 
There is something that presents itself as a lack, which hurts the already thought as plenitude, since it suspends the already thought in order to allow something new to come. Like the leaving of blank margins in Chinese and Japanese calligraphy and painting, the empty is what completes the fullness; the empty is already inscribed. I would like to return to what we discussed in the previous chapter regarding the ratio- nalization of the incalculable or the unknowable, though here Lyotard may use the terms unpresentable or unthinkable. The transcendence would be challenged by the transhumanists: What could not be thought by a superintelligence? And if all is already inscribed in the superintel- ligence, there is no longer an unthought. Does it also mean that there will be no longer any thinking, and no longer anything contingent? 
§42. CONTINGENCY AFTER SYSTEM, OR TECHNODIVERSITY 
In Toward the Postmodern, after having said that we are “in an Umwelt that is the realization of metaphysics as a general physics under the name of cybernetics,” Lyotard continues: “[I]n the Umwelt I am describing, all politics is certainly nothing other than a program of decisions to encourage development. All politics is only . . . a program of administrative decision making, of managing the system.”66 Decades after poststructuralism we are in a much more embarrassing situation with technical systems. Lines of flight can exist only as a refusal to engage with the system, as a self-marginalization or escape to occultism 

and sectlike communes. The question of system remains to be tackled, not only from the perspective of deconstruction, which was carried out in the twentieth century, but also to fragment the system by allowing diversity to emerge.67 
Meillassoux’s notion of absolute contingency provides another perspective from which to approach the inhuman, since he refuses the privilege given to what he calls correlationism as the only possibility of knowledge; or, more generally, he provides an ontological refusal of a unified system of knowledge based on subject-object correlation. The correlation between the thinking subject and the thought object privi- leges a subjectivism that excludes the unthinkable or speculation as a veritable possibility. Empiricism cannot accept that the unthinkable is possible, since if it were possible, it would have to admit the transcen- dental. What is fundamental to Meillassoux’s challenge to correlation- ism is its anthropocentrism. As he writes: 
Would there not be more modesty, then, in considering that the Universe has nothing to do with our subjective qualities, that it could very well do without them at any degree whatsoever, and to say, more soberly, that there is no absolute scale that makes our properties superior (because more intense) to those of nonhuman living creatures or inorganic beings?68 
Contingency is that which exceeds correlationism, and in a certain way we may say the opposite—as did Schelling, whom we quoted at the beginning of chapter 1—that is, that maybe it is correlation itself that is contingent, or, as Paul Klee says in his Notebook, “[W]hat is visible is but a fragment of the whole, there being many other latent realities,”
which according to Blumenberg is a “devaluation of nature.”69 Contin- gency is necessary since it challenges the absolutization of correlation- ism, which in fact leads to a de-absolutization. Reason finds itself in the midst of a jungle of order and disorder. If we admit that correlationism is not the only way of knowing, and that knowledge cannot be reduced to the experience of the subject, it is possible to think of a materialism that is speculative instead of merely factual. Meillassoux aims for an absolute heterogeneity of knowledge, with differences in nature instead of differences in degree, since differences in degree imply a monism, or a fake pluralism: 
We do not need a monism—or a monopluralism, a monism of difference that seeks to be a pluralism (the magic formula: “monism = pluralism”) but ends up reabsorbing all things into one and the same Whole (albeit an open Whole) to a greater or lesser degree (the tragic formula: “pluralism = monism”). On the contrary, what we need are dualisms everywhere: pure differences in nature, with no continuity whatsoever between that which they make differ, between the many regimes of the real—matter, life, mind, society, etc.—whose possible coordination does not at all allow us to think their rapprochement, unless in a crude mode of blind fact.70 
Absolute contingency implies both the limit of thinking and the limit of the unthought: the former, because thinking is limited when it is based on correlationism; the latter, because the unthought can only present itself partially as contingency. The correlation didn’t exist in “ancestral” times. However, as Brassier has showed, this is not the most efficient way to reject correlationism, since this ancestrality can still be thought as such in terms of chronological time, like what paleontologists have been doing, speculating on the images left by the fossils. For Brassier, Lyotard’s solar catastrophe would be a better refusal of correlationism, since it is the annihilation of thinking; as he quotes Lyotard: “[A]fter 
the sun’s death, there will be no thought left to know its death took place.”71 But what is the use (Gebrauch) of thinking the incapability of thinking if this incapability is not fed back to thinking itself in order to interrupt thinking as such? That is to say, does it have an effect at all? It is in relation to Meillassoux’s explication of the relation between con- tingency and pluralism that we may be able to endow his absolute con- tingency with the function of fragmenting the system, and, further, that in between two systems there is a discontinuity or a difference in nature. This is the positive use of absolute contingency. Like Gödel’s incom- plete theorem, it obliterates the illusion of a complete formal system. It is an ontological refusal of monism and a monist system. Contingency means precisely that it can be otherwise or not be. It presents itself as an inessential irruptive fulgurite, which is an irruption ex nihilo, rather than following the principle of sufficient reason.72 However, our read- ing may deviate largely from the author’s own intention in the sense that this is not what Meillassoux really intends to say. The speculative materialism of Meillassoux needs criteria that can justify that it is not unscientific, otherwise it may repeat what Kant calls the Schwämerei of speculation; his critique of Kant and the awareness of the problem of the Schwämerei force him to refrain to another island other than pure reason. This criteria is what he calls “Galileanism,” or, more simply, mathematization, as he is seeking “a materialism capable of founding the thinkability of a nature that is different to our existence and fully mathematizable.”73 This is the same gesture that we find in Bertalanffy and Needham concerning the scientificity of organicism, and which turns into a mechanical organicism. Mathematics is able to describe a world that is independent from the thinking subject, and it is not merely empirical or factual. It is the intention to invent an epistemology that is not based on subjectivism that leads Meillassoux to conceive “signs devoid of meaning,” which have an affinity to mathematics. 
Signs devoid of meaning are antisensible, since they don’t acquire their quality (quality doesn’t necessarily mean meaning here) through sensible difference; in other words, they don’t acquire their identity through the sensibility that is exhibited in time and space, for example, a melody or a motif. The ontology of empty signs is an anti-Bergsonian 
ontology, since Bergson searches for a sensible difference in time and space (by reducing space to temporal experience). In contrast, Meillas- soux wants to affirm an operation or operations of empty signs that are devoid of sensible difference. This is the reason that he makes a distinc- tion between repetition, iteration, and reiteration. 
In repetition—for example, the repetition of the note fa in a mel- ody—each repetition produces a sensible difference, like a motif whose sign repeats in space; it is differential and limited. Iteration is not rep- etition in the sense that it doesn’t produce sensible differences, since it produces only a pure identity, for example: §§§§§§§§§§. Finally, there is reiteration, which is differential and unlimited. Reiteration is a concept that is not satisfactorily explained: “[T]his third type of recur- rence is differential like repetition, but differential in a different way than the latter, since it is conditioned by iteration and opens onto the indefinite.”74 This third type of recurrence is not simply iteration since it raises iteration to another level: 
Reiteration is the foundation of “potential infinity” and the source of all naïve arithmetic. It is involved in mathematical practice not only as a privileged object, but also as a method, namely, in mathematical recur- rence. Reiteration is the entry into the differential territory of iteration: the possibility of thinking differences outside the field of sensible repetition.75 
We may want to ask: Is Meillassoux not really talking about recur- sion here, especially the concept of recursion developed by Gödel and later by Kleene? His confidence in reiteration seems to be based on his ignorance of the history of recursion and history of technology at large. This ignorance risks weakening, if not obliterating, his argument. On the one hand, a complex recursive function is a system of meaning for a mathematician, but in the course of operation it can become completely opaque, since the human mind will lose track of it—it becomes “devoid of meaning,” or, as it is sometimes called, a black box. If it is the case, it is also possible to ask Meillassoux whether machinic knowledge is the noncorrelationalist knowledge that he is aiming at. On the other hand, regarding “potential infinity,” it is not clear how different it is from the Kantian natural end or the malfunction of the Turing machine
with infinite paper tape. The problem of Meillassoux’s inhumanism is that it is only a halfway house, since it refuses to take modern technol- ogy into account or simply treats it as a classical question of logic, so one can still speak like philosophers before the invention of the Turing machine and before digitalization. The formalism that Meillassoux invokes regarding Georg Cantor and David Hilbert, as we have tried to show in the method of Gödel, first becomes arithmetic through Gödel numbering, and mathematical proof becomes a conceptualization of recursivity. It seems to me that it is a step behind today when one is looking for a nonsubjective (human subject) way of knowledge produc- tion, since one has the right to ask if the searching for correlations in big data is not precisely an anticorrelationist strategy. One may want to ask if Meillassoux’s ontology of empty signs is only an affirmation of computationalism instead of really opening up the heterogeneity of knowing and the plurality of systems. 
There seems to be an impasse in Meillassoux’s desire for a new epistemology. But it is necessary to notice that Meillassoux renounced being a reductionist. He is not seeking a mathematical reductionism, but rather sees very clearly the irreducibility of art and life; as he says: “I observe the mathematization of the real, without entering into its theories; and I observe the irreducibility of knowledges and arts one to the other.”76 For us, this irreducibility is at the core of an organological struggle, and organology is not a correlationism. Rather, organological thinking is a synthetic thinking. It is an attempt to connect different regimes and domains in order to preserve life and advance science and technology. There are two significant aspects of Meillassoux’s inhu- manism. One is the necessity to think beyond the human, although how this form of epistemology can be formalized is still a big question. The other is to take the concept of contingency and the opening stretched out by Meillassoux to consider the fragmentation of system. This has to be distinguished from naive discourse on postmodern rootlessness (rootlessness in the sense that cultural differences become no longer significant). On the contrary, fragmentation is a return to locality in order to find a strategy to appropriate the inhuman system, not solely from an economic point of view, but rather with an aim of diversity. To fragment the system is not to refuse science and technology, which are 
76 Ibid., 154. 

262 Chapter 5 

its foundation—and here we must recognize the very limit of sabotage, for it will never do any harm to the system since it is only a contingent event for ameliorating the system—but rather to develop different sci- ences and technologies, to develop different cosmotechnical relations, and in order to do so we will need to recognize both the technical reality and human reality. 
In The Question Concerning Technology in China, I engaged with the projects of anthropologists such as Philippe Descola and Eduardo Viveiros de Castro, which demand an ontological pluralism in order to surpass modernity by refusing nature as a single system.77 The latter is what Descola calls naturalism, a concept of nature reposing on the opposition between culture and nature (besides naturalism there are other ontologies such as analogism, totemism, and animism). Viveiros de Castro has criticized Meillassoux for speaking only from the per- spective of Judeo-Christian eschatology, since Meillassoux proposed to question the world without human beings, in which correlation- ism cannot properly function since there is no direct correspondence between the two parts. In contrast, Débora Danowski and Viveiros de Castro propose that in Amerindian mythology it is the opposite: At the beginning there is the human without the world.78 In other words, Meil- lassoux follows the logic of Genesis—God creates the world before creating human beings—while in Amerindian culture such genesis doesn’t exist. This critique from Viveiros de Castro and Danowski can be taken simply as a reactionary and postcolonial critique of Meil- lassoux’s speculative philosophy imbued with Judeo-Christian ideol- ogy. However, it can also be read as an affirmation of Meillassoux’s emphasis on a pluralism with differences in nature.79 The affirmation of different natures is an affirmation of locality, and such a question of locality cannot be fully posed as a return to indigenous knowledge or a Romantic concept of nature, but rather as a reopening of the question of technodiversity and the strategies needed to maintain and continue the ramification of these diversities. 
However, we must qualify a distinction here between the nonhuman, a category that plays an important role in the “ontological turn,” and Lyotard’s inhuman. The nonhuman is other than human—for example, plants, animals, and minerals—but the inhuman is precisely the nega- tion of the human, what it is not and what it will never be, but the inhuman is inside it. If the concept of the human changes, the inhuman that is its other changes as well. The inhuman may carry the name of God, the infinite, the noumenon, absolute contingency, and so on, but affirming the inhuman will also demand a rationalization that renders a coherent form of life or life of the spirit. Technology in the twenty-first century is becoming inhuman in a negative sense, because it is human, all too human. 
The inhuman of Meillassoux is different from the inhuman of Lyotard, precisely because the former poses a problem for Lyotard. This is because Meillassoux’s inhuman is the affirmation of a nonhuman way of produc- tion of knowledge and systematization—the recursion of meaningless signs—while it is possible to conceive Meillassoux’s inhumanism as a radical opening of production of knowledge that Lyotard didn’t realize. For us, the question is, how is it possible to open up a pluralism when the organizing inorganic is presenting itself as an alienating force, threat- ening to totalize the production of knowledge and the determination of rules? This is the significance of conceiving a cosmotechnical thinking, not only as a philosophy of technology but also as a strategy for rethink- ing the coexistence between humans and machines, organic subject and organizing inorganic, the artificial earth and the cosmos. We are not call- ing for a return of humanism against the inhumanism of the system, but rather trying to conceive the inhuman as a possibility that transcends the system. Insofar as we can speak of a real pluralism and such pluralism is realizable, it is necessarily supported by a technodiversity. The question of technodiversity directs us to the question of epistemology (way of knowing) and episteme (the sensibility that underlies such way of know- ing). The most inhuman part of the human is its sensibility (or intuition, if you wish), which, instead of reason, is the foundation of the moral. Exiting the positive feedback loop that characterizes the modern vision of progress, it is possible for another thinking to function either by negat- ing it or by transcending it—that is to say, by inventing another recursive process, another epistemology, as Bateson might suggest. 
§43. SENSIBILITY AND PASSIBILITY 
The discussion of Lyotard’s concept of the inhuman is a preparation for a cosmotechnical reappropriation of the organizing inorganic. The proposal of fragmenting the system is an attempt to reflect on technodi- versity, which is reduced to a single world history of which the Homo deus is its culmination. In the end, the development of a system approxi- mating a political theology is fundamentally a synchronization and convergence in the sense of Teilhard’s noospheric reflection. Teilhard’s noosphere is very close to Vladimir Vernadsky’s use of it, which desig- nates a phase of the development of the earth after the geosphere and the biosphere. The noosphere is fragmentable due to its being inorganic and its becoming organic. Teilhard’s noosphere is evolutionary in the sense that it has its origin in the Western concept of time as progress, and it will have to conquer cultures that seem to him antitime and antievolu- tion: namely, the Eastern way of thinking, which is devoid of love and progress as well as ignoring synthesis and world as an organic whole.80 
The noosphere has to be challenged for the sake of a noodiversity as an overcoming of the system, however noodiversity also demands technodiversity as its material support. How is this technodiversity possible in a world where capital is striving for synchronization and convergence? Some theorists believe that with full automation it is possible to emancipate both technology and workers from capital- ism, however, they committed the mistake by seeing technology as a universal and that there is only one single history of technology or human-machine complex. It is rather obvious that every nation-state is going to have its own Ministry of Accelerationism (e.g., Dubai appointed its Minister of Artificial Intelligence in 2017), and it is hard 

to imagine that this will be an emancipatory politics and not one that only further strengthens the synchronization of the global axis of time. I attempted to show in The Question Concerning Technology in China that, besides considering the different natures that anthropologists propose, it is necessary to consider different cosmotechnics in order to conceive the possibility of the bifurcations of future and world history. A question is raised immediately: What precisely is the difference between Chinese technology and European technology? Does it mean that they produce spoons of different shapes? But are they not of the same function: spoon? It was not my intention to say that technolo- gies are different functionally, but rather that one has to look beyond functionality, as both Heidegger and Simondon endeavored to do so. Historians, when comparing technologies in different geographical regions, tend to understand which one is more advanced than the other: for example, papermaking in the second century in China was more advanced than in Europe during the same period, or, as Bertrand Gille contested, one shouldn’t compare a particular technology but technical system as a whole. Both cases presuppose an understanding that technology is universal and all technologies could be measured according to a universal progress. When we say different cosmotech- nics, it means to challenge this dominant view in philosophy and his- tory of technology. We will present this différance with an antinomy of the universality of technology: 
Thesis: Technology is an anthropological universal, understood as an exteriorization of memory and the liberation of organs, as some anthro- pologists and philosophers of technology have formulated it; 
Antithesis: Technology is not anthropologically universal; it is enabled and constrained by particular cosmologies, which go beyond mere func- tionality or utility. Therefore, there is no one single technology, but rather multiple cosmotechnics. 
The thesis states that technology has its universal part: for example, the exteriorization of memory and the liberation of organs, which Leroi-Gourhan has already shown very clearly in Gesture and Speech and which we have discussed in chapter 3 concerning the organized inorganic. Then there is also a nonuniversal part, meaning that tech- nology is always complicit with an episteme that is fundamentally 

cosmological and irreducible to universal values.81 It is also the same Leroi-Gourhan, who joined the expedition team in Beijing in 1932 in which Teilhard de Chardin also took part, who warns us in the second part of his book Rhythm and Memory, where he expresses his worry of the arrival of complete synchronization: “Individuals today are imbued with and conditioned by a rhythmicity that has reached a stage of almost total mechanicity (as opposed to humanization).”82 Leroi-Gouhran’s warning came out of the anxiety of an epoch of the mechanical industrialization. Today, as we tried to show, such a clas- sical humanist critique has to be reevaluated, but he is at least right when pointing out the increasing synchronization of corporal, social, and cultural dynamics. 
If we follow Lyotard in saying that the positive inhuman consists of the possibility of resistance, we still need to develop it further. This inhuman is the Unknown, which poses a challenge to the inhuman sys- tem and functions as the necessity of contingency. But here we must respond to a question from the scientists: Are we not here sacrificing science and technology to the Unknown, or, more precisely, to a mythi- cal and religious thinking? This is the central dilemma of moderniza- tion, since in view of modern science archaic cosmologies have to give way. Kant’s attempt to give room to religion is condemned as being lazy and insufficiently rationalist, but here it is not only a question of religion but also of moral values, which can exist only in relation to a cosmology: an axio-cosmology. Modern science is universal insofar as it is applicable to physical phenomenon, as Kant already anticipated, but science and technologies are bounded in broader cosmic realities that cannot be reduced to astronomy. With this notion of axio-cosmology in mind, we would like to come back to the question of sensibility and aesthetics. In the last chapter of Science and the Modern World, “Req- uisites of Social Progress,” Whitehead, like Schiller, raises the question of art and aesthetic education. While commenting on the problems left by the industrialization of the nineteenth century, he attributes these to the unachieved project of aesthetics:

The evils of the early industrial system are now a commonplace of knowl- edge. . . . A contributory cause, of substantial efficacy to produce this disastrous error, was the scientific creed that matter in motion is the one concrete reality in nature; so that aesthetic values form an adventitious, irrelevant addition.83 
In the nineteenth century, Whitehead sees a disaccord between aesthetic intuitions and the mechanism of science,84 which leads to such a “disas- trous error.” Whitehead also uses the word “sensitiveness,” which for him includes “apprehension of what lies beyond oneself; that is to say, sensitiveness to all the facts of the case.”85 For Whitehead this sensi- tiveness can be understood as an intuitive intimacy between parts and whole.86 We will affiliate sensitiveness with what we call sensibility. Whitehead challenges mechanistic science and proposes to understand time and space as relational, hence organic. For Whitehead the aim of constructing an organic philosophy is to “construct a system of ideas which brings the aesthetic, moral, and religious interests into relation with those concepts of the world which have their origin in natural science.”87 This paradigmatic change that Whitehead is aiming at also demands a symbolic support, which is technics. 
It has been suggested that there are similarities between Thomas Kuhn’s concept of paradigm change and Michel Foucault’s concept of episteme, a concept that the philosopher abandoned after The Order of Things. In The Order of Things, Foucault attempts to show how knowledge was produced under different epistemes between the sixteenth and the nineteenth centuries: the Renaissance, the classi- cal, and the modern. I am tempted to understand episteme in terms of sensibility, or, more precisely, the conditions under which such knowledge is produced. Sensibility is always local and historical; it is also the condition of noodiversity. For example, epistemes in Europe were different from epistemes in Asian and African cultures, since underlying these different epistemes are different sensitivities and different senses of existence in relation to the cosmos. I would like to offer a rather unconventional interpretation of the relation between the positive inhuman and the question of sensibility that Lyotard raises in his exhibition Les Immatériaux. This hinges on the question of whether the postmodern is a new episteme, and if so, in what way this episteme is related to technology. Lyotard didn’t connect his notion of the postmodern with Foucault, but it seems to me quite reasonable to make such a connection. The postmodern for Lyotard presents a new sensibility, which was the theme of his 1979 The Postmodern Condi- tion and the main discourse of his 1985 exhibition Les Immatériaux. Lyotard wants to invoke in the exhibition a sensibility of insecurity, of uncertainty, of anxiety. The role of art, and here this exhibition in particular, is the means of sensibilization. The reconstitution of the episteme is what I understand as the discovery of “sensibility” and the project of “sensibilization”: 
“The Immaterials” . . . is a kind of dramaturgy of the epoch that is born. We want to make you feel. This is neither pedagogic nor demagogic. We don’t flatter you (see how well you are), we don’t educate you (see how smart we are). We seek to awaken a sensibility already there in all of us, to make feel [faire sentir] the strange in the familiar, and how difficult it is to get an idea of what is changing.88 
I believe that Lyotard wanted to demonstrate a new sensibility (or maybe we can say an epochal sensibility) and therefore to sensibilize the postmodern through the medium of art and new technologies. Such sensibility, it seems to Lyotard, is able to provide a new framework and new meanings to techno-logos, to illuminate the possibilities opened up by the new technological epoch?? in the sense of phenomenology. This epoch?? doesn’t mean that technology will become the new ground, but rather a new condition under which new syntheses and new composi- tions will have to be produced. Lyotard turned to the thirteenth-century Japanese monk Dôgin’s concept of the “clear mirror” to seek a passibil- ity or a passage (passibilité, a term that he used to translate Sigmund Freud’s Durcharbeiten) in the new technologies. This speculative ques- tion is formulated as such: “[I]s the passage possible, will it be possible with, or allowed by, the new mode of inscription and memoration that characterizes the new technologies? Do they not impose syntheses, and syntheses conceived still more intimately in the soul than any earlier technology has done?”89 We can rephrase this question in the following way: How can we think in terms of indeterminism instead of determin- ism? What kind of thinking is necessary for this indetermination to be carried out, instead of seeking refuge in a metaphysics of contingency? However, Lyotard didn’t go far enough, though he still had projects in mind—it was said that Lyotard wanted to prepare a sequel to the exhi- bition titled Les résistances, which plays upon the opposition between noise and information. 
It seems to me that Lyotard’s attempt must be carried further, and beyond European history, and maybe even beyond what he had in mind at that moment: the condemnation of cybernetics as a trivial and deterministic science. What is important in Lyotard’s concept of the inhuman is not only its fundamental critique of humanism, but also its fundamental potential as resistance. But such resistance has to be reinterpreted here as a search for pluralism as indetermination, and therefore as a multiple cosmotechnics. Cosmotechnical thinking is not a call to return to archaic knowledge but rather to reconstruct technologi- cal thoughts and technological genesis in order to reappropriate modern technology. One may reproach the inhuman as a humanist concept, since Lyotard still want to get hold of the phenomenological body, but as we have seen that it is not the case and this kind of accusation offers nothing productive, since it is only a posthumanist identity fetish while ignoring the organological struggle in Lyotard’s proposal. Lyotard refers to Guillaume Apollinaire’s Les peintres cubistes. Méditation 
esthétique (1913), in which the poet says, “[M]ore than anything, artists are men who want to become inhuman.” The part that Lyotard didn’t cite continues: “[T]hey seek painfully the traces of inhumanity which are never found in nature. These are the real truths, and beyond them, we know no reality.”90 For Apollinaire, this truth is always new, since it is never once and for all. It is this contradiction—a verity in constant change—that is opposed to the reduction of such a verity to commu- nicative writings. The latter could be realized by machines, which are capable of reproducing signs devoid of sense.91 
§44. ORGANICISM, ORGANOLOGY, AND COSMOTECHNICS 
We have been on a long excursion from the organic to the inhuman, in order to trace a trajectory from philosophy of nature to a philosophy of technology, while also speculating on the future of such philosophy. The accidentality of technics becomes the necessity of the survival of mankind, while becoming contingent again in the progress of civiliza- tions, and now it comes back to centrality by imposing a necessity, which is no longer simply about the survival of the human species but also that of the earth. Such a task is often ambiguously referred to as ecology. Philosophies of nature à la Bruno, Spinoza, Schelling, Laozi, and Zhuangzi, among others, don’t answer our problem directly today, though they remain inspiring and necessary for developing new trajectories of thought. This seemingly bold statement resonates with the opening quote from Jean-Luc Nancy regarding catastrophe, pre- cisely because organization through cybernetic thinking has realized (in certain sense) the general organism qua cybernetic system, which is called ecology. Technological progress demands new forms of thinking, which is beyond the love-and-hate game of Continental and analytic philosophy, Western and non-Western thought. Here I risk burning the bridge: Seeking salvation in a philosophy of nature may be no longer possible. We are moving away from the first nature, and beyondfirst-order and second-order cybernetics,92 from imitator to observer to constructor. At the same time, we must also move away from the second nature in which every being is considered standing reserve (Bestand). The concept of nature has to be integrated in the concept of cosmotechnics to conceptually avoid the opposition between nature and technics, and this is the reason why at the beginning of this book I speak of a third nature, which is inscribed in the concept of cosmotechnics. In human history there is no linear temporal development from nature to technics, from nature to politics. Rather, there is an Urtechnik, which I name cosmotechnics. Some cosmotechnics may appear more “organi- cist” than others in the sense that they form a dynamic whole, which allows different forms and levels of complexification to be developed in history. Among these cosmotechnics, there was one that was able to mechanicize the whole cosmos and decompose it into standing-reserve, which Heidegger calls modern technology (moderne Technik). Need- ham—the great thinker of the twentieth century, a world-prominent biologist, a founding figure of the history of science and technology in China—when looking at Chinese civilization, found that Chinese tech- nological thinking is not mechanical but highly organicist: 
[T]he philosophia perennis of China was an organic materialism. This can be illustrated from the pronouncements of philosophers and scientific thinkers of every epoch. The mechanical view of the world simply didn’t develop in Chinese thought, and the organicist view in which every phe- nomenon was connected with every other according to hierarchical order was universal among Chinese thinkers.93 
The recent appropriation of Needham’s work in Chinese science and technology attributes the term holism to Needham without knowing that Needham criticizes this fascination with the whole for obscur- ing scientific understanding through a vagueness of wholeness (as we discussed in chapter 1). We may want to read this in parallel with Deleuze’s somewhat brutal reading of the Taoist body in the famous “How to Make a Body without Organs” in A Thousand Plateaus. The Taoist practices sex with a female without ejaculating in order to rein- force his male power or energy, and thereby constitutes an intensive body without organs.94 For sure this is an “exercise” of the “whole,” as Deleuze declares at the beginning of the chapter (e.g., the whole against the codified functionalities and hierarchies of organs). Any recourse to holism that is not able to give an account of its organizational and causal relations and complexity often falls back to a laziness of defending its vulgarity. Science and technology in China, as Needham observed, was not mechanical as was the case in Europe. It is clear to me that Needham has read Chinese thought from the perspective of his early work on organicism, and his reading of Chinese thought is done through Whiteheadian eyes. Needham remains a great thinker of biol- ogy, and his biological thought is analogical to the image of the Chinese thought that he has described for us. However, like the analogy between the beautiful and the good, we may want to ask if this analogy is con- tingent or necessary. 
When Needham turned his eyes from biology to Chinese civiliza- tion after the Second World War, it was a contingent event that began when he happened to meet researchers from China in Cambridge. In the course of time, however, this historical event becomes necessary. In so doing, Needham brings Chinese thought closer to cybernetics. Probably for him, the Taoists are the first cyberneticians. If we follow the logic of Needham, we may be able to say that Chinese technology has not passed through the period of mechanism that prepared for the Industrial Revolution in Europe. However, modernization and global- ization brought about a new situation, one in which cultures subsumed their cosmotechnics to modern technology, which took up cybernetics as automatism without understanding the epistemological changes brought forth by cybernetics. But the automatism that is the dream of mechanism proceeded to realize a “technician system,” as Ellul rightly put it. On the contrary, in the West we also observe a transition from Cartesian mechanism to organicism and cybernetics/ecology. This chronology, which we call the history of thought or world his- tory, is not a universal principle but rather an instance of noodiversity 

as well as technodiversity. Evolution is possible only when there is diversity, since biology has already taught us that evolution should be understood as coevolution.95 The artificial selection applied in the population (instead of Darwinian natural selection) will finally lead to the reduction and even elimination of technodiversity and therefore noodiversity. The question thus raised is, will the recursive thinking in cybernetics allow us to relaunch the question of organicism and technodiversity, or will it, being driven by efficiency for the final cause imposed by capital, finally only realize a purely deterministic complex system that is moving toward its own destruction, like the one Lyotard described? I believe that in order to respond to this question, we will have to recognize two images of cybernetics that, notwithstanding its diverse schools of thoughts and disciplines, could be summarized as the following: 
* One is reductionist; it reduces organisms to feedback systems, which are imitations; it imposes determinism, since all reductions aim for prediction, all predictions are determinisms; its economy is an economy of finality. 
* The other is nonreductionist, in the sense of Simondon’s general allagmatic, which seeks genesis beyond any form of technological determinism; it is open to contingency without only reducing it to calculation and endorses auto-finality or neo-finalism (in the sense of Ruyer). 
The technophobes see the first image of cybernetics; Simondon sees the second image of cybernetics and imagines a universal cybernetics or general allagmatic to resolve alienation and antagonism between nature and technics. Heidegger sees both mechanism and organism as the impasse of philosophy and therefore wants to go back to another beginning by invoking the pre-Socratic thinkers, an attempt to discove? a new cosmotechnics, as I have claimed elsewhere.96 I believe that it is necessary to read Simondon with Heidegger here, since Simondon’s concept of genesis of technicity resonates with Heidegger’s proposal to overcome modern technology by reconstructing a different think- ing hence another beginning, and in this sense Simondon’s more technology-oriented approach complements Heidegger’s more culture- oriented program. Lyotard, in spite of his fierce critique of cybernetics, allows us to see the importance of the question of sensibility and how it constitutes the postmodern episteme, which may be strategically appro- priated to open society to new transformations. These two images of cybernetics have completely different social, economical, and political implications. The organicist epistemology, presenting a new paradigm shift of thought in the twentieth century, is naturalized in practice and it turns out to be nothing organicist but mechanical, like when we use a recursive machine to write a program printing out “Hello, World.” Control through tertiary retentions and protensions such as surveillance, social credits, and big data analysis is taking the first path, in which recursive machines are integrating individuals as the constituents of computation. What Deleuze calls the society of control is fully demon- strated in our digital epoch, of which digital control and flexibility (e.g., modulation or performativity) are its means. We may want to say that it is a mechanist use of organicist machines for deterministic use, which, as we wanted to show, is something that has to be reproached, and a broader historical and philosophical perspective opened up, as we have attempted throughout this book. However, let us raise the final question: Is it possible to take seriously the organismic philosophy and transform it into elements of an organology that would allow us to reevaluate actual technological development and leave its finality open? 
Organicism is still a philosophy of nature. General systems theory and second-order cybernetics have moved a step further, but in the twenty-first century, can we go even further toward elaborating an organological thinking, one that goes beyond the illusion of human beings as mere observers and machines as replacements for human beings? In order to do so we need to inscribe the cosmos organologi- cally, and this is what cybernetics didn’t do and this is at core of the thinking of cosmotechnics. Cybernetics in the Western tradition has 
already adopted its “modern cosmology,” namely, astrophysics: the end of the cosmos, as some historians have claimed.97 It is also in this sense that Heidegger sees the end of philosophy and the beginning of a world civilization based exclusively on Western thought. In Chinese cosmotechnics, the cosmos is organic insofar as it is analogical to the body. Chinese medicine is therefore very different from Greek medi- cine, even though they share certain similarities (for example, diagnosis according to pulses).98 The cosmos is an organ of principle, governing both the aesthetic and the moral. The heaven-earth that is the name for the cosmos is correlated with the human activities, while these rela- tions are real and maintained by “resonance.” Precisely because of this, Needham considers neo-Confucianism to be a veritable organic philosophy.99 It is also the reason that Mou Zhongsan, the great New Confucian of the twentieth century, characterizes Chinese philosophy as a moral metaphysics and moral cosmology.100 Standing against it is treating the cosmos as a mere resource—the eternal goal of the deter- ritorialization of capital. 
With the question of the moral we also come back to the question of episteme, which I reformulate as the question of sensibility, or, if you wish, a reterritorialization against determinism. The destruction of capitalism will happen not because it is surpassed by its technology, but because its cosmotechnology is fundamentally against the condi- tions of subsistence and existence. The epistemologies of capitalist technologies can be overcome only by different cosmotechnics that provide alternative epistemologies and maintain technodiversity and noodiversity. Or, put another way, the totalization of capitalism through more advanced means can be challenged by inventions and usages only according to different ontologies and epistemologies.101 Looking back at history, the Polynesian gift economy that inspired the work of Marcel Mauss and Georges Bataille has been haunting capitalism ever since, and continues in the anticapitalist thought of anthropologists like David Graeber, though modern science has since long rejected Hau and Mana. This sensibility of the world, of the relation between humans and the cosmos, is different from the modern view, but being at odds with mod- ern science is not an excuse not to develop a cosmotechnical thinking that will organologically inscribe science in its working principle. For a hundred years the absolutization of science has led to conflict, while the absolutization doesn’t mean that one is moving toward an end that is called the Absolute, since the Absolute is neither a thing nor a theory of a thing, but is precisely the unthinged (Unbedingt) of an epoch. If we follow Hegel’s analysis in the Vorlesungen über die Ästhetik that the absolute spirit passed through different stages from art in the ancient Greek time to religion and, arriving at the Enlightenment, philosophy, perhaps cybernetics is the current expression of the Absolute, as Gün- ther has analyzed.102 After Hegel’s verdict on the end of art, we continue to produce more and more artworks. Religions have survived even though they are not compatible with modern science. There are still many Christians, as there are many Buddhists. What sustains religion is not purely fanaticism, but rather faith, and it is in faith that we find the inhuman, as Lyotard found in Saint Augustine’s Confessions. Maybe after the end of the age of reason art will come back with new gestures and as new forms of resistance, which are beyond the linear history that Hegel has perceived. However, all these remain to be thought and explored beyond the Enlightenment humanism. If the end of European philosophy, according to Heidegger, means the need for new forms of thinking to surpass the challenging mode of unconcealment in modern technology, then these new forms of thinking must first render modern
technology contingent before elevating it to necessity. The fundamen- tal question is the regrounding of technology. We have to emphasize that this is not to add an ethics to AI or robotics, since we won’t be able to change the technological tendency by just adding more values. Instead we have to provide new frameworks for future technological developments so that a new geopolitics can emerge that is not based on an apocalyptic singularity but technodiversity; this is also the reason cosmotechnics is a political concept. 
What Needham tried to think through in his multiple volumes of work is the relation between ancient Chinese thought and modern Western science and technology. In other words, he wanted to render Chinese thought contemporary: contemporary not in the sense that Chi- nese thought has already anticipated and is more superior than modern Western science and technology (in the bad spirit of nationalism and ethnocentrism), but rather in the sense that Chinese thought may be use- ful for showing another way of thinking without being simply opposed to European thought.103 I hold the view that the contribution of a study of Chinese thought of technology in The Question Concerning Technol- ogy in China (and this is by no means limited to China, but has to be open to all cultures and civilizations) is not only the demonstration of a philosophy of the organism, which has been done by Needham, but rather a reopening of the concept of technics as multiple cosmotechnics and the future of technological imaginations. This will necessitate the rediscovery of the nonmodern epistemologies and the reinvention of epistemes through the regime of aesthetics as responses to the current crisis from the point of view of localities, or as what Augustin Ber- que calls recosmosizing [récosmiser]. Schiller’s aesthetic education remains important for us today, and it is all the more significant when we recognize it as a political and cultural project, but we can no longer respond to Schiller’s question with the same humanist approach, since future aesthetic education will be about inhumanity. Aesthetics is at the base of the episteme in the sense that it is local and constituted by its particular way of living and sensing, which are very often mistakenly considered as mere customs. When Whitehead claims that time and space are relational, he is proposing at the same time a new science and a new aesthetics.
We started our journey on recursivity and contingency by recon- structing a philosophy of nature in Schelling and organicism, and pass- ing to the realization of such philosophy in logic and cybernetics. We tried to suggest a new way to look into the relations between philosophy and technology, organism and machine. We want to supplement this with a cosmotechnical thinking that can be perceived only in systems of knowledge in which the alter-cosmologies remain effective and it is possible to reflect on both questions of epistemology and episteme. The question is not to simply demonize and undermine cybernetics as a mere governmentality, as it is now often conceived, but rather to conceive a new perspective of cybernetics by undermining the tendency of its totalizing and deterministic thinking. However, this is not exactly what Simondon called the open machine, since for Simondon the open machine is only a cybernetic machine possessing a margin of indeter- mination inscribed in its recursive structure and causality. In saying this we mean precisely to move beyond this image of the open machine by resituating technologies in their genesis, which means to resituate technologies in various cosmic realities. A true pluralism, which Meil- lassoux attempts to open up with his concept of contingency, cannot be sustained without technodiversity, and such technodiversity is always in conflict with the totalizing power of its mechanism, whether mechanical or organicist. If cybernetics is the end of philosophy, in the sense Hei- degger has attributed to it, and if recursivity becomes a “synonym” for process philosophy, then a post-European philosophy can be perceived only by reappropriating this cybernetic moment through different tech- nological thoughts.104 This is the trajectory that we attempted to sketch out in this book. While the questions that we raised still merit further responses, due to the limits of individual effort, such an attempt will remain a common task of philosophy. 


Ultimately with the integrated use of remote sensing, big
data analysis and Artificial Intelligence, DeepGreen can be
deployed to assess urban vulnerabilities and find specific
urban design solutions to achieve immediate and long-lasting
impact (Fig.3, 4). Through these case studies, the authors
speculate on the possibility for a general applicability of
the DeepGreen protocol, its scalability and possible
widespread adoption. This speculation provides a critical
assessment of the current paradigm of urban green planning
and the conceptual and practical significance of urban regreening in the current, crisis-ridden, post-anthropocentric, reality.
Training and testing GAN Physarum

cycleGAN algorithms belong to the class of machine learning
frameworks (Zhu et al. 2017). They involve an automatic
unsupervised training of image-to-image translation models
without paired examples. In this case, the domain of sources
images and target images refer to the slices of two actual
input-images that belong into two different domains, the
urban and the biological. cycleGAN algorithms are deployed at
level four of the overall DeepGreen workflow and the process
can be further divided in four phases for clearer
illustration.
The first phase describes the preparation of the input
images deployed to train GAN_Physarum. The second phase is
the training of the algorithm based on these input-datasets.
The model is trained to detect the urban morphology of case
study cities and the biological growth patterns of Physarum
Polycephalum. During training the algorithm typically runs
200 epochs, or iterations, over which it self-improves.
At this stage, the models are sufficient for generating
plau- sible slices in the target domain but are not
translations of the input slice. This occurs in the third
phase, the testing phase, during which the algorithmtranslates the bio-com- putational patterns onto the cases
study city.
In the fourth phase all resultant tiles are recombined
and the final output image is recreated. This can produce
true colour satellite visions of the case study, its urban
morphology and serval testing scenarios for the integration
of biotic and abiotic systems within the urban landscape.
The case of Paris

To illustrate this protocol in action the GAN-Physarum is sent
on a computational derive on the streets of Paris (Debord,
1956). While at first the visions conjured by GAN-Physarum have
the disorienting quality of the
non-human mind that conceived them, they also connect us with
contemporary Paris at a more fundamental systemic level. (Fig.57)
It is in fact known that contemporary cities share unique
emergent behaviours and self-organizing growth patterns with
organisms like Physarum Polycephalum (Tero et al. 2010).
Transport networks are ubiquitous in both urban and biological
systems. Robust network performance involves a complex trade-off
involving cost, transport efficiency, and fault tolerance.
Biological networks have been honed by many cycles of
evolutionary selection pressure and they develop without
centralized control.
This represents a readily scalable solution for growing
networks in general and as part of a complex urban system.
Therefore GAN_Physarum has the potential to evolve into a
resilient urban planning model.
When the trained GAN_Physarum is deployed it actua- lises
the capacities of Physarum Polyclephalum in solving problems
of urban re-metabolisation, carbon neutrality, energetic
self-sufficiency and increased biodiversity. Most
importantly these ambitious objectives are tackled within a
non-human conceptual framework therefore opening up a whole
new palette of potential design solutions in the urban realm.
GAN_Physarum, in other words, offers the prospect of avoiding some of the common pitfalls of biophilic
and biomimetic design protocols, namely the need for all too

human metaphorical interpretations, as in the case of “urban
forests”, or reductionist abstractions, as found in most
“green networks” proposals.
Following the typically distributed model of practicing
research in the global pandemic reality of 2020, the Parisian
derive of GAN-Physarum starts in the air condi- tioned
laboratories of the Synthetic Landscape Lab at University of
Innsbruck. Here at a constant room tempera- ture of 20
degrees a living Physarum Polycephalum in its active
plasmodium phase is made to hunt for nutrients within the
sterile environment of a lab grade borosilicate glass Petri
dish. Initially the growth substratum consists
of a 2% Agar with oat flakes as locally condensed food
sources.
The plasmodium’s typical networked body began spreading
within minutes. For better documentation the Petri dish is
positioned on a backlit surface and a camera is held above by
a tripod. The camera lens has a focal length of 50mm and the
resolution of the raw images is 4240 x 2832 pixels. One photo
is taken each 60 seconds for a total of 5190 images. The
typical experiment lasts for approximately 3 days and half.
The frames are then further edited by inverting the
original input image and then copy the original layer on top
with the blend mode colour filter. For further clarity of
detail a drop of blue food colouring is added on the agar
substratum.
Once combined into a video sequence these frames reveal the
extraordinary morphogenetic process during which Physarum
Polycephalum’s body transforms into an opti- mised network
for nutrients distribution. The process starts with a
searching phase during which the pulsating body branches out
in all direction detecting food sources and their relative
distribution and size. In this phase traces of actin are left
on the substratum, constituting a form
of embedded memory, Physarum Polycephalum’s own outsourced
brain.
What follows is a phase of optimisation. Finely detailed
branches emerge in the relevant areas of the Petri while
other parts are abandoned. Eventually some of the branches
grow in size and become thickening convoluted transportation
arteries. However the optimized configu- ration never
settles. As the resources are consumed and their overall
distribution changes in real-time, Physarum Polycephalum’s
morphology adjusts.

understood in this way, is planetary, by definition. The city/country
opposition is not resolved however - it clearly persists - rather, the
metropolis is a concept operating at another (global) level of abstraction.
There is a sense in which we might conclude that the metropolitan stands
in the same relation to city, as the ecological does to the country. This
does not however quite capture it though. Cunningham, in response to
Lefebvre’s ‘theoretical need’ to think about the urban, suggests that the
kind of trans-disciplinary ‘post-philosophy’ that can think the metropolis,
would necessarily share something of the pattern-form of the metropolis
itself. In fact, I wonder whether the kind of knowledge that a theoretical
In experimental academic architecture and over the past decade, the term “matter” has
achieved great popularity. Its meaning has definitely moved away from the neutral and
flat mold argued by Aristotle, acquiring in the last centuries and through the concepts
of “truthfulness” and “vitalism” some of the features traditionally ascribed to “form”.
The aim of this article is to argument that the current culmination of this process is
triggered by the technological condition of the Age of Antrobology and paradoxically
does not imply the radicalisation or re-interpretation of “matter”, but its replacement by
“form”. This reflection is particularly manifested in most of the contemporaneous
projects that revolve around the expression “Black Ecologies”. In this article it is
applied to three of its most popular representatives: BitMap Printing, Slow Furl, and
BIK house. The argument developed on them is twofold: on the one hand it is based
on the relevance that the notions of “information”, “performance” and “platform” have
acquired in certain sectors of experimental architecture, and on the other hand deepen
on the close relation that these three notions have with the concept of “form”.

The operating manual of the self-organizing city incorporates into the design and planning
of contemporary cities the bottom-up mechanisms found in nature as well as in the
functioning of rural villages and post-industrial cyber-communities.
Intense urbanization is arguably one of the defining characteristics of our society; as
capitalism reaches its terminal stage and the digital revolution shapes an instantaneous
global world, cities are replacing national states as the social and economic centres of our
new civilization. Spatial scales are transformed by the intensification of interscalar
communication, influencing the relationship between the personal and urban realms. As
Virilio points out:
Home shopping, working from home, online apartments and buildings: “cocooning”,
as they say. The urbanization of real space is thus being overtaken by this
urbanization of real time which is, at the end of the day, the urbanization of the actual
body of the city dweller, this citizenterminal.
Paul Virilio, Open Sky,
London: Verso, 1997, p. 20
Socially, urbanization is often perceived as an irresistible tendency, whereby the rural
population urbanizes to fulfil the dream of a more comfortable life and a more rewarding
career. However, with a more critical reading we find that migrations are often forced by
the lack of investment into rural regions and into their informal market, or by the
disappearance of the rural landscape itself. Such disappearance is both physical, as a
result of property development, but mostly mental or psychogeographical, as a result of
the emergence of a “real-time world city”, whose spatio-temporal pull reduces to zero the
perception of the journey across the nodes of the urban network:
Today, when we are all so worried about the ecological balance of a human
environment seriously threatened by industrial waste, would it not be appropriate to
add to the concerns of green ecology those of a grey ecology that would focus on the
postindustrial degradation of the depth of field of the terrestrial landscape?
Virilio, Open Sky, p. 41
This loss of “depth of field” is exacerbated by a correlated process, the emergence of a
new form of nomadic behaviour, whereby people leave their original settlements to search
for more fertile pastures – richer markets; the material loss is replaced by a gain in
economic value, a trading capacity that can grant this nomadic clan access to better

services, including training and education. As the new nomads start their journey they
inevitably break their symbiosis with their original land, losing their own mental
interpretation of it as a productive landscape or, as landscape designer and philosopher
Gilles Clement would define it, “a garden”:
If we look at the earth as a territory devoted to life, it would appear as an enclosed
space, delimited by the boundaries of living systems (the biosphere). In other words,
it would appear as a garden (the etymology of the word garden comes from the
German garten, the etymological root for which is enclosed or bounded space). …
This statement impels every human being, in his transient existence on earth to
commit to his responsibilities as guarantor of the living world that he has received for
management. And here he is becoming a gardener.
Gilles Clement, Il giardiniere planetario,
M ilan: 22Publishing, 2008, pp. 58–59,
translation by M arco Poletto
Their original material practices are slowly forgotten, converted from rural and largely
pre-industrial to urban and technologically mediated; they lose the ability to sustain their
own community and become dependent on external provisions, even when these are of a
lower quality. They stop contributing to the tending of our “planetary garden”.
The current phenomenon of global urbanization therefore coincides with the
weakening of the conception and perception of the landscape as a garden to nourish and to
be nourished by. It is not just physical erosion of the natural land by urban structures, but
a more fundamental loss of material practices and protocols and of their social meaning.
As the global capacity to read the landscape as a “garden” disappears so does the ability
to connect urban development with a sustainable relationship with the biosphere. From
this perspective the global ecological crisis is a problem of the culture of urbanization; of
loss of diversity of material practices; of transformation of the physical landscape in
relation to the social one.
The self-organizing city is, instead, a new form of emergent “real-time world city”: a
conceptual and operational model of urbanization for promoting the re-structuring of
endangered species of social, economic and environmental practices and organizations.
The self-organizing city is described in this book by means of an operational design
manual. Within each section dedicated protocols describe the assemblage of protoarchitectures, the incubation of proto-gardens and the coding of proto-interfaces; these
prototypes of machinic architecture materialize as synthetic hybrids embedded with
biological life (see proto-gardens, in “Environments” and “Behavioural Spaces”),
computational power, behavioural responsiveness (see cyber-gardens, in “Behavioural
Spaces”), spatial articulation (see ecoM achines and fibrous structures, in “M achines”),
remote sensing (see FUNclouds, in “Behavioural Spaces”) and communication capabilities
(see the Ecological Footprint Grotto, in “Behavioural Spaces”).
If urbanization strongly correlates to global ecological stress, it is within the context of

urban networks that we can effectively speak of ecological development: national states
are limited by both their physical boundaries and their internal bureaucracies, while many
international organizations lack operational focus. The emergent urban communities are
dynamic, fluid and far from equilibrium, ideal incubators of new cultural networks; they
connect the main academic and research institutions to the self-organized clusters of
creative development; they continuously develop new material practices or know-how by
testing novel technologies or by recycling traditions; they fabricate new mechanisms to
interact among themselves and with the surrounding environment.
By harvesting this inherent vitality the self-organizing city proposes a vision of
urbanity that has interaction and a narrative of productive know-how as its constitutional
protocols; as such the self-organizing city has no limits in either time or space, no
beginning and no end, no fixed and final configuration, no permanent dweller, no single
planner. Within the self-organizing city these forces generate diversity and cultivate
differentiation as a means to evolve true novelty, to create new forms of material life on
our planet.
Inhabiting the self-organizing city gives us the ability to play an active role in the
making of an open future, turning destruction and erasure into potential for new
originations, transformations and migrations; generative movements or trajectories of
escape that become seeds for new virtual plots, new proto-gardens and new ecoM achines.
The Urban Algorithm

In mathematics and computer science an algorithm is an effective method or procedure
expressed as a finite list of logically defined instructions for calculating a function.
Algorithms take in various inputs and after a series of transformations deliver an output;
the relationship between input and output may not be deterministic or linear and may
involve chance. We would argue here that the mathematical definition of algorithm needs
to be reframed to encompass the coding of the “real-time world city” and to become what
we will call an “urban algorithm”.
Algorithms are traditionally conceived as “powerful problem solving machines” and as
such have been used throughout history in various form. They require computation and
computational power to run; in the past material or analogue computation was used
extensively. Such a form of computation harvests the natural ability of materials to selforganize and “solve”, for instance, problems of structural stability by means of a process
of analogue form-finding; analogue computers were material assemblages able to multiply
the effects of material computation, thus achieving sufficient problem-solving
performance. Such computers were often large – in fact since antiquity many whole
buildings have been conceived as architectural computing machines, a well-known example
being the so-called “Nilometers” in Egypt. These beautifully ornamental structures were
in fact large graduated tanks that, once connected by a complex network of pipelines to
the river Nile, were able to measure the extent of each year’s flood. This was then used to
calculate the taxation rate to be imposed on the local farmers benefitting from the flooding.
In good years, large flooding would make the ground more fertile and higher taxes could be
imposed. In this example the river Nile’s topography, its network of infrastructural
canals, the cultivated landscape and the Nilometers operated together as a large analogue
computer, embedded within the material substratum of the “garden” that it was designed
to monitor and help to cultivate.
The digital revolution has dramatically increased the power versus size ratio of analogue
computation; a computer as complex and large as the Nilometer system can now be
reasonably well simulated by a 2 kg laptop processor. This progression has disconnected
material computation from the fabric of our built environment and of our urban landscape;
the architecture of computation is now beyond the scales of architectural and urban
design. The discipline of architectural composition and urban design has evolved
independently from any practice of analogue computation or, in other words, of material
organization. So even if it’s becoming possible to simulate complex organisms like our
cities through the application of specifically customized algorithms, the main obstacle to
their effective application to the design of our urban environment has been the need to
overcome the conceptual difficulty of expressing urban and architectural design problems

in the form of algorithmically solvable questions; or, in other words, to embed these
algorithms in a coherent form of material organization.
Despite the rising use of algorithmic processes in design it is still necessary to
overcome this conceptual shortcoming: a first step might be the expansion of the concept
of algorithms as mathematical entities with strong disciplinary connotations and an almost
exclusive applicability to the logical formulations typical of the scientific and engineering
domains.
French philosopher Gilles Deleuze offers us some support here by providing an
extended meaning of the term “machine” beyond the mechanical paradigm and towards a
larger “machinic” framework. Such conceptual reframing can be adopted to expand the
notion of algorithms as mere mathematical machines. For Deleuze the notion of machine
exceeds the technical realm and includes abstract generative mechanisms that can be
recognized in any organic or inorganic being and that operate across multiple realms and at
multiple scales:
It makes no sense to think that an organism stands a chance of survival independently
of the survival of its milieu; the milieu is a precondition for the organism’s
development; … If we frame the organism plus milieu as a unit as Bateson suggests,
than it is impossible to define it neatly as having a clear form, or limit.
Andrew Ballantyne, Deleuze and Guattari,
Oxford: Routledge, 2007, p. 85
A simple example used repeatedly by Deleuze and Guattari is the relationship between
the wasp orchid and the Thynnie wasp. The orchid flower has evolved parts that
resemble very closely the female wasp; the seduced wasp male tries to mate with the
flower and by doing so it pollinates the plant; the two have evolved so inseparably that
even their appearance has become similar. Despite being an insect and a plant, the wasp is
inherently part of the orchid so that it becomes very hard to draw a frame around its
identity; however resisting doing so allows us to conceptualize the pair as a larger
ensemble or machine, and their coupling as the process of reproduction of that machine.
In a similar fashion human beings can be defined as an assemblage of what Deleuze calls
“desiring machines”, thousands of mechanisms that without us noticing produce the
desires that we are aware of. A similar machinic framework can be recognized in the
formation and evolution of inorganic assemblages such as dunes and deserts: millions of
sorting mechanisms create coherent patterns of sand distribution that travel in space and
time until final dissolution.
The most important aspect of Deleuze’s definition of machines, for our discourse, is
that they cannot exist outside their “milieu”, or environment; in other words, they are
inextricably embedded in the environment within which they perform or are conceived:
exactly like an animal and its habitat, or the Nilometer within the river Nile region, they
form a “unit of survival”. Following this line of thought we propose an evolved notion of
algorithm, referred to as the “urban algorithm”, a computational machine whose definition

and evolution is inextricably embedded into its “milieu”.
Two pioneering applications of an expanded notion of an algorithmic machine are
provided by engineer Le Ricolais, working with models of structural analysis and
behavioural simulations of structures observed during collapse, and by cybernetician
Gordon Pask, who famously developed a series of artefacts, conceived in conversation
with the surrounding environment as well as with the users, as testing beds for his
theories on second order cybernetics:
Le Ricolais suggests that matter, material, construction systems, structural
configurations, space, and place comprise a continuous spectrum rather than isolated
domains. Such an understanding provides a model for organizing forces and their
effects that is communicative, reverberating across scales and regimes.
Reiser + Umemoto, Atlas of Novel Tectonics,
New York: Princeton Architectural Press, 2006,
p. 110
It seems to me that the notion of machine that was current in the course of the Industrial
Revolution – and which we might have inherited – is a notion, essentially, of a machine
without goal, it had no goal “of”, it had a goal “for”. And this gradually developed into the
notion of machines with goals “of”, like Thermostats. Now we’ve got the notion of a
machine with an underspecified goal, the system that evolves. This is a new notion,
nothing like the notion of machines that was current in the Industrial Revolution,
absolutely nothing like it. It is, if you like, a much more biological notion; maybe I’m
wrong to call such a thing a machine; I gave that label to it because I like to realize things
as artefacts, but you might not call the system a machine, you might call it something else.
Gordon Pask quoted in M ary Catherine Bateson,
Our Own Metaphor: A Personal Account of a Conference on the Effects of
Conscious Purpose on Human Adaptation,
New York: Alfred A. Knopf, 1972
It is along these lines that the definition of algorithm inherited from mathematics and
computer science can be re-defined to formulate the “urban algorithm” as a computational
design protocol embedded in its extended milieu.
Coding as Gardening

“Urban algorithms”, as defined in the previous short chapter, possess a rather peculiar
nature which makes coding specific to a certain milieu, be it the ecology of a landscape or
an architectural material system. This peculiarity can be exemplified by comparing the
methods of a chemist synthesizing artificial tissues in a lab with those of a gardener
reviving a patch of dried land; while both are running generative protocols, the first
requires a perfectly controlled and refined testing ground for her procedures to acquire
general applicability while the second needs to consider the unexpected fluctuation of the
ecology of his garden.
The chemist, within the controlled setting of the lab, can trigger a series of reactions and
morpho-genetic processes and slowly grow coherent and therefore potentially functioning
tissues; if all the variables are well controlled and managed the final outcome can
reproduce with enough precision the results predicted by early computer simulations of
specific scenarios of development; a little unexpected mistake or variation in the testing
bed and the final results would become unpredictable, incoherent and often lose their
applicability. The gardener, instead, operates within a very different framework; his
testing bed is by definition very differentiated and partially unpredictable in its daily or
seasonal fluctuations; his operational protocols need to consider these fluctuations and
differences in their formulas. As Gilles Clement points out in his beautiful description of
the “moving garden”, the gardener operates through a process of intensification of
difference; his only chance to reconcile his desire for beautification and the natural
expressivity of living processes resides in movement, intended in its biological and
physical sense. The formalization of the garden becomes for Clement a process of
formalized transmission of biological messages or, in our terms, of algorithmic coding;
algorithms are for the gardener machines for breeding biodiversity.
Differences in slope, insulation, soil moisture and so on are registered and then
exploited by the algorithm to promote the growth of different arboreal species; also the
growth, being itself a variable and partially unpredictable process, needs to be read,
assessed and then considered in the formulation of future actions, or in the future lines of
the gardening code. The garden grows and beautification progresses in loops; each step
generating more difference and local complexity that can be in turn recognized and bred;
the management of this generative process is what makes the garden a potentially
beautiful and healthy organism. In Clement’s words:
Reality is entirely contained within experience. Uniquely. Without gardening there is
no garden.
Gilles Clement, Il giardiniere planetario,

M ilan: 22Publishing, 2008, p. 66,
translation by M arco Poletto
Coding here is based on experience and “know how”; with no gardening there is no
garden. The gardening code is embedded in its milieu.
In algorithmic design both models of development, the laboratory and the garden, can be
used. We favour the second, which we have been defining as the “urban algorithm”, where
coding becomes a form of “gardening”. The reasons to prefer this model are multiple –
Clement suggests an eco-political one:
How can we oppose the brutality of these so-called modern techniques, with a
handling [of the earth’s biosphere] that is sensitive, diverse and therefore really
contemporary? By bending the technological know-how to planetary ecological needs
rather than, as we can still observe in all the rich countries, enslaving it for the
exclusive benefit of the lobbies.
Clement, Il giardiniere planetario, p.71
The urban algorithm responds to the systemic and ecological sensibility of our times as
opposed to the mechanical productive logic of the twentieth century; today we seek to
cherish and cultivate our biosphere, to understand how we can develop it or transform it
in tune with its own vital cycles and the generative processes of life. Just like an “urban
algorithm”, life proliferates by breeding potentials of intensive differences; it is constantly
opportunistic and open to the actualization of new material organizations. We seek to
intensify these mechanisms while rescuing them from the conforming force of modern
technology. In his book The Three Ecologies Félix Guattari suggests related technological
and social reasons to adopt the machinic model:
So, wherever we turn, there is the same nagging paradox: on the one hand, the
continuous development of new techno-scientific means to potentially resolve the
dominant ecological issues, … on the other hand the inability of organized social
forces and constituted subjective formations to take hold of these resources in order to
make them work.
Félix Guattari, The Three Ecologies,
New York: Continuum, 2008 (originally published in France, 1989), p. 22
M achinic protocols embed technology into material organizations that become part of
everyday ecological practices of planetary cultivation of the biosphere, operating at
multiple interconnected scales. Unlike the dominant technological protocols, the “urban
algorithms” do not need the production of problems, or catastrophes, to be able to
fabricate meaningful solutions; they operate on the self-organization of social forces by
means of technological augmentation and their usefulness emerges out of daily material
experimentation within the milieu of the “real-time world city”; in relation to this

technological paradox Guattari adds:
Social ecosophy will consist in developing specific practices that will modify and
reinvent the ways in which we live as couples or in the family, in an urban context or
at work, etc. … Instead of clinging to general recommendations we would be
implementing effective practices of experimentation, as much on a micro social level as
on a larger institutional scale.
Guattari, The Three Ecologies, p. 24
The successful survival of the “real-time world city” requires participation and
exchange at the various social levels and material scales; a code that incorporates
participation must be able to grow as the network grows, it cannot be defined a priori in a
controlled or predetermined environment. “Urban algorithms” co-evolve within their
milieu, the articulation of their structure increases in relation to the complexity and
diversity of the urban network they serve. “Urban algorithms” are the necessary coding
logics for the self-organizing city.

Algorithmic Diversecity

What are, then, the key features of the “urban algorithm” and how does it encode the selforganizing city? First of all, as mentioned above, in order to become a systemic design
instrument the “urban algorithm” must be embedded in its milieu; its set of instructions
cannot be generic but need to be referred specifically to the “algorithmic plan”. In the
design phase of the self-organizing city we propose the algorithmic plan to be a
parametric relational model diagrammatically representing the output of the algorithmic
simulation; as Ciro Najle has suggested in his manual for the machinic landscape, the plan
will constitute a pre-urban medium:
a technically controlled sieve that acquires consistency as it integrates a multiplicity of
determinations in a medium of production, virtualizing potentials by constantly
oscillating between management of information, programming of responses, generation
of organizations, evaluation of performance, coordination of collaborations, scripting
of protocols, coding of communication, engineering of materials, modulation of
expression and fine-tuning of inflections.
Ciro Najle, Landscape Urbanism,
London: AA Publications, 2003, p. 39
Running a new line of code will generate a new pre-urban algorithmic plan; any
following lines of code must operate in the plan generated by the previous one and
recognize the potentials it has produced. As the code evolves the plan differentiates and
generates novel potentials or breeding grounds; the recognition of such potentials
generates in turn new specific lines of code, thus evolving the algorithmic machine as a
whole. As a consequence the “urban algorithm” cannot be defined a priori, or outside of
the algorithmic plan.
The solutions of each line of code influence the formulation of the following line; this
process has the inherent tendency to branch, that is, to generate multiple solutions; the
“urban algorithm” is therefore generative by definition; each branch leads to increased
internal diversity and articulation. As a result, a second feature of the “urban algorithm” is
nonlinearity; the algorithm doesn’t treat growth as a linear chronologic sequence, rather it
establishes horizontal connections in the plan, it relates circumstances, nodes of
attraction, organizations of power promoting growth as a nomadic propagation; as a
consequence its ramifying structure will not resemble the centralized branching of a tree
but rather, as Deleuze suggests, a rhizome.
The growth of the algorithm is related to the growth of a proto-representation of the
self-organizing city and is defined by its articulation and differentiation. Growth is

therefore not merely a linear process of geographical expansion but rather one of
complexification of its network of internal links; a good example is the process of
evolution of a forest. Organisms like forests are extremely diverse, or bio-diverse; this
biodiversity has been generated in thousands of years of constant differentiation; all the
species that inhabit forests have adapted and in turn have evolved into lineages composed
of many instances; the algorithm of the forest must include all the algorithms that regulate
each species within it; as such it is extremely articulated and diverse.
Biodiversity can also decrease, as is often the case when human action intervenes or as
an effect of globalizing forces; as such cities, like forests, can shrink, perhaps not so much
in terms of physical boundaries but in terms of bio-social-diversity and internal
articulation. The case of post-industrial Detroit is emblematic, with its inner plots
reverting to countryside or abandoned wasteland. Artist Kyong Park has re-inhabited this
post-industrial terrain through a choreography of signs and messages able to re-code the
emergent gaps as part of a new network of urban proto-spaces. His experimental practice
has, in turn, encountered and perhaps even stimulated the vitality of the local suburban
dwellers, now turned into urban gardeners of a form of network agriculture:
Artistic behavior, then, once considered characteristic of small, excluded, marginalized
and unpredictable social groups once viewed as foreign to the business world, has now
gained a new centrality in the social economy. … This is then a “buzz economy” that
follows discontinuous flows of an energy that is more relational than productive.
Andrea Branzi, Weak and Diffuse Modernity,
M ilan: Skira, 2006, pp. 39, 50
Such socio-economic models require urban designers to map emergent network spaces
in real time and to promote urban transformation through the re-coding of urban gaps. In
this circumstance the “urban algorithm” enables them to operate through a protorepresentation of the city, turning the algorithmic plan into a device of urban incubation
able to map the potential for social interaction and to promote the emergence of urban life.
Another key feature of the “urban algorithm” is therefore its ability to deploy technology
as a means to activate the vital forces of the self-organizing city:
The metropolis of the information age is not, then, the capital of technology; it is
rather the land of the humane, in all its ability to connect its own DNA with that of
business, disseminating its own genes in a tight network of parental and
entrepreneurial relations.
Branzi, Weak and Diffuse Modernity, p. 24
As we said before, each “urban algorithm” is unique; it belongs to its milieu and from
this specific interaction, it grows articulation as well as its specific traits and qualities.
This makes it very different from a classic optimization algorithm whose applicability is
generic; the mode of operation of the “urban algorithm” is local, it operates by exploiting

local difference or constraints; it turns constraints into generative opportunities for the
specification of new infrastructures and urban spaces:
Every little dissimilarity is an event, a useful landmark for the construction of a mental
map composed of points (particular places), lines (paths) and surfaces (homogeneous
territories) that are transformed over time. The ability to know how to see in the void
of the places and therefore to know how to name these places was learned in the
millennia preceding the birth of nomadism.
Francesco Careri, Walkscapes,
Barcelona: GG, 2002, p. 42
A city will never emerge in a neutral and controlled environment such as that of the lab
or the virtual space of a computer; existing complexities, topographic irregularities,
political boundaries or social lines of conflict are always present. Traditional algorithms
have been applied in multiple site conditions by considering constraints to be external
interferences; as such constraints are often problematic and non-generative and the
temptation is to try to remove them. “Urban algorithms” instead grow out of a productive
manipulation of these constraints; they use these differences to breed new diversity and
to slowly produce higher levels of efficiency in the milieu. This process has an inherent
economy of means as it works with the local conditions rather than against them; this
recalls the metaphor of the gardener, and his ability to promote the colonization of the
garden by various animals and insects as part of the process of beautification.
Negotiations, rather than weakening the algorithm’s efficiency, actually contribute to its
evolution and to the richness and beauty of the algorithmic plan:
Architecture would thus become part of a wider ranging activity, and like the other
arts would disappear in favor of a unified activity that sees the urban environment as a
relational ground for a game of participation.
Careri, Walkscapes, p. 116
The urban environment, its socio-cultural-economic context and the “urban algorithm”
itself are interconnected parts of the same machinic assemblage. This relationship
materializes in a multitude of architectural and urban prototypes; the “urban algorithm” is
the working protocol of such a design apparatus as described in the algorithmic plan.
Eventually, design intentions, technological media as well as local constraints will coevolve and become inextricably part of the urban machine:
Public spaces must have a prototypical character; they are instruments of change for a
society. … [it only gradually became] clear that in fact this combination between form
– specially diagrammatic form – and the operational mechanism of a prototype
together is the link between architectural space and urban dynamics.
Raoul Bunschoten, Public Spaces,

London: Black Dog, 2002, p. 5
The “urban algorithm” as well as the city is never complete or finished; the design,
construction and evolution of a city can be all conceived algorithmically as a continuum.
The regulatory body of the city and its inhabitants become part of the design apparatus as
they begin to influence the coding of the design algorithm generating the self-organizing
city.

Architecture as Systemic Design Practice

A Deleuzian (extraordinary) diagram is an abstract machine that is valued precisely
because its downstream implications are totally open. The crucial difference between
ordinary and extraordinary diagrams does not reside within the graphic or digital object
itself, but in the patterns of its use.
Patrik Schumacher, The Autopoiesis of Architecture, London: Wiley, 2010, p.
350
One of the transformations that took place in architecture in the 1990s is the
abandonment of the ideology of form and the advance towards establishing what we could
call “the ideology of the process”, based on the exploration of potential patterns of use of
new diagrammatic representations supported by emerging digital design technologies.
Ideology here is not intended as political but refers instead to the infrastructure of a
general knowledge regime which forms the base for a professional practice. Before the
introduction of algorithmic design processes it was common to define architecture as a line
of autonomous practices, related one to the other in linear fashion, creating a sort of
hierarchy.
The architectural system of production was based on the illusion that it was the
determiner of the final form of what was imagined during the expressive phase of activity
within the studio of the lead architect. As work was carried out with a repertory of forms
totally dependent on Euclidean geometry, it was easy to believe this illusion. Each
mathematical expression represented a single constant form that, when represented in
design, was easily associated with the real form of the final product.
Algorithmic design demonstrates the illusory nature of this association. One of its key
initiators, Gregg Lynn’s work on animate forms can be considered the first manifesto of a
new knowledge regime of production which declared the end of the era of predictable
forms that started with the Renaissance. As Schumacher argues:
The limits of our design language are the limits of our design thinking. The medium of
representation delimits the domain of architecture and implicitly defines what
architecture is. How we represent architecture determines how we anticipate (design)
architecture.
Schumacher, The Autopoiesis of Architecture, p. 330
Algorithmic design techniques allow us to conceive form as an emergent effect in a
design process; the initial design actions or sketches merely define starting points within a
process of creation or production and cannot determine what the final result will be like.

What emerges from this are architectural practices implemented by the continuous going
back and forth within the process, which we call “systemic design practices”. The old
hierarchy of expertise is now being replaced by a new, possibly more democratic,
production process where the efficiency of the design solution is built up through
revisions which consequently incorporate intelligence and performance. This is a process
where the initial decision of form can be repeatedly re-described by another decision taken
on another level of the urban algorithm; it is a process where all practices are the
indivisible parts of an emergent whole:
The truly innovative architects’ designs swarm architecture for an open source in real
time. Building components are potential senders and receivers of information in real
time … People communicate. Buildings communicate. People communicate with
people. People communicate with buildings. Buildings communicate with buildings.
Building components communicate with other building components; all are members
of the swarm, members of the hive.
Kas Oosterhuis, Hyperbodies, Basel: Birkhäuser, 2003, pp. 5–6
While complex algorithmic procedures can be developed in order to predict a final form,
this is not vital for their existence; they are virtual multiplicities held by a mathematical
formula with infinite possible geometric solutions that can be made to evolve and interact
within the milieu and in real time. Form is then the product of an algorithmic procedure of
material organization applied in a specific space and time:
M ies’s constraint of matter by ideal geometry is based on an essentialist notion: that
matter is formless and geometry regulates it. … When freed from such essentializing
conception, matter proves to have its own capacities of self-organization. As an
analogue computer, it can perform optimizing computations that have been shown to
be trans-scalar; … it becomes a model not only for dealing with structure but for
dealing with the feedback that occurs between multiple forces at work on a building.
Reiser + Umemoto, Atlas of Novel Tectonics, New York: Princeton
Architectural Press, 2006, p. 88
Geometry does not domesticate material; rather it operates as a tympanum against
which material properties resound. As novel spatial effects emerge, so structural
behaviour evolves as a consequence of this dynamic exchange of intensive and extensive
information; structural design logics are then also transformed. Empirical calculations
representing a building’s structural behaviour on a two-dimensional plane, depicted within
the context of formulas which are the universally valid results of previous
experimentation, are no longer relevant. Systemic design practices do not operate within
such a strict polarity; there is no such yes/no solution; a particular structural behaviour is
always modelled in real time, defining an area of efficient possibilities within an almost
chaotic whole, generating design solutions which evolve iteration after iteration:

The sedentary space is striated by walls, enclosures and routes between the
enclosures, while the nomadic space is smooth, marked only by strokes that are erased
or shift with the journey. … The nomadic city is the path itself, the most notable sign
in the void … The points of departure and arrival are less important, while the space
in between is the space of going, the very essence of nomadism, the place in which to
celebrate the everyday ritual of eternal wandering.
Francesco Careri, Walkscapes, Barcelona: GG, 2002, p. 38
Systemic design practices suggest a different spatial logic exemplified by the smooth
space of the Fibrous Room; this architectural apparatus does not prescribe form, rather it
describes an informational protocol, an experimental “derive” in the field of architectural
prototyping, concrete structural design and material ornamentation guided by strictly
defined rules of information. The experiment works as a sort of training session in material
disorientation; how wide a series of relevant possibilities can we create in the context of
designing, calculating and building a concrete structural prototype? Certainly there is no
guarantee that all these possibilities will become useful in the future; all the same it is
worth trying, because what creates the future is the experiment itself, the practice of
experimentation which has no definitive end:
So that is another rule for the whole nature of architecture: it must create new
appetites, new hungers – not solve problems, architecture is too slow to solve
problems.
Cedric Price, Re: CP, ed. by Hans-Ulrich Obrist, Basel: Birkhäuser, 2003, p.
57

Discussions on Systemic Architecture
with
Patrik Schumacher
Michael Batty and Andrew Hudson-Smith
Michael Weinstock

On systemic architecture
The liquid city
Ecology and material culture

On Systemic Architecture
Claudia Pasquero (CP) and M arco Poletto (M P) in conversation with Patrik Schumacher
(PS)
CP Ecological processes always include a component of unpredictability; do you think
architecture can be conceived as an experimental practice able to engage
unpredictability as a generative/creative force?
PS Yes! The use of non-linear, unpredictable processes delivers creativity. One might
even define creativity in terms of unpredictability. Radical innovation depends on
radical newness as one of its conditions. The task is to go beyond familiar forms of
spatial organization, to expand the search space for viable solutions. However, rather
than just harnessing pure randomness (subject to intuitive selection and postrationalization) one might try to simulate processes of self-organization that embody
a certain performative rationality, via constraints or even via (relative) optimization.
What I said so far pertains to the design process in advance of construction. A
master-plan, by contrast, often evolves in parallel to phased construction. A
“master-plan” today is no longer a master-plan but only viable as a temporary
hypothesis. As a parametric system it might offer a range of solution possibilities on
a number of dimensions. The market can choose. If the ranges offered are insufficient,
the plan must be recalibrated or even radically expanded to adapt to developments
that were not anticipated. The plan can evolve without losing its identity only if key
principles and criteria have been stated that guide the plan’s adaptation. It is in this
way that I would like to interpret your concept of the self-organizing city made up
of “virtual plots”. Beyond this we can still rely on the heuristics of Parametricism to
guarantee ongoing continuity within an evolving plan that continues to adapt to
radical shifts in market conditions and shifts in political impositions. Thus although
not predictable in its detailed form, the urban future can be expected to follow a
certain paradigm. Within the paradigm of Parametricism local unpredictability can be
creatively harnessed or absorbed while maintaining a global frame of principles. We
should not think that we could invent an urban system that could automatically, on
its own account, react to and resolve such unpredictable contingencies. This can only
be the result of the reflective application of a historically pertinent heuristics, i.e. via
creative work within a historically pertinent style or design research programme.
M P The foundation of cities was often a response to opportunities found in the
landscape and in the local ecological systems; do you think the notion of the
contemporary metropolis should be re-evaluated in systemic terms by including all
the relevant global systems that are co-defining its actual state?

PS You talk about the contemporary metropolis as eco-social landscape. That’s
pertinent. However, the architect has certainly no control over all systems that codefine the actual state of the contemporary metropolis. He cannot even evaluate
these systems, far less re-evaluate them. Within functionally differentiated society all
societal subsystems – the economy, the political system, the mass media,
engineering/science, architecture etc. – evaluate and regulate themselves according to
their own unique criteria of success. These systems co-evolve. They observe and
irritate each other, as demands and constraints, and then adapt to each other. They
depend on each other and must serve each other without being able to control each
other. I think you point to this when you define urban space as “the product of
processes of co-evolution of multiple agents behaving as a coherent assemblage”.
Architecture has the universal, exclusive competency and responsibility for the
adaptive innovation of the built environment as ordering frame and interface for
social interactions of all kinds: economic, political, recreational, etc.
The engineered infrastructures, traffic, land values, investment opportunities and
other economic parameters are so many constraints and/or demands upon
architecture’s adaptive, organizational and articulatory repertoire and intelligence. In
this sense architecture must take account of all systems that co-define the actual
state of the contemporary metropolis, but it cannot re-evaluate or re-define these.
That’s not within architecture’s competency.
M P Do you think ecologic problem solving can define a contemporary architectural
agenda?
PS The ecological challenge is confronting all subsystems of global society, the
economy, the political system, science/engineering and also architecture/design. So,
ecological sustainability is on the agenda of architecture. Initially, the demand for
environmental sustainability is just one more constraint that burdens architecture’s
ability to deliver on its societal task: the framing of social interaction/communication.
However, Parametricism is able to take this constraint and turn it into an
architectural opportunity by utilizing environmental parameters as occasions to
differentiate envelopes accordingly, on the basis of sun exposure, wind, rain, etc.
Environmentally adaptive differentiation can become an orienting articulation. In this
way an engineering constraint is transformed into an architectural pursuit.
CP Simulating the city is the new mantra of urban design; is it an illusion or is it actually
possible to simulate the behaviour of a city? What can we understand from urban
behavioural simulations? Can urban forecasting influence or even stimulate a new
form of emergent, real-time urban design model?
PS I think our capacity to develop sophisticated models that are able to take on and
simulate more and more aspects and factors of the urban process is increasing by the
day. However, there is indeed an inherent dilemma and limitation in all modelling and
forecasting. The very fact that the model/forecast is available and communicated

becomes a new, potentially important factor that immediately changes the situation.
The forecast thus defeats itself as relevant actors adapt their expectations and
behaviours in response to the forecast. But this defeats the forecast only as forecast.
If we understand it as a platform of constructing emergent collectives it can work.
Then it’s not about prediction but about construction. One of our teams at DRL
constructed such a scenario of real-time online participa-tory
planning/developing/selling with regards to a residential community.1 You also talk
about “ecologic feedback, participation and social self-organization”. You note that
“urban self-organization requires the definition of an operational medium that
generates responses out of urban stimuli”. Perhaps it is the new social networking
media that can be harnessed here somehow.
M P Should architecture reconsider its traditional Vitruvian canons to redefine its
materiality in relation to the flows of information, matter and energy crossing its
boundaries?
PS Architecture has already shifted its canons a number of times since Vitruvius. The
last great shift was the shift from Historicism/Eclecticism to M odernism, and now
we are finally moving beyond M odernism after 25 years of experimental explorations
plus 10 years of cumulative design research under the auspices of the new paradigm
of Parametricism. The flows of information, matter and energy are more complex and
dynamic. What is more important though is the increasing density, complexity and
intensity of social communication and interaction. This is what ultimately drives and
justifies the new paradigm, whether this is always fully understood or not.
M P Our society is obsessed with control; can architecture mature a critical role by
deploying novel design techniques and computational sensibilities to challenge this
contemporary obsession of our society?
PS Yes, there seems to be an obsession with control. However, this does not lead to an
increase in control. Rather I see this obsession as a rear-guard reaction to selforganizing societal dynamics which are impossible to control. As Luhmann has noted
contemporary society is a world society with no control centre. It is a society of
global, co-evolving subsystems. The computational techniques of contemporary
architecture – generative and genetic algorithms, agent-based modelling, etc. – are
congenial to a world that can no longer be controlled or predicted. This world invites
everybody to co-evolve within the evolution of society. Architecture joins this
dynamic co-evolution.
CP The style of Parametricism vs. the non-figurative architecture of Andrea Branzi,
Cedric Price, John Frazer, etc. Are these two discourses, as it appears, mutually
exclusive or should reconciliation between the stylistic and the machinic/systemic be
an ambition for the future?

PS Yes, I think that Parametricism will have to absorb the innovations proposed by the
strand of the avant-garde you allude to. I would also count Bernard Tschumi and
Rem Koolhaas in this group. They are all concerned with charting and advancing the
programmatic/ software side of the built environment. Formal and programmatic
research needs to converge and be synthesized within a new, comprehensive
paradigm. Parametricism as it currently presents itself empirically has not yet risen
to this challenge. However, both my general theory of architecture’s autopoiesis and
my special theory of Parametricism accommodate this requirement as my theory
moves from the descriptive into the normative mode. M y normative reconstruction
of the concept of style and of Parametricism demands and formulates both a formal
and a functional heuristics as definitory requirements of a mature style/
Parametricism. So, what you call the machinic/ systemic, as well as the
programmatic/ functional, as well as the formal/aesthetic is included in my enhanced
concept of style. The concept of style deserves to be reinstated on this
comprehensive, ambitious level because it has a history of giving the defining stamp
on an epoch. It’s not only about appearances, it’s much deeper than that. However,
we should also be aware that the appearance of the built environment matters
enormously. The built environment functions through its appearance, via its
legibility and its capacity to frame and prime communication. The built environment
is not just channelling bodies. It is orienting sentient, socialized beings who must
comprehend and navigate ever more complex urban scenes.
CP The more urbanized our society becomes, the more technologically mediated it will
need to be; technology is the necessary instrument in developing a more effective and
sustainable relationship with the biosphere. However a very small effort is devoted
to the spatial and material integration of such innovations within the fabric of our
cities and landscapes. Should architecture claim a new role in the evolution of this
urban machine? Should architects participate in the development of new technologies
by evolving their spatial, temporal and material framework?
PS Yes, architects are in charge of the overall organization of the built environment, in as
much as it is interfacing with social communication. What is underground or under
the hood is engineering business. But as Piano + Rogers have demonstrated with their
Centre Pompidou, architects might bring the technological systems into view and let
them become part of the task of articulation. Tom Wiscombe has picked this idea up
within the style Parametricism. I think this is a fascinating proposition. To reveal the
technological systems and networks can play an orienting role that enhances the
legibility of our built environment. If you want to find the big auditorium, just follow
the biggest ductwork. The same attitude that allows architects to opportunize on the
differentiated structural systems for the differentiated articulation of space can be
applied to the mechanical systems as well as to the passive environmental systems.
Close collaboration with engineers allows architects to harness the articulatory
potential of material technical systems. That’s what has always been called
tectonics. This could also be scaled up into an urban tectonics. In this way the city

fabrics you envision to include a lot of environmental technology can become an
architectural project.
M P M achines have been always part of the history of architecture; however recently the
“machinic” in architecture has acquired a new meaning, expanding the mechanical
significance it has acquired during the Industrial Revolution. What relevance do you
think the notion of the machine can acquire in redefining architecture and its
relationship with technology?
PS There are a number of avenues to consider here. “M achinic” sometimes means
nothing other than avoiding preconceived ideas by harnessing blind material or
computational processes. This is certainly a powerful heuristic. I also like your
insistence on a new conception of materiality where the matter of architecture is
freed from the essentialist conception that considers it as a formless entity regulated
by transcendental geometric rules, and it becomes an active, generative force instead.
I also would subscribe to your concept of the ecoM achine.
Concerning machines proper: on the side of production/fabrication a new era is
dawning that requires design research. Here the AADRL a.o. has taken the lead with
the attempt to seed design projects with the invention of fabrication machines and
processes that allow for the fabrication of complex geometries without the aid of
moulds. The idea here is to harness the self-computing, form-finding capacity of
material systems. This paradigm was pioneered by Frei Otto and lies beyond the
mechanical paradigm. It’s part of a new paradigm of material self-organization. The
second great arena of post-mechanical technology within architecture is the
investigation of electronically augmented, responsive environments that respond to
both environmental variables as well as to social occupation and event parameters.
Again the AADRL a.o. has moved into this arena with its three-year agenda
Responsive Environments.
CP Is the communication of architecture evolving from a figurative narrative to a more
direct immersive and interactive language of smart materials and embedded
electronics?
PS I think the potential of embedded electronics (responsive environments) is exciting.
However, it provides augmentation rather than substitution. That’s the way we
treated it at the AA Design Research Laboratory (AADRL). Complex spatial
configurations and complex geometries were the premise of our responsive
environments. (This made the introduction of kinetic capabilities much more
difficult.) The variable, transformative parts of the built environment will always be
the smaller part. The fixed figurations remain decisive. What we were interested in
was to achieve strong reconfigurations – Gestalt-catastrophes – with a minimal
amount of kinetic investment. Thus we had to build in perceptual ambiguity/latency.
I termed this ambition “parametric figuration”. For me this is one of the more exciting
research agendas within the overarching paradigm of Parametricism. It introduces

observer parameters in addition to object parameters into the parametric set up.
1

Patrik Schumacher, “Autopoiesis of a Residential Community”, in Brett Steele and
RAM TV (eds), Negotiate My Boundary: Mass-customisation and Responsive
Environments, London: AA Publications, 2002.

Patrik S chumacher is Partner at Zaha Hadid Architects and Founding
Director at the AA Design Research Laboratory. Schumacher studied
philosophy and architecture in Bonn, London and Stuttgart, where he
received his Diploma in architecture in 1990. In 1999 he completed his PHD
at the Institute for Cultural Science, Klagenfurt University. He joined Zaha
Hadid in 1988.

The Liquid City
M ichael Batty and Andrew Hudson-Smith
Cities are machines for connecting people. Until some 200 years ago, when energy began
to be widely exploited to move people and materials over greater distances, their size and
functions were largely limited by how far one could walk. The largest cities, invariably
capitals of empires, were limited by the resources that could be used to control distant
places and rarely grew to populations of more than one million, a size that was difficult to
sustain. The internal combustion engine changed all that. First railways, then cars,
perhaps planes, enabled cities to spread out physically but in parallel; the shift from
energy to information, from “atoms to bits” as Negroponte has so eloquently phrased it,
is now changing cities in ways that are no longer immediately visible. The telegraph and
the phone dramatically changed the extent to which cities could link with one another in
terms of trade and this forced globalization, but the transition of these information
technologies into computable form and their miniaturization into digital devices now
means that everyone has access to these technologies. The effects of such communication
at a distance, globally, means that cities are beginning to merge into one another, if not
physically, then digitally. We will all be connected in a giant urban cluster by the end of
this century, which will be the physical manifestation of an intricate global nervous
system created by a world of information technologies.
The digital revolution is creating multiple layers or skins of communications media,
some visible but much invisible, some substitutable for traditional physical media but
most complementary to it. This is building complexity. At the same time, much of this
digital media is being embodied into material infrastructures with the prospect of real-time
control of cities using online sensing from the bottom up. The process of constructing the
built environment is fast moving from traditional top-down actions to notions about
growing physical structures from the bottom up, but structures whose growth is only
possible through embedded digital media and information systems. All this is changing the
rate at which we can engender change. When information is instant, from anywhere,
anybody and at any time, things in cities begin to be different. Cities are becoming
considerably faster in their response to new information, to innovation, to physical
change. Populations with more information are able to make decisions ever more
coherently at faster and faster rates. In this sense, cities manifest a new liquidity of action,
a confluence of light and speed, which we term the “liquid city”: a place where physical
desires, face to face contacts, and digital deliberations provide a new nexus of innovation.
Flows, networks, connections rather than inert buildings dominate this physicality as
infrastructure comes to represent this new liquidity which is built on layer upon layer of
flux and flow.
Our new understanding of how cities function is predicated on action from the bottom

up. Cities are built by actions exercised by individuals on behalf of themselves or larger
collectivities, agencies and groups mainly configured as local actions. Global patterns
emerge, best seen in how different parts of the city reflect the operation of routine
decisions which combine to produce order at higher and higher scales. Cities are fractal in
their form and function, as many of the insights in this book suggest. In the main, they
self-organize from the bottom up where local actions are successively done, undone and
transformed as individuals adapt to what is locally optimal. This is design the way nature
intended, and as cities enable more and more of their populations to indulge in positive
decision-making, they are becoming more and more organic. Traditional planning and
design that fights against such self-organization will fail and in this, the best principles for
design must reflect organization from the bottom up: the metaphor is evolution, the way
nature works its magic.
The exemplar par excellence is the network. The various examples which we show here
illustrate network structure at many levels where the hierarchy is implicit in the volumes
of flows. This kind of structure reflects growth around a series of market centres and
other hubs or cores in the urban landscape. Networks spread out to capture consumers
and producers who come together at the core to buy and sell in the market: the traditional
function of transportation in cities is to connect people in economic exchange. Flows of
people to work and to markets to produce or consume through trade and exchange are the
outward manifestations of the most obvious flows of energy which form the glue that
keeps the components of the city together. Networks to channel these flows cannot
develop everywhere so they spread out, tree-like, as if reaching out into the air or the soil,
the way a tree grows out in search of energy to sustain it in the most economical way.
The capacity of these networks increases according to the population that can be
sustained by each of its nodes, and when a node reaches capacity, new ones appear, as in
edge cities that are rapidly changing the dominant form from the monocentric to the
polycentric city. The way cities fill their space is intimately related to the way we try to
use space efficiently, in two and three dimensions, and the form that results is the product
of many decisions that grow structures from the bottom up. Plans that do not account for
such structures and are imposed from the top down are bound to falter for we cannot
exercise the degree of control necessary to contain the diversity of individual actions by
the many that might conflict with the more grandiose schemes of the few that reflect
homogeneity of structure. Plans, in so far as they impact on naturally growing urban
structures, occur in short bursts and are rapidly absorbed into the urban fabric which
continues to mirror social structure, culture and the predominant technology, regardless.
Such rudimentary theories of how cities grow and evolve help us understand the
enormous changes that are being wrought on the city by new information technologies.
Although the emergence of mass physical transportation in the form of the car has
provided an order of magnitude change in our opportunities to communicate at a distance,
the now almost universal use of devices that enable us to capture, share and create
information that can be manipulated at a distance is providing countless new ways of
producing, consuming, thinking, innovating, entertaining and so on. All of these are likely
to have a profound impact on the way we use cities and we are already beginning to see

this in the way we use information to direct our physical movements and interactions. For
the first time, we are beginning to see how the economy is underpinned by transactions as
much of this is now online, how we communicate in various ways with friends and social
groups, how we search for information about others which changes our behavioural
responses to the decisions and actions we continue to make across a wide spectrum of
activities. What is much harder to second guess is how all this information technology is
making an impact on the physical form of the city, particularly at the level of the built
environment. The issue is complicated by the fact that the very information technologies
that we are using to provide a new sense of how we use the city are also being embedded
in the very physical infrastructures that actually compose the city.
Cities are becoming their own sensors at their most elemental level, as their physical
fabric is being automated in ways that enable us to monitor their performance and use.
But these combined material and digital forms are also being overlain with digital skins
that seek to enable populations to use the city in countless different ways, such as
figuring out in real time what services are located in different places, where friends and
acquaintances reside, and what physical means of transportation there are to move oneself
to distant locations. When we combine this with more basic sensors that reflect the way
buildings are working, we are augmenting our reality in ways that have not been possible
hitherto. At higher spatial and temporal scales, our ability to sense the city remotely
offers new insights into how they are growing or declining, how they are being used over
longer periods than in real time, and how we might identify problems that emerge at a
more global scale than those that are more associated with individuals operating in real
time.
All of this offers enormous opportunities for a new age of urban design that takes
account of the city as a self-organizing system. In any age, we always know less than we
need to know to develop the best designs for the future. Our knowledge is always
incomplete and this is a time when so many new developments are pushing us from a
world built on life styles and chances that constrain us to single places to ones that enable
us to embrace many places. Cities are still largely fashioned around physical styles and
behaviours that reflect medieval or even ancient cities, and we may look back and see the
era in which we are currently living as one of a great transition from the city as a village to
the city as a global metropolis where space and time act very differently. In this sense,
our designs in the future are likely to reflect the fact that cities will be so complex that it
will be essential that their continued evolution be based on self-organizing behaviours with
respect to how the environment is created and how we use it. What is very clear is that
the future form of the city must and will reflect a multitude of interaction patterns which
suggest that cities will be used much more intensively for many different purposes over
much longer diurnal and seasonal time spans than cities in the past. M ulti-functional land
use will once again, as in the medieval city, become the norm as information technologies
embedded in ourselves and in our buildings enable us to coordinate many new and
different ways of using the environment. Design based on self-organization where
buildings are reconfigurable, in ways that we have barely begun to imagine, is likely to
become the norm and for this, we require much more than the rudimentary theories about

how cities form and evolve that we have at present. These are in the making as we have
implied here and this book contains a glimpse of what we might expect in this future.
Michael Batty is Bartlett Professor of Planning at University College
London where he runs the Centre for Advanced Spatial Analysis (CASA).
Previously (1990–1995) he was Director of the NSF National Center for
Geographic Information and Analysis (NCGIA) in the State University of
New York at Buffalo, and from 1979 to 1990 he was Professor of City and
Regional Planning at the University of Cardiff. His research work involves
the development of computer models of cities and regions (see
www.complexCity.info/).
Andrew Hudson-S mith is Director and Deputy Chair of the Centre for
Advanced Spatial Analysis (CASA), he is Editor-in-Chief of Future Internet
journal, an elected Fellow of the Royal Society of Arts and Course Founder
and Director of the M Res in Advanced Spatial Analysis and Visualisation at
University College London. He is author of the Digital Urban Blog with 5200
daily readers and has been at the forefront in CASA of developing digital
geographical technologies that support design professionals working in the
built environment.

Ecology and Material Culture
M ichael Weinstock

There are no ecological systems on the surface of the earth that have not been modified in
some way by the effects of the extended metabolisms of human societies. The emergence
and subsequent development of human settlements and cities, of interlinked systems of
cities, of imperial systems and the evolutionary development of the global system that we
now inhabit has extended the metabolic systems of civilization across the face of the
earth. City forms originally emerged from the extended networks of smaller settlements,
from which they condensed. Cities expanded and proliferated, developing in size and
complexity, variations arising as adaptations to the dynamic changes of climate and
ecology within which they were situated. Until quite recently cities extended their
metabolic network across their immediate local territory, and linked systems of cities
across whole regions. As the systems of civilization developed in complexity, cities
became less dependent on their immediate surroundings, and today few cities draw their
energy and materials from their local territory. Cities now extend their metabolic systems
over very great distances, so that the territory of a city and its geographical “place” are
often completely decoupled. Humans have always modified ecological systems and the
topography of the earth, and have done so at a variety of spatial and temporal scales.
The growth and vitality of many cities is no longer dependent on the spatial
relationship with their local territory but on the regional and global flows of resources.
Cities and extended urban conglomerations across the world will continue to expand in the
coming decades as populations rise. Europe already has 80 per cent of its population
living within cities or extended urbanized areas. Conurbations are common right across
Europe, consisting of several overlapping city territories that extend urban fabric across a
range of scales, often coterminous with agricultural, industrial and energy generation
territories. The most densely populated regions of Asia, Europe and North America now
consume biological materials at more than twice the rate at which the ecological systems
of their own regions can regenerate. They are now dependent on resources from distant
regions to meet the deficit in the flow of energy and materials required to support their
preferred patterns of consumption. In turn, the increased demands on other regions limit
their capacity to expand their own populations. M ore than half of all the fuel energy
consumed in the USA and Europe today is imported from other regions, and the
dependence on energy imports in Asia is accelerating.
There are many indicators that suggest that the global system of civilization is poised at
the critical threshold of stability and is consequently very sensitive to social, climatic and
ecological changes. It is clear that the current metabolic system of the world, with its
accelerating informational complexity, extreme velocity and volumes of fuel and food

energy flowing across continents and oceans, and high but inequitable energy and material
consumption, exhibits similar characteristics. Despite the high-density flow of
information and the high proportion of the population engaged in regulating and
administering the energy and material flows, the global system is now highly vulnerable to
disruption and perturbation.

Information Flow
Information transmission has always been an essential characteristic of human culture,
although the means of transmission were slower in the past than they are today, with less
immediate effects. The transmission of detailed information concerning material practices
and architectural forms has been accelerated exponentially several times, with the
sequential emergence of large trading networks, mathematical notation, writing and
drawings systems, printing, shipping and worldwide navigation. As printed images and
text opened up an exchange of knowledge, strong geographical separations between
cultures were weakened: people, materials and artefacts were no longer bound to their
place of origin. Until very recently the working methods of architects were determined by
the basic pattern of the late-nineteenth-century drawing systems of engineering industries.
These were the product of what was then a new practice of industrial drawing, which had
acquired a central role in the production of ships, railway carriages and engines and, later,
motorcars and aeroplanes. The hierarchy of designers and draughtsmen set an increased
distance between the origination of the design and the execution of its construction.
Embedded in the discipline were the concepts of industry, particularly standardization
and accurate repetition, functional instruments of control with an emphasis on the
interchangeability of parts, standardization of forms, and “management” of design,
materials and fabrication.
The conceptual apparatus of architecture has always given a central role to the relations
of mankind and nature. The human body has been a source of harmonious proportions
and the shapes of many living organisms have been adapted for architectural use.
Architecture’s current fascination with nature is a reflection of the availability of new
modes of imaging the interior structures of plants and animals, of electron microscopy of
the intricate and very small, together with the mathematics of biological processes. The
new emerging architecture, that relates pattern and process, form and behaviour, with
spatial cultural parameters, offers new behaviours and adaptations to the changing
ecologies and climate of the natural world.
The transference of material knowledge by spoken, graphical and numerical languages
constitutes a system of information transmission that is distinct from the biological
system of transmission, the genome. Culture acts to transmit complex, social and
ecologically contextualized rules for material practices laterally between local populations
and vertically down through time. It is clear that material culture is inherited by
descendents, there is descent with modification, and that the material forms of buildings
and even cities can be grouped into morphological taxonomies. There are, however,

significant differences between the mode of operation of material cultural evolution and of
biological evolution. Such differences include the mode of inheritance, which in culture
may be horizontal or oblique, as cultural practices concerned with material construction
diffuse between distinct social groups. Perhaps the most significant difference between
biological and cultural evolution lies in the “selection” of forms that survive to pass on
their genes or information to their descendents.
The substantial recent changes to culture, climate and energy economies construct a
new regime of “natural selection” that has destabilized the prior relationship of the
material, cultural and physical ecology within which architecture is produced and
inhabited. M aterial practices are at the beginning of a substantial reconfiguration, and our
future practices are to be located in the intersecting fields of knowledge and data flows.
The interaction of computational systems, the transmission of information by the
internet, and the emergence of a worldwide network of rapid transit systems have each
acted in turn as a positive feedback on cultural transmission and diffusion, and collectively
they have produced a marked contemporary tendency to the convergence of architectural
forms and material practices right across the world. This is as true of motorcars and
mobile phones, of clothes and computers as it is of skyscrapers and shopping malls.
Architecture has begun a systemic change, driven by the changes in culture, science,
industry and commerce that are rapidly eroding the former boundaries between the natural
and the artificial. The complex interaction between the form, material and structure of
natural material systems has informed “biomimetic” industrial processes, generating
“artificial” materials that can be manufactured with specific performance characteristics.
Such new materials have radically transformed everyday consumer products, motor
vehicle and aerospace design. M anufactured cellular materials, especially metals and
ceramics, offer an entirely new set of performance and material values, and have the
potential to reinform and revitalize the material strategies of architectural engineering and
construction. Biomimetic strategies that integrate form, material and structure into a single
process are being adopted from the “nanoscale” of material science for the design and
construction of very large buildings, and new cities.

Architectural Culture and Ecology
The material practices of contemporary architecture cannot be separate from this
paradigm shift, as the context in which architecture is conceived and made has changed. In
the natural world change is normal, but its intricate choreography is now further
accelerated and perturbed by human activities. Global climate change is upon us, and its
effects will be local and regional – more energy trapped in weather systems produces
emergent behaviour and consequences that are not entirely predictable. So too, the
emergent behaviour of local economies and cultures, now connected and interlinked
globally, are substantially reconfigured.
The cultural parameters of the emerging regime of selection that is driving the evolution
of a new architecture are clear. There is a growing cultural fascination with the new

understanding of nature and of natural form both living and non-living. The architectural
and material manifestations of fluidity and dynamics, of networks and new topologies, are
at the centre of architectural discourses and innovations. The materiality of the boundaries
between interior and exterior space, between public and private territories, is no longer so
relentlessly solid and opaque. The increasing transparency of such boundaries is
accompanied by less rigid territorial demarcations. Programmes are not so strictly
confined within the building envelope, and connections and co-existence are enhanced. The
experience of being in spaces that flow one into one another, with “soft” transitions
between private and public domains, and between interior and exterior space, is
increasingly recognized as an essential characteristic of contemporary life. The largest
public spaces, for example in the concourses of transit spaces such as airport terminals,
ports and railway interchanges, have boundaries that are achieved less by rigid walls than
by extended thresholds of graduated topographical and phenomenological character,
enhancing spatial connectivity and coded communication. This form of spatial
organization is not confined to transit systems, but is increasingly found in many
architectural forms that range from the scale of the apartment or house, through the largest
high-rise buildings to new urban configurations and spaces.
New working methods of architectural design and production are rapidly spreading
through architectural and engineering practices, as they have already revised the world of
manufacturing and construction. They include computational form-generating processes
based on “genetic engines” that are derived from the mathematical equivalent of the
Darwinian model of evolution, and from the biological science of evolutionary
development that combines processes of embryological growth and the evolutionary
development of the species. New architectural and infrastructural forms will emerge, with
structural and material behaviours derived from the logics of biological systems. They will
proliferate across the world as constructed material artefacts that are more closely and
symbiotically related to the ecological systems and processes of the natural world.

Michael Weinstock is an architect, currently Director of Research and
Development, and Director of the Emergent Technologies and Design
programme in the Graduate School of the Architectural Association School of
Architecture in London. Born in Germany, he lived as a child in the Far East
and then West Africa, and attended an English public school but ran away to
sea at age 17 after reading Conrad. Weinstock spent years at sea in traditional
wooden sailing ships, with shipyard and shipbuilding experience. He studied
Architecture at the Architectural Association from 1982 to 1988 and has
taught at the AA School of Architecture since 1989 in a range of positions
from workshop tutor, Intermediate and then Diploma Unit M aster, M aster
of Technical Studies and through to Academic Head.

The World Dubai M arine Life Incubators: model of artificial island formations
About …
The eco-social frameworks of this part of the book are two research projects unravelling
the tight relationship existing between the ecology of the local landscape and the related social
practices in M essina, in the Sicilian Stretto, and in Caracas, the Venezuelan capital city.
The projects have a strong analytical and methodological character and have been
instrumental in setting the ground for the synthesis of new eco-social landscapes.
Both contexts are rich in social complexity and both present a particular case of ecologic
stress or rupture between local residents and their own land; the consequences are suffered
by the ecologic systems and the social groups alike, with pollution, landslides and
desertification matched by poverty, violence and grief.
Following a systemic method of investigation of the site, the eco-social frameworks start
by mapping the local topographic regions and related operational fields. An operational
medium is produced, able to generate responses out of urban stimuli. Such a design device
affords the architect a more comfortable and productive platform to breed his or her design
arguments; through correlation of variables it is then possible to develop from a critical
understanding of landscape processes to an engagement with the multiple related social
conflicts. The resolution of ecologic tensions becomes therefore a strategic technique to
resolve parallel social conflicts, thus setting the ground for processes of social reconnection to develop and self-regulate.


In M essina the framework extends to the entire region of the Stretto, to what has been
named a Regional Proto-Garden. In Caracas the framework has more intimate connotations,
involving a form of domestic operational landscape; a manual of landscape consolidation and
phyto-depuration is passed on to the local community to empower the cultivation of this
new domestic resource through a slow process of material and psychological remediation.
The intense experiences provided by these projects in such charged environments have
proved important tests for the development of a method of urban ecologic design that could
span scales and regimes and could be deployed in “tabula rasa” scenarios, where urban life is
absent and architecture is asked to incubate urban life intensity. Examples of such scenarios
exist on the outskirts of Dubai and Al Ain, UAE.
Here eco-social landscape projects operate as urban life incubators; the landscape itself
provides the substratum for the incubation of living mechanisms able to generate a new
abundance of sustainable resources and support interaction among new social groups.
The challenges posed by these two project scenarios are widespread and constitute a clear
point of interest for the contemporary architect operating on a global scale. In Dubai the
artificial World Lagoon was left unfinished by the financial crash, leaving desolation, marine
life devastation and a damaged local fishing industry; with any plan of conventional
development non-viable, some alternative strategy is necessary to avoid the deterioration of
the environmental conditions of a site that has no form of balanced ecology.
In Al Ain, a magnificent and wild desert wadi, or valley, at the outskirts of the city has
been subjected to development pressure. In contrast to Dubai, the design brief here calls for
the development of a new city from zero, devoted to research on renewable technologies and
open to a population of Westerners as well as local academics.
Both projects have a contradictory nature, that seems to be one of the inevitable
consequences of the influence of the current global socio-economic system onto urban
development. The resolution of such contradictions is necessary to promote a sustainable
future for our society; however this requires architects to stop being complacent about the
system and start equipping themselves with the intellectual and methodical resources
necessary to promote valid design alternatives.
In Dubai such an alternative appears in the form of a coral garden, an artificial reef where
multiple processes of mineral accretion, sand sedimentation and water depuration have been
imagined. Characterized by very low initial investment and a slow self-propelling growth,
this design solution will transform the lagoon into a living reef able to attract researchers and
visitors, and with them investing institutions with interests in global ecology. Such
programmatic intensification will be supported by the material substratum that, on reaching
maturity, will provide a stable ground for construction, avoiding the need for expensive
consolidation works.
The protocol of development derived from our experience in Caracas is adopted more
tactically here, an algorithmic form of planning that is able to choreograph the territory,
without prescribing any rigid final arrangement; such an open and co-evolving framework is
necessary to cope with the accidents of the future and embrace their vital powers.
In Al Ain this protocol is more literally deployed as a normative system for the new city,
a cyber-favela where the informal logic of shanty town developments is coupled with the

possibility offered by ubiquitous environmental sensing and energy-producing technologies.
What emerges is a form of rhizomatic village, where tight social groups opportunistically
colonize the landscape while at the same time nurturing it, in a delicate dynamic equilibrium;
as trading emerges among the groups so does urban life and social diversity.



This anthology assembles more than two dozen essays by many of the key figures
in present-day continental philosophy. They hail from thirteen countries, speak seven different native languages, and are separated from eldest to youngest by a range
of more than forty years. (The collection would have been even more diverse, if not
that several additional key authors were prevented by circumstance from contributing.) A number of well-established authors can be found in the pages that follow,
joined by various emerging figures of the younger generation. These are exciting
times in our field. No dominant hero now strides along the beach, as the phase of
subservient commentary on the history of philosophy seems to have ended. Genuine attempts at full-blown systematic thought are no longer rare in our circles; increasingly, they are even expected. And whatever the possible drawbacks of globalization, the new global networks have worked very much in our favour: enhanced
technologies have made the blogosphere and online booksellers major contributors
to a new ‘primordial soup’ of continental philosophy. Though it is too early to know
what strange life forms might evolve from this mixture, it seems clear enough that
something important is happening. In our profession, there has never been a better time to be young.
The first wave of twentieth century continental thought in the Anglophone
world was dominated by phenomenology, with Martin Heidegger generally the most
influential figure of the group. By the late 1970s, the influence of Jacques Derrida
and Michel Foucault had started to gain the upper hand, reaching its zenith a decade or so later. It was towards the mid-1990s that Gilles Deleuze entered the ascendant, shortly before his death in November 1995, and his star remains perfectly visible today. But since the beginning of the twenty-first century, a more chaotic and in
some ways more promising situation has taken shape. Various intriguing philosophical trends, their bastions scattered across the globe, have gained adherents and started to produce a critical mass of emblematic works. While it is difficult to find a single
adequate name to cover all of these trends, we propose ‘The Speculative Turn’, as a
deliberate counterpoint to the now tiresome ‘Linguistic Turn’. The words ‘materialism’ and ‘realism’ in our subtitle clarify further the nature of the new trends, but also
preserve a possible distinction between the material and the real.
Following the death of Derrida in October 2004, Slavoj Žižek became perhaps the
most visible celebrity in our midst, eased into this role by his numerous publications
in English and his enjoyable public persona. To an increasing degree, Žižek became
closely linked in the public mind with his confederate Alain Badiou, whose major works
were increasingly available in English during the first decade of the century, with a key
assist from Peter Hallward’s encyclopaedic survey, Badiou: A Subject to Truth.1 It is probable that Badiou and Žižek are the most widely read living thinkers in Anglophone continental philosophy today. But others of their approximate age group have entered the
mix as well, championed initially by smaller groups of readers. Bruno Latour, already
a giant in anthropology, sociology, and science studies, was smuggled into continental philosophy by way of the ‘object-oriented ontology’ of Ian Bogost, Levi Bryant, and
Graham Harman. Somewhat ironically, Latour’s longtime intellectual friend Isabelle
Stengers followed a rather different path into the Anglophone debate, by impressing
the younger Deleuzians with her work on Deleuze and Whitehead, and with her own
series of books known as Cosmopolitiques.2 The ‘non-philosophy’ of François Laruelle has
captured the imagination of many younger readers, despite relatively little of his work
being available in English so far. This rising generation of Laruellians has also tended
to show great interest in cognitive science and the various practitioners of ‘neurophilosophy’. Another important year was 2002, when Manuel DeLanda in Intensive Science and Virtual Philosophy3 and Graham Harman in Tool-Being4 both openly proclaimed
their realism, perhaps the first time this had been done with a straight face in the recent
continental tradition.5 A half-decade later, this explicit call for realism was reinforced
by what is so far the best-organized movement of the next generation. Inspired by the
publication of Quentin Meillassoux’s Après la finitude6 (After Finitude) in early 2006, the
first Speculative Realism event was held in April 2007 at Goldsmiths College, London.
The original group included Ray Brassier, Iain Hamilton Grant, Harman, and Meillassoux; Alberto Toscano was moderator in 2007 and Meillassoux’s replacement at the
follow-up event at Bristol in 2009. But while the group has already begun to break into
various fragments, it remains a key rallying point for the rising generation of graduate
students. Thanks to the recent importance of the blogosphere, and the aggressive acquisitions policies of new publishers such as zerO Books, many of these students are already surprisingly well known. The editors of this volume are pleased to have Nick Srnicek on board as a fitting representative of this group.
An Introduction to Continental Materialism & Realism
 the vaunted anti-humanism of many of the thinkers identified with these trends, what
they give us is less a critique of humanity’s place in the world, than a less sweeping critique of the self-enclosed Cartesian subject. Humanity remains at the centre of these
works, and reality appears in philosophy only as the correlate of human thought. In
this respect phenomenology, structuralism, post-structuralism, deconstruction, and
postmodernism have all been perfect exemplars of the anti-realist trend in continental
philosophy. Without deriding the significant contributions of these philosophies, something is clearly amiss in these trends. In the face of the looming ecological catastrophe,
and the increasing infiltration of technology into the everyday world (including our
own bodies), it is not clear that the anti-realist position is equipped to face up to these
developments. The danger is that the dominant anti-realist strain of continental philosophy has not only reached a point of decreasing returns, but that it now actively limits
the capacities of philosophy in our time.
Yet in the works of what we describe as ‘The Speculative Turn’, one can detect the
hints of something new. By contrast with the repetitive continental focus on texts, discourse, social practices, and human finitude, the new breed of thinker is turning once
more toward reality itself. While it is difficult to find explicit positions common to all
the thinkers collected in this volume, all have certainly rejected the traditional focus
on textual critique. Some have proposed notions of noumenal objects and causalityin-itself; others have turned towards neuroscience. A few have constructed mathematical absolutes, while others have attempted to sharpen the uncanny implications of psychoanalysis or scientific rationality. But all of them, in one way or another, have begun
speculating once more about the nature of reality independently of thought and of humanity more generally.
This activity of ‘speculation’ may be cause for concern amongst some readers,
for it might suggest a return to pre-critical philosophy, with its dogmatic belief in the
powers of pure reason. The speculative turn, however, is not an outright rejection of
these critical advances; instead, it comes from a recognition of their inherent limitations. Speculation in this sense aims at something ‘beyond’ the critical and linguistic
turns. As such, it recuperates the pre-critical sense of ‘speculation’ as a concern with
the Absolute, while also taking into account the undeniable progress that is due to the
labour of critique. The works collected here are a speculative wager on the possible
returns from a renewed attention to reality itself. In the face of the ecological crisis,
the forward march of neuroscience, the increasingly splintered interpretations of basic
physics, and the ongoing breach of the divide between human and machine, there is
a growing sense that previous philosophies are incapable of confronting these events.

The Origins of Continental Anti-Realism
The new turn towards realism and materialism within continental philosophy comes
in the wake of a long period of something resembling ethereal idealism. Even while
disdaining the traditional idealist position that all that exists is some variation of mind
or spirit, continental philosophy has fallen into an equally anti-realist stance in the
form of what Meillassoux terms ‘correlationism’. Stated simply, this is ‘the idea according to which we only ever have access to the correlation between thinking and being,
and never to either term considered apart from the other’. Towards a Speculative Philosophy

that we can aim our thoughts at being, exist as beings-in-the-world, or have phenomenal experience of the world, yet we can never consistently speak about a realm independent of thought or language. Such a doctrine, in its countless variations, maintains
that knowledge of a reality independent of thought is untenable. From this correlationist stance, there results a subtle form of idealism that is nonetheless almost ubiquitous.
The origins of this correlationist turn lie in Immanuel Kant’s critical philosophy,
which famously abjured the possibility of ever knowing a noumenal realm beyond human access. In Kant’s famous Copernican revolution, it is no longer the mind that conforms to objects, but rather objects that conform to the mind. Experience is structured
by a priori categories and forms of intuition that comprise the necessary and universal
basis for all knowledge. Yet the price to be paid for securing this basis is the renunciation of any knowledge beyond how things appear to us. Reality-in-itself is cordoned
off, at least in its cognitive aspects.
Lee Braver’s fine book recently showed that this Kantian prohibition, with its antirealist implications, has wound its way through the continental tradition, taking hold of
nearly every major figure from Hegel to Heidegger to Derrida.8 While for Kant there remains the possibility of thinking the noumenal (if not knowing it), Hegel absolutizes the
correlate to encompass all that exists: his critique of the noumenal renders it merely a
phenomenal illusion, thus ‘completing’ the critical philosophy by producing an absolute
idealism. This effacement of the noumenal continues with phenomenology, as ontology becomes explicitly linked with a reduction to the phenomenal realm. As Braver outlines, Heidegger furthers the anti-realist project by rejecting the possibility of Absolute
Knowledge as the singular and total self-understanding of the Absolute Subject. Finally, with Derrida the mediation of language becomes all-encompassing, as the phenomenal realm of subjectivity becomes infested with linguistic marks. Throughout this process, any possibility of a world independent of the human-world correlate is increasingly
rejected (as is nicely symbolized by Heidegger’ famous crossing-out of the word ‘Being’).
This general anti-realist trend has manifested itself in continental philosophy in a
number of ways, but especially through preoccupation with such issues as death and
finitude, an aversion to science, a focus on language, culture, and subjectivity to the
detriment of material factors, an anthropocentric stance towards nature, a relinquishing of the search for absolutes, and an acquiescence to the specific conditions of our
historical thrownness. We might also point to the lack of genuine and effective political action in continental philosophy—arguably a result of the ‘cultural’ turn taken by
Marxism, and the increased focus on textual and ideological critique at the expense
of the economic realm.

The Speculative Turn
Against this reduction of philosophy to an analysis of texts or of the structure of consciousness, there has been a recent surge of interest in properly ontological questions.
Deleuze was a pioneer in this field, including in his co-authored works with Félix Guattari. In these seminal texts of the 1970s and 1980s, Deleuze and Guattari set forth an
ontological vision of an asubjective realm of becoming, with the subject and thought
being only a final, residual product of these primary ontological movements. Rather than circling around the negative limitations of conceptual systems, Deleuze and Guattari constructed a positive ontological vision from the ruins of traditional ontologies. While there are still significant questions about whether Deleuze managed to escape correlationism fully,9 there can be little doubt that his project was aimed at moving beyond the traditional Kantian limitations of continental thought.10 More recently,
a number of other leading thinkers in the continental tradition have articulated philosophies that avoid its standard (and oft-ridiculed) tropes.
Žižek is one of the foremost exemplars of this new trend, drawing on the naturephilosophy of Schelling, the ontological vastness of Hegel, and the insights into
the Real of Jacques Lacan.11 In his recent major work The Parallax View, Žižek has denounced what he sees as the naïve materialist postulate that includes the subject as just
another positive, physical thing within the objective world. He calls it naïve because
it assumes the position of an external observer from which the entire world can be
grasped—a position that presumes in principle to encompass all of reality by reducing
its own perspective to a thing in the world. For Žižek, by contrast, ‘Materialism means
that the reality I see is never “whole”—not because a large part of it eludes me, but because it contains a stain, a blind spot, which indicates my inclusion in it’.12 Reality, he
repeatedly states, is non-All; there is a gap, a stain, an irresolvable hole within reality
itself. The very difference between the for-itself and the in-itself is encompassed within the Absolute. Only by attending to this gap can we become truly materialist. But
while Žižek has signalled a ‘transcendental materialist’ turn within recent continental
thinking, it is perhaps Badiou who has raised the anti-phenomenological flag most explicitly, attempting thereby to clarify the ontological stakes of contemporary continental philosophy. This rejuvenation of ontology is particularly clear in his now famous
declaration that ‘mathematics = ontology’.13 Taking mathematics to be the discourse
of being—that which speaks of being as devoid of any predication (including unity),
remaining only as a pure multiple—Badiou has constructed an elaborate ontology on
the basis of set theory. In addition, Badiou has nobly resuscitated the question of truth,
which was formerly a term of derision in much continental philosophy.
While still read more widely in the social sciences than in philosophy, Latour has
nonetheless been an important figure in the recent Speculative Turn. Against all forms
of reduction to physical objects, cultural structures, systems of power, texts, discourses, or phenomena in consciousness, Latour argues for an ‘irreductionism’ in which all
entities are equally real (though not equally strong) insofar as they act on other entities. While nonhuman actors such as germs, weather patterns, atoms, and mountains
obviously relate to the world around them, the same is true of Harry Potter, the Virgin Mary, democracies, and hallucinations. The incorporeal and corporeal realms are
equally capable of having effects on the world. Moreover, the effort to reduce one level of reality to another invariably leaves residues of the reduced entity that are not ful        ly translatable by the reduction: no interpretation of a dream or a historical event ever
gets it quite right, nor would it even be possible to do so.
Beyond the institutionalized sphere of philosophy, continental materialist and realist currents have had some of their deepest effects through a series of emerging online communities. This began in the late 1990s with the creation of the Cybernetic
Culture Research Unit (CCRU)—a diverse group of thinkers who experimented in
conceptual production by welding together a wide variety of sources: futurism, technoscience, philosophy, mysticism, numerology, complexity theory, and science fiction,
among others. The creativity and productivity of this collective was due in no small
part to their construction of a space outside the constrictions of traditional academia.
It is notable, then, that many of the contributors to CCRU have continued to be involved in the online community and have continued to push philosophy ahead.
One of the most notable of these projects has been the journal Collapse, which
along with the Warwick-based journal Pli has acted as one of the vanguard publications of recent continental realism and materialism. First issued in September 2006,
Collapse has attempted to mobilize a cross-section of innovative thinkers from a wide
range of disciplines. Combining philosophy, science, literature and aesthetics in a way
that refuses to draw divisions between disciplines, Collapse has exemplified the spirit of assemblage—letting a heterogeneous set of elements mutually resonate to become something entirely unpredictable. As its opening salvo proclaims, ‘the optimum
circumstance would be if each reader picked up Collapse on the strength of only one
of the articles therein, the others being involuntarily absorbed as a kind of side-effect
that would propagate the eccentric conjuncture by stealth, and spawn yet others’.14 In
its third volume Collapse also reproduced the text of the first conference devoted to the
speculative realist movement, a galvanizing event that did much to focus attention on
the wider trends contained in this volume.
Along with Collapse, another non-institutional forum for conceptual production
has been the online community. Initially operating in the 1990s through email listserves, online discussion has shifted to the blogosphere as this medium emerged in the
opening decade of the century. Indeed, each of the editors of The Speculative Turn authors one or more philosophy blogs,15 and in a further wondrous sign of the times, we
have never met in person. As any of the blogosphere’s participants can attest, it can be
a tremendously productive forum for debate and experimentation.16 The less formal
nature of the medium facilitates immediate reactions to research, with authors presenting ideas in their initial stages of development, ideally providing a demystifying
sort of transparency. The markedly egalitarian nature of blogs (open to non-Ph.Ds in
a way that faculty positions are not) opens a space for collaboration amongst a diverse
group of readers, helping to shape ideas along unforeseen paths. The rapid rhythm of
online existence also makes a stark contrast with the long waiting-periods typical of refereed journals and mainstream publishers. Instant reaction to current events.
Continental Materialism and Realism
As should be clear from the earlier discussion of Deleuze, Žižek, Badiou, and Latour,
the various strands of continental materialism and realism are all entirely at odds with
so-called ‘naïve realism’. One of the key features of the Speculative Turn is precisely that the move toward realism is not a move toward the stuffy limitations of common sense, but quite often a turn toward the downright bizarre. This can be seen quite
clearly in the works of the four original members of the speculative realism group, perhaps the most visible group among those now reaching maturity.
Ray Brassier’s work combines a militant enthusiasm for the Enlightenment with a
theoretical position that drastically limits the presumptions of thought in its ability to
grasp the nature of reality. Cutting across a number of closely-held human conceits—
including our usual self-esteem as a species and our aspiration towards harmony with
nature—Brassier’s work aims at eliminating anything that might falsely make us feel
at home in the world. The result is a position that might be called an eliminativist nihilism that takes the destruction of meaning as a positive result of the Enlightenment
project: something to be pushed to its ultimate end, despite all protests to the contrary.
A stark contrast is provided by Iain Hamilton Grant’s return to the naturephilosophy of Schelling, which aims to construct a transcendental naturalism capable of providing an ontological foundation for science. Grappling fully with the implications of
Kant’s critical turn even while constructively opposing it, Grant tries to move the transcendental project beyond its idealist tendencies so as to connect it with a dark and
rumbling field of pure ‘productivity’ lying beneath all phenomenal products. It is from
these very depths that nature, mind, society, and culture are all produced. Grant also
aims to provide a consistent metaphysical foundation for contemporary science. A different approach to the non-human world is found in the object-oriented philosophy of Graham Harman. Like many of the Austrian philosophers of the late nineteenth century, Harman pursues a general theory of objects ranging from quarks to
solar systems to dragons to insurgencies, but he also adds several weird twists to the
theory. From one side he treats objects according to the Heideggerian insight that objects withdraw into depths inaccessible to all access. And from another side he follows
Whitehead’s model, in which the relation between human and world is merely a special case of any relation at all: when fire burns cotton, this is different only by degree from
the human perception of cotton. Whereas the phenomenological method bracketed
the natural world out of consideration, Harman treats the phenomenological and the
natural, or the perceptual and the causal, as neighbours in a drama in which objects
can only make indirect contact with one another.
Quentin Meillassoux, whose 2006 debut book might be called the trigger for the
Speculative Realist movement, argues for a mathematical absolute capable of making
sense of scientific claims to have knowledge of a time prior to humanity. These ‘ancestral’ statements pose a problem for philosophies that refuse any knowledge of a realm
independent of empirical access to it. If we are to understand these ancestral statements literally, however, it must be shown that we already have knowledge of the absolute. Meillassoux’s uniqueness lies in showing how correlationism (the idea that being and thought are only accessible in their co-relation) is self-refuting—that if we take
it seriously, it already presupposes a knowledge of the absolute. Yet unlike the other
Speculative Realists, Meillassoux is not dismissive of correlationism, but seeks to radicalize it from within. From the facticity of our particular correlation, Meillassoux derives the necessity of contingency or ‘hyperchaos’: the apparently counterintuitive result that anything is possible from one moment to the next.
GANs have been successfully adapted to classical image processing
problems, such as inpainting [Pathak et al. 2016; Yeh et al. 2017; Yu
et al. 2018] and super-resolution [Wang et al. 2018c]. While traditional GANs generate images starting from a random vector, the
GAN training can be extended to the problem of image-to-image
translation using either paired or unpaired training data [Huang
et al. 2018; Isola et al. 2017; Zhu et al. 2017a,b]. In computer graphics, recent papers apply GANs to the synthesis of caricatures of
human faces [Cao et al. 2018], the synthesis of human avatars from
a single image [Nagano et al. 2018], texture and geometry synthesis of building details [Kelly et al. 2018], surface-based modeling
of shapes [Ben-Hamu et al. 2018] and the volumetric modeling of
shapes [Wang et al. 2018a]. The most related problem to our work
is the problem of terrain synthesis.
The first step of preprocessing is to create a large set of texture
samples S that are generated using the generator network from a
standard deep convolutional GAN. Each sample si comprises two
components: (1) an intermediate tensor tl = G Al (z), which we refer
to as a latent tile where l is the level at which we will synthesize the
latent field, and (2) dr a downsampled version of the texture map
G Bl (tl ), where r represents its spatial resolution. The greater the
number of samples in S, the more texture variability is afforded by
our framework. The second step in this phase is to cluster the texture
samples in S by their visual appearance, using dr , in order to enable
fast lookup of visually similar latent tiles. We perform the clustering
using k-means and assign cluster centers c k as representative texture
samples.

Cities, estates and routing systems develop, change constantly and fundamentally cannot be planned. Claims to ownership, land and building regulations, planning decisions and political interventions make it difficult for settlement structures to adapt to constantly changing requirements to such an
extent that meaningful and totally ecological use of the surface of the earth
is becoming increasingly difficult, although new techniques and flexible
planning models mean that a connection could be found with the self-designing processes of urban-development history.
Plants are anchored in their location on the face of the earth, animals and
human beings have mobile territory and encampments that become static
with increasing density. Human settlements are organisms, but they are not
hereditarily anchored in their form like corals, sponges or beehives. They
often grow and shrink at the same time. Their form can almost never be
called chaotic. Typical self-formation processes lead to astonishing genetic
optimization in the course of time. Processes of change have become so
rapid today that current urban-planning theories have been overtaken.
But high effectiveness of self-created, in other words unplanned settlements in terms of energy and biology is totally achievable today in »natural«
town and transport planning and leads to ecologically meaningful solutions
that are also full of beauty.
The study was written in the context of special research »Natural Constructions« by the Deutsche Forschungsgemeinschaft, and has hitherto
been available only in German and as a working paper for circulation between those involved in the research project.
Frei Otto is one of the 20th-century’s most important architectural visionaries. Although at a first glance his buildings like the German Pavilion for
the 1967 World Fair in Montreal, designed with Rolf Gutbrod, the roofs for
the Olympic buildings in Munich designed by Günter Behnisch or the project developed with Christoph Ingenhoven for a new main station in Stuttgart seem to be in the tradition only of the great constructors of this century
like Felix Candela or Pier Luigi Nervi, his work goes way beyond mere construction. He is a technician, artist and philosopher in one, and his central
concern is for a new and all-embracing link with nature in building.
Investigating »minimal pathways« was one of the first
research subjects for the Institut für leichte Flächentragwerke at the Technische Hochschule in Stuttgart, newly
founded in 1964. It was a subject I had already studied
at the Entwicklungsstätte für Leichtbau in Berlin.
These experiments and numerous ways of generating minimal pathways – the shortest route between two
random points – were published in the first little volume
published in German and English, by the institute, IL 1.
This interest in minimal pathways and minimal surfaces
was initiated not least by interdisciplinary contacts with
the geodetist Klaus Linkwitz in Stuttgart and the mathematician Stefan Hildebrand in Bonn.
Application-related research work on nets in nature
and technology and on lattice shells followed as part
of the special research on »Weitgespannte Flächentragwerke« (Wide-span two-dimensional frameworks; 1970
to 1985). The next special research project »Natürliche
Konstruktionen« (Natural structures) at the universities
of Stuttgart and Tübingen was devoted to the fundamentals of light-weight construction in nature and technology, continuously accompanied academically and
stimulatingly by the Berlin biologist and anthropologist
Johann Gerhard Helmcke. The special research projects of the DFG, the Deutsche Forschungsgemeinschaft (German Research Foundation), called into being
in 1968 by the geneticist Helmut Baitsch and the then
DFG president Heinz Maier-Leibnitz gave academics
from different disciplines the chance to conduct joint
basic research across subject areas. They are and were
the actual predecessors of today’s excellence initiatives,
embedded in the structure of research and teaching in
German universities at that time.
Of course housing estate construction and urban
development were essential components in the special
field of »Natürliche Konstruktionen« (Natural structures).
Two Stuttgart university institutions worked together
on this sub-theme, the Institut für leichte Flächentragwerke and Klaus Humpert’s Institut für Städtebau.
As is often the case in interdisciplinary research projects, relatively long lead times were often needed for
various reasons. Different specialist languages, working
methods and ways of thinking have to be understood
by the partners, accepted or even adapted. Even while
a special research project was still running it was possible for the IL and the town planning institute to publish
important working approaches and results, above all a
work by Eda Schaur on the subject of unplanned housing estates.
The present work on the history, origins, function
and changes in housing estates and their connection
may be seen as the beginning of a new way of looking
at town planning as a field.
Knowledge of the self-perpetuating processes of
natural occupation of points, lines, areas and spaces
would have to be a fundamental requirement of any
town planning. It is quite clear that few planners are familiar with them. Planning means applying knowledge. Architecture and planned towns come into being
by arranging familiar things. Researching the processes
of occupying and connecting in nature and technology
requires a fresh start, with observations, experiments
and the development of explanatory models.
Nets, paths, connection, nodes and occupied areas
run all through our natural and technical environment,
creating and influencing it. Knowledge about occupying
and connecting is thus one key to understanding historical and modern contexts. Leonhard Euler’s solution
to the problem of the »Sieben Brücken von Köngisberg« (Seven bridges of Königsberg; 1736) was the first
mathematical network model, and is just as topical today as current tasks facing urban development and
transport and communication technology.
These statements should only be seen as an introduction to a very broad subject. They are intended to
stimulate people to look even more closely then previously at the surface of our earth, with open eyes, in
order to learn how to understand the processes that
shape its form.
They could then perhaps also lead to new concepts
for planning towns and housing that run less counter
to nature as a result of conscious integration or of promoting self-education processes.
I dedicate this work to my comrades-in-arms Berthold Burkhardt (member of the research team since
1964), Klaus Humpert, Marek Kolodziejzyk, Ulrich Kull,
Klaus Linkwitz and Eda Schaur. I would further like to
thank Berthold Burkhardt for his help in implementing
the work in its present form. I would also like to thank
Michael Robinson for the translation, Nora Krehl-von
Mühlendahl for the editorial work, Helga Danz for the
layout, and Dorothea Duwe and Axel Menges for making the book part of the Edition Axel Menges publishing programme.
Human beings and animals, but also plants all occupy
surfaces, but also points (locations), lines and spaces.
However, the elements of non-living nature of all kinds
also spread across the most diverse surfaces. The occupation may be mobile, but may also be static, may
be random and chaotic, unplanned and planned,
planned and altered, improved or worsened, alienated
or simply made more natural by self-constitution processes.
A territory is the living space and sphere of influence
of animals, the habitat of plants (or rocks), the situation
of fields, sites, farmland. For human beings, exterior
and interior immediate surroundings, such as a house
and a garden, play a significant role. Familiar ground
and human spheres of influence, often known as territories, distance themselves. They strive to achieve the
greatest possible intervening distance.
The closest proximity with which animals and humans can position themselves is largely dependent on
the physical construction of their bodies. In the case of
human beings, one speaks of a private area and bodily
contact.
The root stocks of plants, the nests and colonies of
animals and the houses of human beings, together with
connecting paths, are the distinguishing marks and static elements of occupied surfaces.
The sphere of influence of individual human beings is
mobile. Only a sleeping human being rests and occupies his own portion of the Earth’s surface, his property.
The most restricted form here, the bed, may sometimes
only exist for a single night. The pretension of considering a part of the Earth’s surface to be one’s property is
an invention of modern and recent human history.
In fact, ground being declared property often retards
and hinders the processes of natural and peaceful occupation.
However, only a person familiar with these processes
can engage in conflict limitation. Research into these
processes supports the peaceful coexistence of human
beings and nature.
Occupying specific points
Free-standing trees and towers at a distance from
other constructions statically occupy a location, a point.
A single bird on a roof ridge, ready to fly away again –
that is, a temporary presence – occupies a point. Occupying specific points may also imply distancing.

Linear occupation
Birds sit on a wire, close together yet with a
minimum interval between allowing them to launch immediately, or not to come too close to their neighbours.
Glue beads and fog dew  on spiderwebs
arrange themselves on threads like strings of pearls
Plots of ground occupy surfaces on roads There are countless further examples.

Natural and technological occupations
Occupations belonging in the areas of non-living nature,
living nature and technology can be distinguished. To a
great extent, technology utilises occupation mechanisms from non-living nature and spontaneous physical
or chemical processes. Geodesy or the division of the
Earth’s surface can be considered a planned, i. e. less
natural occupation mechanism. The division of the
Earth’s surface into meridians and circles of latitude, for
instance, is both artificial and at the same time useful.
Almost all natural occupations are subject to selfconstituting principles of varying strength. This is especially clear in the »occupation« of an even surface by
shrinkage cracks (in clay or glazes), which predominantly enclose hexagonal surfaces and whose key
points, in an ideal situation, form a triangular pattern.
The occupation (structure) of the surfaces of leaves
or insect wings and to a great extent the occupation of
territories by animals and human beings can be placed
in the same category.

Mobile and static occupations
Birds of prey have mobile territories, whose situation
depends among other things on the population density,
behaviour and incidence of their prey animals.
The same is true of many animals and also of human
beings who hunt. The narrowest human territory is
one’s own private sphere. Like the entity it belongs to, it
is mobile.
Firmly rooted plants have static habitats for the duration of this rooting. Due to drifting seeds or tendrils,
they are mobile for the purposes of distribution, or territorial expansion. Animals which create dense or established colonies also occupy a part of the Earth’s surface
during the time they are resident.
In cities, paths and large houses reduce human mobility. Towns with a high degree of mobility can adapt
their size and situation to deal with processes which
accelerate, retard or even force the abandoning of an
occupation.
The primordial and basic human settlement forms
can be observed in particular in the spontaneous occupation of bathing areas and beaches by bathers, but
also in trailer and tent cities in which no form of intervention by planning has taken place. It is clear that in these situations settlement structures with typical properties, details and forms develop which are common to
different countries and cultures, and are not even
greatly changed by climatic influences.

Distancing occupations
Objects which occupy points, lines, surfaces or spaces
at the largest possible intervals are described as distancing occupations. This can be seen when, for instance, birds sit on pylons with the largest possible interval between them. The way lone predators mark out
their territories is a typical distancing occupation process. However, hunters, trappers and village land / forest husbandry associations also have »territories« situated at the largest possible intervals (illus. 5,6).

Random occupations
Occupation structures which show no principles of regulation whatsoever can be described as random or
chaotic. However, there appear to be no occupation
processes without principles of regulation. For the most
part, their order is simply difficult to recognize, even
where coincidences have had an effect.
The places where seeds carried by the wind land
can be described as random. Only a few of the seeds
germinate and take firm root in favourable habitats.
Only a few plants grow to maturity. The elimination
processes influence the occupation of the situation and
the form of all surviving plants. Coincidence is regulated.


The occupation structures have entirely typical
forms, which will be described in detail later in this text.
Distancing occupations and processes which initiate
or promote them are common in non-living nature and
have many variations. Examples include the configuration of cracks in drying clay layers or in hardening rock.
Thermal columns are also distributed over the Earth’s
surface in a distanced way. With grasses, trees, tall
buildings and towers, which usually show distanced
occupation on ground level as well, the third dimension
starts to open up. Typical examples of three-dimensional occupations include birds’ nests, many spiderwebs and space frames for all kinds of high-rise buildings. Distancing occupations in three-dimensional
space are also numerous.

It is revealing to study random occupation systematically. A suggested experiment:
Allow a variety of objects to fall from a balcony on a
clear day. The situation, form, size and density of the
type of territories associated with such »random occupations« can be determined.
If, for instance, one demarcates the territory of an
object by the perpendicular bisectors of the nearest
points, it can easily be seen that the majority of these
territories are hexagonal and border on six neighbours.
Only in exceptional cases do pentagonal territories
form, or the borders of four territories converge on a
single point 


The peculiar human drive towards order which enables
humans to create structures using technological means
and also to recognize, maintain and measure structures
more easily, automatically leads to planned occupations.
The most familiar structures occupy surfaces with a
3-, 4- or 6-cornered grid, forming hexagonal, quadratic,
rectangular, rhomboid or triangular territories (illus. 4).
Every planning, whether of a road with boundary posts,
division of plots of ground or the building of multi-storey
structures, relies on a knowledge of the rules for oneto three dimensional occupations. In evaluating the results, it is in theory irrelevant what materials are used to
plan the occupation. And yet the materials (T-square,
curve template, optical instruments and computers,
use of construction machinery, planting and harvesting
instruments) do have an influence on the outcome.

Molecules in gases and liquids move away from one
another. In enclosed spaces, they take up positions as
far away from each other as possible. This is shown especially clearly in the expansion of strongly fragrant
substances. A similar procedure can be seen in the ice
crystals of cirrus clouds. This is especially clear in the
so-called anvil shape of a thundercloud, which, in contrast to the lower-lying cumulus clouds, drifts apart. The
same can be observed in cirrus clouds and freezing
contrails 

Attractive occupation of surfaces can be seen in
throngs of people. The closest possible proximity is to
be in bodily contact. Normal attractive occupation excludes private area.
Many animal species »huddle« together, creating
dense occupations. Seagulls brood close together, but
in such a way as not to endanger each other or to prevent a quick takeoff.
Seen from above, the herds formed by herd animals
have rounded forms, which, even during rapid movement, maintain a minimal circumference and are reminiscent of drops of mercury rolling over rough terrain.
The same is true of flocks of birds and shoals of fish
moving in space. Even during rapid movements, individuals maintain the closest possible proximity. The outlying flyers always press towards the centre. The flock
has a boundary layer, although this is not necessarily
distinct. The form of the flock seen as a whole is related
formally not only to soap bubbles in the wind, but also
to moving drops of oil floating in water 



Attractive occupations
When objects which are mutually attractive to one another occupy lines, surfaces or spaces, this can be referred to as attractive occupation. Attractive occupation
is characterised by the close proximity of the occupying
elements.
Strings of pearls, birds flocking on cables and
houses built close together along roads are typical representatives of attractive occupation of linear elements




As with a flat plane, there are many ways for threedimensional »territories« to fit together in three-dimensional space. The most familiar involve the placing together of cubes or blocks. Their key points form a
Cartesian grid network.
The closest configuration, creating the minimal surface area and placing the key points extremely close together, is assumed by soap bubbles of the same size.
This is however a highly complicated configuration, in
which every single bubble has a slightly different form.
This is the structure of foam.
Such foam formations and structures with a high degree of order which are often described superficially as
chaotic are shown and described in detail in the series
of publications by the Institut für leichte Flächentragwerke (see p. 112, bibliography).

Many occupation mechanisms show both kinds of occupation simultaneously. Gregarious birds such as starlings, sitting on an electrical cable, initially huddle together nervously, but also keep a certain distance, in
order to be able to flee. Nesting gulls or settling people
converge while keeping their distance (private area, individual territory). Perhaps one could include in this category of occupation stars drifting apart in outer space,
which are however held together to form galaxies?
Animals or human beings hunting in groups keep together while at the same time occupying the largest
possible hunting territories. Inhabited forest houses or
colonists’ homesteads occupy a surface in a way which
is both attractive and distancing. The same is true of
defensive castles, as well as agricultural villages with
closely-packed houses.
Many examples can also be cited from the occupation of space taking place in terms of surface area,
whether frost crystals, hairs, bushes, grasses, bamboo
or trees.
Human beings build high-rise buildings in which people
live closely packed together, or towers which allow antenna to transmit electromagnetic signals over a wide
area.
Attractive and at the same time distancing occupations are also numerous in the third dimension. Molecules in gases and liquids distribute themselves at the
maximum distance from one another. When absorption
capacity (saturation) is reached, they combine to form
densely-packed crystals and create solid bodies, which
then occupy nearby objects of any form or float distanced from each other in space (air or water). The
whole weather cycle is affected by attractive and distancing occupations playing out in three-dimensional
space. Not only do areas of high and low pressure, and
also localised thermals with simultaneous formation of
a spiralling column of air, take up distancing / attractive
positions on the earth’s surface, but cloud formation,
rain and snowfall processes are created or affected by
distancing / attractive occupation processes. Water molecules distribute themselves in the air in a distancing
way. When saturation is reached, water condenses
around particles of solid substance which have also
distributed themselves in space in a distanced way.
These may be fine dust or rust particles (silver iodide
crystals, the so-called rainmaker’s substance, are extremely attractive to water particles). Extremely small
water droplets, which initially float at a distance, form
inert, flat mist with indistinct outlines. As the moisture
level increases, the mist precipitates on the ground (wet
fog), or rises with the air that has been warmed by condensation.
Water droplets floating in a high density are presumably attractive to each other in the same way as soap
bubbles on water surfaces. The droplets become bigger, while the total surface area of the drops in relation

to their volume becomes smaller. During this process,
energy is released, and a horizontal air movement towards the centre of the cloud is created. The larger
droplets do sink, but are drawn upwards by the
warmed air. The upper side of the cumulus cloud has a
sharp outline (illus. 10). Cumulus clouds normally reach
unsaturated zones, where the external droplets evaporate, cooling in the process. Their molecules take up
distancing positions again. The cloud loses its sharp
outline, it becomes »tattered«, grows old, and »thaws
out«, sometimes to the point that it disappears. When
clouds rise to a great height, as with thunder clouds,
they reach zones which are so cold that their water
droplets abruptly freeze to form extremely fine ice crystals, which clearly and visibly tend away from each
other, forming the »big hat« of the »anvil«, which often
forms a cirrostratus cloud which covers the sky, but
also frequently disappears.
Water droplets within the cloud become ever bigger
in the up draft. If the speed with which the air is rising
and the speed with which the ice particles are falling
balances out for a time they may freeze to form hailstones, reaching abnormal size. Winter snow on the
other hand is usually created when warm rain falls
through a zone of cold air close to the ground. Dry
snowflakes fall in a distanced way, wet snowflakes fall
in clumps.


Distancing occupations can be empirically predicted or
outlined relatively well. Positions at the furthest distance
to each other can be quickly identified 
However, in this example, the occupation of an island, occupant number 5 must remove himself when
occupant number 6 arrives 
He must move again when occupant number 7 assumes his relatively stable position 
If further occupants are added, a pattern of occupation is formed which primarily depends on the form of
the surface that can be occupied, and secondarily depends on the sequence of occupation (illus. 12E).
The sequence of occupation plays a larger role if migration of occupations is prevented or simply hindered.
As has already been mentioned, distancing occupations can frequently be observed in animals who maintain a distance from others and in people.
Studying the occupation of surfaces in field experiments is certainly a worthwhile exercise for behavioural
studies, urban development and architecture. Cameras
on the ceilings of tall meeting rooms and guest houses
and on towers overlooking beaches and open air swimming pools and ongoing mapping of the territories of
lone predatory animals all present opportunities for this.
Logging the nests of bird species which settle in a distancing way in solitary trees and woodland in order to
comprehend spatial settlement mechanisms in nature is
particularly demanding, but no less informative.

The preceding demonstrations will be expanded on,
with further details and experimental plans in the field of
distancing and attractive occupation.


When a restaurant is being occupied (illus. 11.1), the
corners are preferred (illus. 11.2). The places along the
walls or in the central area are only filled afterwards (illus. 11.3 and 4). As the room fills up, the shifting of
chairs and tables can be observed as seating is moved
(illus. 11..5). This ceases once the room is fully occupied
(illus. 11.6). Occupation has temporarily become static,
but only until chairs become free again.


The experimental apparatus
This device was developed by us to directly investigate
distancing occupations. Small rod magnets float on water, each with the same pole uppermost (illus. 13). The
magnets repel each other and move away from each
other. They adopt a form of occupation which can be
described as distancing (illus. 14).
Ordinary steel needles which have been passed over
a permanent magnet and thereby magnetised are used.
Small balls of polystyrene serve as buoys. Beneath the
water, a template marks the surface which can be occupied. The best results are achieved when the template is approximately in the centre of the group of
magnetised needles. The exactitude with which the distancing occupation establishes itself is remarkable.

Once they have reached their position, the magnets remain there as if held in place by invisible threads, even
when violently disturbed. They can remain in this position for days.
The photos with which the experimental results are
documented were taken using a very simple apparatus
(illus. 15). It is best to cover the feet of the tripod in
black so that they do not reflect in the water.
When making more detailed observations, it may be
advisable to use the minimal path apparatus developed
by the Institut für leichte Flächentragwerke (IL) as a recording apparatus.
So far, too few experiments using this apparatus
have been able to be made. However, it appears to be
universal. One can place any number of points, up to
about a hundred, on any surface. In addition, varying
the magnetic field strength can alter the distances,
thereby increasing and reducing the associated territories. When carrying out the experiments, changes in
field strength are very easily achieved by placing two or
more needles on a buoy (illus. 16).
The experiments previously carried out with this simple apparatus do not as yet lead to a definite conclusion. It could be observed that when points were
added, the whole movement system changed, unless
the occupiable space had sharp corners. The so-called
occupiers almost always remain in these corners.
It can further be observed that as the number of
points increases, they arrange themselves into a regular
triangular grid, which however is usually disturbed at
the edges by the boundaries of the surface (illus. 17).
This disruption does not take place if the occupied
space is triangular or hexagonal and the points are free
to arrange themselves in a triangular grid.
Time exposure allows the migration of the points to
be captured. When one or several points are added or
removed, migration results.
The floating magnetic needles in a triangular frame
(illus. 18) and in a rectangular frame (illus. 19) present
their south poles uppermost. The needles in the water
can be seen.


 Introduction: Strangement as a literary device
The narrator of Leo Tolstoi’s ​Strider​ is a horse. The Rusian writer instrumentalises the
equine nature of its singular raconteur as a literary device of defamiliarisation: its distanced
and at times deliriant point of view on ordinary human subjects produces what Viktor
Shklovsky defines​1​ as an effect of ​enstrangement​. Strider’s astonishment while describing
the concept of property is emblematic:
“Such are the words "my" and "mine," which they apply to different things, creatures, objects,
and even to land, people, and horses. They agree that only one may say "mine" about this,
that or the other thing. And the one who says "mine" about the greatest number of things is,
according to the game which they've agreed to among themselves, the one they consider
the most happy. I don't know the point of all this, but it's true.”​2
Tolstoi’s equine narrator defamiliarizes the common notion of property; the latter's practice is
converted into a “game which they’ve agreed to among themselves” and whose existence
seems to be pointless despite being “true”. This is precisely the target of Shklovsky’s
enstrangement​: “to make objects “unfamiliar”, to make forms difficult, to increase the
difficulty and length of perception because the process of perception is an aesthetic end in
itself and must be prolonged”.​3​ The work of art is considered in sheer opposition with daily
life; while the latter is based in the unconscious automatisms characteristic of our habits and
the immediacy of its action, the former is, above all, a process of retardment: it
over-complexifies, and, thus, extends, the act of perception beyond what common life and its
habitualization would require.
Tolstoi recursively instrumentalizes this literary device through other strategies: in ​War and Peace​ (1864) he describes the well-known idea of “battle” as if it would be an entirely unprecedented practice, and, later, in ​Shame! (​ 1895), he defamiliarizes the act of flogging by proposing abnormal manners of harming without transforming flogging’s essence. Bertolt Brecht coined the german expression ​Verfremdunseffekt​ in order to refer to this distancing effect. Despite Brecht instrumentalized a different set of alienation techniques, his theater
aimed at estrangement by “playing in such a way that the audience was hindered from
simply identifying itself with the characters in the play. Acceptance or rejection of their
actions and utterances was meant to take place on a conscious plane, instead of, as
hitherto, in the audience’s subconscious”​4​. Defamiliarization is then used in order to prevent
the audience from bathing themselves in empathetic emotions, forcing instead the viewers
into a critical frame of mind. Thus, and in contrast with Aristotle’s ​catharsis​, estrangement
precludes the identification of the audience with the fictional characters, promoting an
Viktor Shklovsky, “Art as Technique” in​ Twentieth-century Literary Theory: A reader,​ ed ​K. M.

attitude of intellectual attention that opposes unconscious habits and automatic
entertainments.
In ​Strider, Tolstoi obtains this distancing effect through the equine logic of its narrator. What
is relevant for Tolstoi is neither the proximity that the horse could have to the human being,
nor the mistakes that it could make when attempting to explain human life. What is crucial for
Tolstoi is to speculate on the different lenses of animal’s logic. This implies a twofold
movement. First, Tolstoi positions the zoological being in the same ontological footage as
the human being by considering the former's narrative as valid as that of the latter. Second,
he celebrates the zoological difference characteristic of a horse by instrumentalizing it as a
literary device of estrangement that traverses the whole plot.
Tolstoi’s operative assumption of this zoological alterity is particularly contemporaneous.
Today our cultural landscape is populated by a series of others whose ecological, biological,
geological or algorithmic nature can no longer be reduced to that of the human. The latter
has lost its modern condition of subject, that is, its condition of being an ontologically
privileged object. In this scenario, the human has become “a nomad assembly in a shared
life space that it does not control or own. S/he just occupies it, always in community with
biological, technological and cultural others.”​5 ​The discipline of architecture is no exception:
in the last years, the arrival of artificial intelligence (AI) in architectural design is profoundly
altering latter's method and performance. Despite the uncertainty that orbits around its
generalisation in our daily practice, in most cases, design processes associated with AI are
intimately connected with a Prometheic agenda particularly concerned with notions such as
improvement, optimization, automatisation, efficiency, prediction, monitoring, adaptation,
calculation, or self-regulation. However, Tolstoi’s instrumentalization of Strider’s zoological
otherness suggests that another term, associated with a non-positivistic mode of action,
could be added to this list: estrangement. This does not mean that Tolstoi is a 21st century
intellectual ​avant la lettre.​ His obsessive misogyny, for example, precludes this honor. What
it means is that ​Strider’​ s​ i​ nstrumentalization of zoological otherness as a literary device of
strangement resonates with today’s xenological condition and might be useful in order to
complexify the role of AI in experimental architecture beyond its Prometheic register.
Perception’s de-automatization implied by processes of estrangement could be particularly
relevant for the discipline of architecture, as soon as it could challenge what has been
defined as the architectural mode of perception ​par excellence:​ Walter Benjamin’s
“distracted perception”.

The Xenological Condition: algorithmic otherness
The term ​xenology ​refers to the dense and intimate (dis)entanglement between organic and
inorganic beings that are no longer subordinated to the human, but attuned to a vast litany of
species. In spite of its wide speculative vocation, the concept of xenology is rooted in the
immediacy of our here and now: the hole in the ozone layer, frozen seed banks, global
pandemics, xenobots, ocean garbage patches or transgenic animals constitute some of
those strange objects invading our world while circumventing the epistemic categories
characteristic of Modernity. Bastard creatures conforming a vast litany of multispecie entanglements, eclectic composites and hybrid processes resulting from intimate relations
between strangers, between “others”, between ​xenos​.
The xenological condition accounts thus for a new sensibility towards the biological,
ecological, algorithmic and geological other: intelligent robots, planetary viruses, digital
boots, human-made earthquakes, transgenic plants…; the otherness evoked by these
creatures is no longer that of a “Galilean object” and its perpetual human subjugation, but
that of an emancipated active being, a “Lovelockian agent”​6​ that mutates and evolves in
community with different forms of life: human and nonhuman, organic and machinic, cultural
and natural. Thus, far from the invariably vampirized mark of alterity of classical philosophy
or the fetishized and necessarily othered other of deconstruction, the other is understood
here as “a moving horizon of exchanges and becoming, towards which the non-unitary
subjects of postmodernity move, and by which they are moved in return”​7​.
In contemporary computational architecture, the other ​par excellence​ is the algorithmic
other, characterised not just by its autonomy, but also by its ubiquitous presence both in
physical and virtual bodies. Faced with a dual ontology that is no longer alluding to
Heideggerian human nudity but to a planet inhabited by algorithmic beings that live with and
against us, Eric Sadin defines this technological scenario as antrobological. This notion
expresses the “increasingly dense intertwining between organic bodies and “immaterial elfs” The propagation of artificial intelligence and the multi-scalar robotization of
the organic establishes, in addition to a change of medium, a change of condition: its
algorithmic power does not merely offer itself as an automatic pilot for daily life, but it also
triggers a radical transformation of our human nature, setting up a perennial and universal
intertwining between bodies and information; if in the 1990’s bits were associated with
atoms, bits are now associated with cells such as neurons, myocytes or keratinocytes. In this
sense, the multidisciplinary generalization of machine learning, the progress in genetic
engineering or the robotization of the mundane no longer refer to a humanity that is merely
extended, but to a humanity that is expanded, that is, inclined, or better, deviated: it is woven
by algorithmic (and biologic or ecologic) agents whose symbiosis is not only metaphorical or
narrative, but performative. Under this scenario, “artificial extelligence” becomes “artificial intelligence” through a process of in-corporation: intelligence, the ​eidos​, what has
traditionally been understood as form, is no longer an external entity that articulates
(in)organic bodies from outside, but it is carnally embedded in them.
However, rather than as a legendary form of alien cognition ​in silico,​ it is more reasonable to
consider machine learning as an instrument of knowledge magnification that performs
pattern recognition​9​. 
neural networks and head of Facebook AI, argues that current AI systems are not
sophisticated versions of cognition, but of perception.​10​ To study the impact of AI is then to
study the manner and degree by which information flows are absorbed and treated by AI,
and, more in particular, to study the algorithmic anatomy of the statistical models underlying
machine learning. Matteo Pasquineli affirms that “from the numerical perspective of machine
learning, notions such as image, movement, form, style and decision can be all described as
statistical distributions of a pattern”​11​. And, he continues, from this point of view, three steps
are given:
First, the “training data”, that is, the training dataset containing data to be analysed in order
to extract knowledge and “intelligence”, that is, patterns of association. This extraction
occurs in the second step, defined by Pasquineli as “learning algorithm”: it constructs a
statistical model of the previous patterns of association. Finally, there is the “model
application”, the instrumentalization of the statistical model once it is considered sufficiently
trained in different tasks, such as classification or prediction.
In the light of Pasquineli’s assertions, could we then define artificial intelligence as
intelligent? It, of course, depends on how we understand the term. Its etymology offers us
some insights; intelligence, from the latin ​intelligentia,​ composed of the terms ​inter a
​ nd
legere​. While the former refers to the notion of “in between”, the latter's significance is more
complex: on the one side, ​legere ​means to choose (​scegliere,​ in italian), but, on the other
side, ​legere ​means also to read (​leggere​, in italian). While the former would associate
intelligence with the capacity of “choosing (​legere)​ in between (​inter​)”, the latter would
associate intelligence with the capacity of “reading (legere) in between (inter)”. Thus, in the
first case, an agent is intelligent if it is capable of choosing the best option between many,
and, in the second case, an agent is intelligent if it is capable of reading what lies in
between, as, for example, reading in between lines.
It is particularly difficult and largely debatable if artificial intelligence processes are indeed
performing any of those two activities or if they can be reduced, as Dan McQuillan argues, to
“simply mathematical minimisation”.​12​ ​This epistemological demystification is actually crucial in order to secularize Artificial Intelligence from the auratic narrative of autonomy that
underpins a neoliberal practice worldwide: the growth of a geopolitical regime of high-tech companies invisibilizing workers’ autonomy through a corporate apparatus of knowledge
extractivism and epistemic colonialism. However, with or without intelligence, AI, as an
instrument of knowledge magnification, perceives and operates with patterns that are
beyond the reach of the human mind​13​.


There is no other way out for the
philosopher Ð who, regarding human beings
and their play in the large, cannot at all
presuppose any rational aim of theirs Ð
than to try whether he can discover an aim
of nature in this nonsensical course of
things human; from which aim a history in
accordance with a determinate plan of
nature might nevertheless be possible even
of creatures who do not behave in
accordance with their own plan É [Nature]
did produce a Kepler, who subjected the
eccentric paths of the planets in an
unexpected way to determinate laws, and a
Newton, who explained these laws from a
universal natural cause.7

that it reproduces another tree. Second, the tree
produces itself as an individual; it absorbs
energy from the environment and turns it into
nutrients that sustain its life. Third, different
parts of the tree establish reciprocal relations
with one another and thus constitute the whole;
as Kant writes, the Òpreservation of one part is
reciprocally dependent on the preservation of
the other parts.Ó11 In such a totality, a part is
always constrained by the whole, and this is true
of KantÕs understanding of cosmopolitical
wholeness as well: ÒAll states É are in danger of
acting injuriously upon one another.Ó12 Nature is
not something that can be judged from a
particular point of view, just as the French
Revolution cannot be judged according to its
actors. Rather, nature can only be comprehended
as a complex whole, and the human species, as
one part of it, will ultimately progress towards a
universal history that coincides with the
teleology of nature.13
ÊÊÊÊÊÊÊÊÊÊHere we only want to show that as Kant
develops his thinking towards universalism, his
conceptualization of the relation between
cosmopolitics and the purposiveness of nature is
situated within a peculiar moment in history: the
simultaneous enchantment and disenchantment
of nature. On the one hand, Kant recognizes the
importance of the concept of the organic for
philosophy; discoveries in the natural sciences
allowed him to connect the cosmos to the moral,
as indicated by his famous analogy near the end
of Critique of Practical Reason: ÒTwo things fill
the mind with ever new and increasing wonder
and awe, the more often and constantly
reflection concerns itself with them: the starry
heavens above me and the moral law within
me.Ó14 Howard Caygill makes an even stronger
claim, arguing that this analogy points to a
ÒKantian physiology of the soul and the cosmosÓ
that unites the Òwithin meÓ (freedom) and the
Òabove me.Ó15 On the other hand, as we saw in
KantÕs citation of Kepler and Newton in ÒIdea for
a Universal History with a Cosmopolitan Aim,Ó
the affirmation of Òuniversal historyÓ and
advancements in science and technology led in
the eighteenth century to what RŽmi Brague calls
the Òdeath of the cosmosÓ:


New discoveries in the natural sciences thanks
to the invention of the telescope and the
microscope exposed human beings to
magnitudes they could not previously
comprehend, leading us to a new relation with
the Òentire span of natureÓ (in dem ganzen
Umfang der Natur).17 The Kantian scholar Diane
Morgan suggests that through the Òworlds
beyond worldsÓ revealed by technology, nature
ceases to be anthropomorphic, for the relation
between humans and nature is thus reversed,
with humans now standing before the
Òunsurveyable magnitudeÓ (Unabsehlich-Gro§) of
the universe.18 However, as we indicated above,
there is a double moment that deserves our
attention: both the enchantment and
disenchantment of nature via the natural
sciences, leading to a total secularization of the
cosmos.
In addition to the revelation of nature and
its teleology through technical instruments,
technology also plays a decisive role in KantÕs
political philosophy, when he asserts that
communication is the condition of the realization
of the organicist whole. Arendt made explicit the
role of the sensus communis in KantÕs
philosophy, as both the question of community
and consensus.19 But such a sensus communis is
achieved only through particular technologies,
and it is on this ground that we should
problematize any naive discourse on the
common as something already given or
preceding technology. The age of Enlightenment,
as noted by Arendt (as well as Bernard Stiegler),
is the age of Òthe public use of oneÕs reason,Ó and
this exercise of reason is expressed in the
freedom of speaking and publishing, which
necessarily involves the technology of printing.
On an international level, in ÒToward Perpetual
Peace: A Philosophical SketchÓ Kant writes that
Òit was trade that first brought them into
peaceful relations with one another and thereby
into relationships based on mutual consent,
community, and peaceful interactions even with
remote peoples,Ó later adding, Òit is the spirit of
trade, which cannot coexist with war, which will,
sooner or later, take hold of every people.Ó20
¤2. ÒOntological TurnÓ as Cosmopolitics
This reiteration of Kantian cosmopolitanism is an
attempt to demonstrate the role of nature in
KantÕs political philosophy. Kant somehow
assumes one single nature, which reason truth about it É Can we still speak of
cosmology? It seems that the West ceased
to have a cosmology with the end of the
world of Aristotle and Ptolemy, an end due
to Copernicus, Galileo, and Newton. The
ÒworldÓ then no longer formed a whole.

compels us to recognize as rational; the
rationality corresponds to the organicist
teleological universality ostensibly realized in
the constitution of both morality and the state.
This enchantment of nature is accompanied by a
disenchantment of nature, driven by the
mechanization enforced by the Industrial
Revolution. BragueÕs Òdeath of the cosmosÓ
brought about by European modernity and its
globalization of modern technology necessarily
forms one of the conditions for us to reflect on
cosmopolitics today, insofar as it illustrates the
inefficacy of a biological metaphor for
cosmopolitanism. If we start with Kant rather
than with more recent discussions on
cosmopolitanism Ð such as Martha NussbaumÕs
rootless cosmopolitanism, HabermasÕs
constitutional patriotism, or Anthony AppiahÕs
cosmopolitan patriotism21 Ð it is because we
want to reconsider cosmopolitanism by
examining its relation to nature and technology.
In fact, AppiahÕs rooted cosmopolitanism is
relevant to our discussion below. He holds the
view that cosmopolitanism denies the
importance of affiliations and particular
loyalties; this means that it is necessary to
consider cosmopolitics from the point of view of
locality. This crucial point is the reason I would
like to engage with the idea of Òmulti-naturalismÓ
recently proposed by anthropologists associated
with attempts to present a new way of thinking
cosmopolitanism.
The Òontological turnÓ in anthropology is a
movement associated with anthropologists such
as Philippe Descola, Eduardo Viveiros de Castro,
Bruno Latour, and Tim Ingold, and earlier, Roy
Wagner and Marilyn Strathern, among others.22
This ontological turn is an explicit response to
the crisis of modernity that expresses itself
largely in terms of ecological crisis, which is now
closely associated with the Anthropocene. The
ontological-turn movement is an effort to take
seriously different ontologies in different
cultures (we have to bear in mind that knowing
there are different ontologies and taking them
seriously are two different things). Descola has
convincingly outlined four major ontologies,
namely naturalism, animism, totemism, and
analogism.23 The modern is characterized by
what he calls Ònaturalism,Ó meaning an
opposition between culture and nature, and the
formerÕs mastery over the latter. Descola
suggests that we must go beyond such an
opposition and recognize that nature is no longer
opposed or inferior to culture. Rather, in the
different ontologies, we can see the different
roles that nature plays; for example, in animism
the role of nature is based on the continuity of
spirituality, despite the discontinuity of
physicality.

A funny thing to see these days is how all
these absurd modern leftists, all unable to
see anything, all lost in themselves, all
feeling so bad, all desperately trying to exist and to find their existence in the eyes
of the Other Ð how all these people are
jumping on the Òsavage,Ó the Òindigenous,Ó
the ÒtraditionalÓ in order to escape and not
face themselves. I am not speaking of being
critical towards oneÕs Òwhiteness,Ó towards
oneÕs Òmodernism.Ó I am talking of the
ability to peer inside [transpercer] oneself.
My refusal of the above two extremes does not
come out of any postcolonial Òpolitical
correctness,Ó but rather out of an attempt to go
beyond postcolonialismÕs critique. (Indeed, I have
elsewhere reproached postcolonialism for its
failure to tackle the question of technology.24) I
hold the thesis that an ontological pluralism can
only be realized by reflecting on the question of
technology and a politics of technology. Kant was
aware of the importance of technology in his
comment on trading as communication; however,
he didnÕt pay much attention to the technological
difference that finally led to planetary
modernization, and now planetary computation,
since what was at stake for him was the question
of the whole that absorbs all differences. Kant
criticized the impolite guests, the greedy
colonizers who brought with them Òoppression of
the native inhabitants, the incitement of the
different states involved to expansive wars,
famine, unrest, faithlessness, and the whole
litany of evils that weigh upon the human
species.Ó25 Commenting on the defense
strategies of China and Japan, Kant said that
both countries have

In Beyond Culture and Nature, Descola has
proposed an ontological pluralism that is
irreducible to social constructivism. He suggests
that recognizing these ontological differences
can serve as an antidote to the dominance of
naturalism since the advent of European
modernity. But does this focus on nature (or the
cosmos, we might say) in the interest of opposing
European naturalism actually revive the
enchantment of nature, this time in the name of
indigenous knowledge? This seems to be a
hidden problem with the ontological-turn
movement: many anthropologists associated
with the ontological turn have focused on the
question of nature and the politics of the
nonhuman (largely animals, plants, minerals,
spirits, and the dead). This is evident when we
recall that Descola proposes to call his discipline
an Òanthropology of nature.Ó Furthermore, this
tendency also suggests that the question of
technics is not sufficiently addressed in the
ontological-turn movement. For example,
Descola talks often of practice, which may
indicate his (laudable) desire to avoid an
opposition between nature and technics; but by
doing so, he also obscures the question of
technology. Descola shows that analogism,
rather than naturalism, was a significant
presence in Europe during the Renaissance; if
this is the case, the ÒturnÓ that took place during
European modernity seems to have resulted in a
completely different ontology and epistemology.
If naturalism has succeeded in dominating
modern thought, it is because such a peculiar
cosmological imagination is compatible with its
techno-logical development: nature should be
mastered for the good of man, and it can indeed
be mastered according to the laws of nature. Or
put another way: nature is regarded as the
source of contingency due to its Òweakness of
concept,Ó and therefore it has to be overcome by
logic.

These oppositions between nature and
technics, mythology and reason, give rise to
various illusions that belong to one of two
extremes. On the one hand, there are rationalists
or ÒprogressivistsÓ who hysterically struggle to
maintain their monotheism after having
murdered god, wishfully believing that the world
process will stamp out differences and
diversities and lead to a Òtheodicy.Ó On the other
hand, there are left intellectuals who feel the
need to extol indigenous ontology or biology as a
way out of modernity. A French revolutionary
thinker recently described this situation thus:

wisely, limited such interaction. Whereas
the former has allowed contact with, but
not entrance to its territories, the latter has
allowed this contact to only one European
people, the Dutch, yet while doing so it
excludes them, as if they were prisoners,
from associating with the native
inhabitants.
When Kant wrote this in 1795, it was too early for
him to anticipate the modernization and
colonization that would take place in Japan and
China.
I propose to go beyond the notion of cosmology;
instead, it would be more productive to address
what I call cosmotechnics. Let me give you a
preliminary definition of cosmotechnics: it is the
unification of the cosmos and the moral through
technical activities, whether craft-making or artmaking. There hasnÕt been one or two technics,
but many cosmotechnics. What kind of morality,
which and whose cosmos, and how to unite them
vary from one culture to another according to
different dynamics. I am convinced that in order
to confront the crisis that is before us Ð namely,
the Anthropocene, or the intrusion of Gaia
(Latour and Stengers), or the ÒentropoceneÓ
(Stiegler), all presented as the inevitable future
of humanity Ð it is necessary to reopen the
question of technology, in order to envisage the
bifurcation of technological futures by
conceiving different cosmotechnics. I tried to
demonstrate such a possibility in my recent book
The Question Concerning Technology in China: An
Essay in Cosmotechnics. As one can gather from
the title, it is an attempt to respond to
HeideggerÕs famous 1949 lecture ÒThe Question
Concerning Technology.Ó I propose that in order
to rethink the project of overcoming modernity,
we must undo and redo the translations of technē, physis, and metaphysika (not as merely
independent concepts but also concepts within
systems); only by recognizing this difference can
we arrive at the possibility of a common task of
philosophy.
Why, then, do I think itÕs necessary to turn to
cosmotechnics? For a long time now we have
operated with a very narrow Ð in fact, far too
narrow Ð concept of technics. By following
HeideggerÕs essay, we can distinguish two
notions of technics. First, we have the Greek
notion of technē, which Heidegger develops
through his reading of the ancient Greeks,
notably the Pre-Socratics Ð more precisely, the
three ÒinceptualÓ (anfŠngliche) thinkers,
Parmenides, Heraclitus, and Anaximander.28 In
the 1949 lecture, Heidegger proposes to
distinguish the essence of Greek technē from
modern technology (moderne Technik).
If the essence of technē is poiesis, or
bringing forth (Hervorbringen), then modern
technology, a product of European modernity, no
longer possesses the same essence as technē
but is rather an ÒenframingÓ (Gestell) apparatus,
in the sense that all beings become standing
reserves (Bestand) for it. Heidegger doesnÕt
totalize these two essences of technics, but nor
does he give space to other technics, as if there
is only a single homogenous Machenschaft after
the Greek technē, one that is calculable,
international, even planetary. It is astonishing
that in HeideggerÕs so-called Black Notebooks
(Schwarze Hefte) Ð of which four volumes have
been published so far Ð we find this note: ÒIf
communism in China should come to rule, one
can assume that only in this way will China
become ÔfreeÕ for technology. What is this
process?Ó29 Heidegger hints at two things here:
first, that technology is international (not
universal); and second, that the Chinese were
completely unable to resist technology after
communism seized power in the country. This
verdict anticipates technological globalization as
a form of neocolonization that imposes its
rationality through instrumentality, like what we
observe in transhumanist, neoreactionary
politics.
My effort to go beyond HeideggerÕs
discourse on technology is largely based on two
motivations: 1) a desire to respond to the
ontological turn in anthropology, which aims to
tackle the problem of modernity by proposing an
ontological pluralism; and 2) a desire to update
the insufficient discourse on technology that is
largely associated with HeideggerÕs critique of
technology. I have proposed that we reopen the
question of technics, to show that one must
consider technics as a variety of cosmotechnics
instead of either technē or modern technology. In
my book, I used China as a testing ground for my

thesis and tried to reconstruct a lineage of
technological thought in China. However, this
task is not limited to China, since the central
idea is that every non-European culture must
systematize its own cosmotechnics and the
history of such a cosmotechnics. Chinese
cosmotechnical thought consists of a long
history of intellectual discourse on the unity and
relation between Qi and Dao. The unification of Qi
and Dao is also the unification of the moral and
the cosmic, since Chinese metaphysics is
fundamentally a moral cosmology or a moral
metaphysics, as the New Confucian philosopher
Mou Zongsan has demonstrated. Mou suggests
that if in Kant we find a metaphysics of the
moral, it is at most a metaphysical exploration of
the moral but not a moral metaphysics, since a
moral metaphysics can only start with the moral.
MouÕs demarcation between Chinese and
Western philosophy situates his conviction that
Chinese philosophy recognizes and cultivates
the intellectual intuition that Kant associated
with knowing the noumenon, even as Kant
dismissed the possibility that human beings
could possess such an intuition. For Mou, the
moral arises out of the experience of the infinity
of the cosmos, which necessitates infinitization
as the condition of possibility for DaseinÕs
finitude.30
ÊÊÊÊÊÊÊÊÊÊDao is not a thing. It is not a concept. It is
not the diffŽrance. In the Cixi of YiZhuan (易傳‧繫辭),
Dao is simply said to be Òabove forms,Ó while Qi is
what is Òbelow forms.Ó31 We should notice here
that xin er shang xue (the study of what is above
forms) is the word used to translate
ÒmetaphysicsÓ (one of the equivalences that
must be undone). Qi is something that takes
space, as we can see from the character and also
read in an etymological dictionary Ð it has four
mouths or containers and in the middle there is a
dog guarding the utensils. There are multiple
meanings of Qi in different doctrines; for
example, in classic Confucianism there is Li Qi
(禮器), in which Qi is crucial for Li (a rite), which is
not merely a ceremony but rather a search for
unification between the heavens and the human.
For our purposes, it will suffice to simply say that
Dao belongs to the noumenon according to the
Kantian distinction, while Qi belongs to the
phenomenon. But it is possible to infinitize Qi so
as to infinitize the self and enter into the
noumenon Ð this is the question of art.
In order to better understand what I mean
by this, we can refer here to the story of the
butcher Pao Ding, as told in the Zhuangzi.
However, we will have to remind ourselves that
this is only an example from antiquity, and a
much larger historical view is necessary to
comprehend it.
Pao Ding is excellent at butchering cows. Heclaims that the key to being a good butcher
doesnÕt lie in mastering certain skills, but rather
in comprehending the Dao. Replying to a
question from Duke Wen Huei about the Dao of
butchering cows, Pao Ding points out that having
a good knife is not necessarily enough; it is more
important to understand the Dao in the cow, so
that one does not use the blade to cut through
the bones and tendons, but rather to pass
alongside them in order to enter into the gaps
between them. Here, the literal meaning of ÒDaoÓ
Ð ÒwayÓ or ÒpathÓ Ð meshes with its
metaphysical sense:

Hence, Pao Ding concludes that a good butcher
doesnÕt rely on the technical objects at his
disposal, but rather on Dao, since Dao is more
fundamental than Qi (the tool). Pao Ding adds
that a good butcher has to change his knife once
a year because he cuts through tendons, while a
bad butcher has to change his knife every month
because he cuts through bones. Pao Ding, on the
other hand Ð an excellent butcher Ð has not
changed his knife in nineteen years, and it looks
as if it has just been sharpened with a
whetstone. Whenever Pao Ding encounters any
difficulty, he slows down the knife and gropes for
the right place to move further.
ÊÊÊÊÊÊÊÊÊÊDuke Wen Huei, who had posed the
question, replies that Òhaving heard from Pao
Ding, now I know how to liveÓ; and indeed, this
story is included in a section titled ÒMaster of
Living.Ó It is thus the question of Òliving,Ó rather
than that of technics, that is at the center of the
story. If there is a concept of ÒtechnicsÓ here, it is
one that is detached from the technical object:
although the technical object is not without
importance, one cannot seek the perfection of
technics through the perfection of a tool or a
skill, since perfection can only be accomplished
by Dao. Pao DingÕs knife never cuts tendons or
bones; instead, it seeks the void and enters it
with ease. In so doing, the knife accomplishes
the task of butchering the cow without
endangering itself Ð i.e., without becoming blunt

What I love is Dao, which is much more
splendid than my skill. When I first began to
carve a bullock, I saw nothing but the whole
bullock. Three years later, I no longer saw
the bullock as a whole but in parts. Now I
work on it by intuition and do not look at it
with my eyes. My visual organs stop
functioning while my intuition goes its own
way. In accordance with the principle of
heaven (nature), I cleave along the main
seams and thrust the knife into the big
cavities. Following the natural structure of
the bullock, I never touch veins or tendons,
much less the big bones!32

and needing to be replaced. It thus fully realizes
itself as a knife.
ÊÊÊÊÊÊÊÊÊÊWhat I have said above is not sufficient to
be formulated into a program, since it is only an
explanation for the motivation behind the much
larger project that I tried to initiate in The
Question Concerning Technology in China. Also,
we must pay attention to the historical
development of the relationship between Qi and
Dao. Specifically, the search for unity between Qi
and Dao has gone through different phases in
Chinese history in response to historical crises
(the decline of the Zhou Dynasty, the
proliferation of Buddhism, modernization, etc.);
it was widely discussed after the Opium Wars of
the mid-nineteenth century, but such a
unification was not resolved due to a very limited
understanding of technology at the time and an
eagerness to look for equivalences between
China and the West. I have attempted to reread
the history of Chinese philosophy not only as
intellectual history, but also through the lens of
the Qi-Dao episteme, with the aim of
reconstructing a tradition of technological
thought in China. As I have emphasized
elsewhere, this question is by no means only a
Chinese affair.33 Rather, every culture must
reflect on the question of cosmotechnics for a
new cosmopolitics to come, since I believe that
to overcome modernity without falling back into
war and fascism, it is necessary to reappropriate
modern technology through the renewed
framework of a cosmotechnics consisting of
different epistemologies and epistemes.
Therefore, my project is not one of
substantializing tradition, as in the case of
traditionalists like RenŽ GuŽnon or Aleksandr
Dugin; it doesnÕt refuse modern technology, but
rather looks into the possibility of different
technological futures. The Anthropocene is the
planetarization of standing reserves, and
HeideggerÕs critique of technology is more
significant today than ever before. The unilateral
globalization that has come to an end is being
succeeded by the competition of technological
acceleration and the allures of war, technological
singularity, and transhumanist (pipe) dreams.
The Anthropocene is a global axis of time and
synchronization that is sustained by this view of
technological progress towards the singularity.
To reopen the question of technology is to refuse
this homogeneous technological future that is
presented to us as the only option.


Aesthetic value is included in virtually all accounts of the
values of biodiversity, but this value is still incompletely
understood. Here I offer an account of the aesthetics of
biodiversity based on the understanding of aesthetics developed by Immanuel Kant. The claim of this analysis is
that (to use Kant’s terminology) while individual organisms may be considered beautiful, biodiversity as a whole
is sublime. This distinction poses challenges and opportunities for those who manage lands for biodiversity value.
Comparison to managing art museums and wine cellars
and a new vision for the role of systematics and taxonomy
offer some insight into the management of the sublime aspects of biodiversity. If our forests were as magnificent and inspiring as the books proclaimed, then why were
none left? It didn’t make sense. Accepting
what she said would have meant acquiescing to something so incongruous as to be unnerving. Would the civilization I belonged
to systematically and maliciously destroy every last copy of Shakespeare’s plays or of
Beethoven’s symphonies? That would have
been unthinkable! The comparison of forests to sets of works of art (oeuvres)
as entities to be preserved and protected by our civilization sets the management problem addressed by this paper: How can aesthetics be used to motivate and guide the
preservation and protection of sets of biodiversity. However, while the analysis of the aesthetics of the natural
world is a well developed topic, comparatively little effort has been devoted to the development of a theory of
the aesthetics of biodiversity per se. Norton (1987) contains an account of the use of aesthetics in motivating the
preservation of biological diversity, but does not clearly
distinguish biodiversity from other aspects of the natural
world. Here we focus in on diversity and concentrate on
the aesthetics of sets of organisms and other biological
entities rather than on individuals at any level of organization. The aesthetics of biological diversity needs its own
theory so that aesthetic considerations can be included in
the work of managing biological diversity. The theory
should be useful to both institutions and individuals: individual humans must be able to appreciate the aesthetics of
biodiversity and they must be able to communicate those
experiences and the conditions which lead to them to insti-

Virtually every account of the value of biological diversity includes aesthetics. For example, Ehrlich and Ehrlich
(1992) and two recent sets of essays (Norton, 1986; Orians et al, 1990) contain various references to the aesthetic value of biological diversity. In addition, the illfated National Biological Diversity Conservation and Environmental Research Act (Congress of the United States,
1988; see also Blockstein, 1988) explicitly includes aesthetics among the reasons why biological diversity should
be preserved as a matter of national policy. Among the
most eloquent statements of the purely aesthetic value of
biodiversity is that of Terborgh (1989):
My mother used to read aloud to my sister
and me when we were children. She read
dozens of things, among them Evangeline
and James Fenimore Cooper. At that impressionable age I was intrigued by the description of America’s primeval forests – majestic trees, deep shadows, hushed stillness descriptions couched in poetic imagery that tutions responsible for managing the appropriate aspects Here I will sketch the outline of a theory of the aesthetof the natural world.
ics of biological diversity. This is a new field in some
ways although it has many close antecedents. I will beThere are at least three reasons why the analysis of the gin by giving an initial account of what could constitute
aesthetics of biological diversity is important.
a biological diversity experience. A set of analogies for
the management of aesthetic resources are then presented
1. Aesthetic Value is Immediate and Durable: Appeals to set the stage and define problems. The central thesis of
to the commodity value of biological diversity often ask the paper is then that Immanuel Kant’s analysis of the aesus to wait upon future discoveries of new drugs et cetera thetics of the beautiful and the sublime provides a fundawhich may or may not happen. The aesthetic value of mental structure for an account of the aesthetics of biologbiological diversity is immediately available and can be ical diversity. Following the development of this thesis, I
evaluated now. Moreover, its value does not rely on any will address the question of the development of terms of
uncertain future condition or state of affairs.
criticism for biological diversity. I will conclude with the
implications of this analysis for systematics, the manage2. Increase Enjoyment and Appreciation: A greater un- ment of biological diversity, and criticism of biological
derstanding of the aesthetics of biological diversity will diversity.
enable more effective teaching of the appreciation of bioWhat is a Biological Diversity Experience?
logical diversity. That is, just as music or art appreciation
courses make art or music more appealing and desirable,
so the teaching of the appreciation of biological diversity Any understanding of the aesthetics of biological diversity
will make it more appealing. This process naturally leads must begin with some account of just what a biological diversity experience is. While the answer to this question is
to greater demand for biological diversity.
far from clear, I will give a preliminary account of some
3. Goal Setting: From a management point of view the elements of a biological diversity experience derived primost important aspect of the aesthetics of biological diver- marily from introspection and some discussion with othsity is that aesthetic considerations must be used in setting ers. Biological diversity, as currently conceived, gives a
the goals for the management of biological diversity. Al- focus to consideration of the natural world that is expliclan Savory (1988) quotes Albert Einstein: “Perfection of itly concerned with variety. That is, biodiversity concerns
means and confusion of goals seem, in my opinion, to the properties of a set, not the properties of the members
characterize our age” and argues that goals in natural re- of the set. It is the variety per se, not the individual elesource management are not to be set by science. Yet most ments, that are of interest (although the relations between
managers naturally want to use science as the means to the elements of diversity and the diversity itself are not
achieve biological diversity. This frequently leads to what simple). Similarly it is the aesthetics of the variety with
I will call the science means hypothesis which states that which we are concerned.
if we manage biological diversity according to the best
scientific means the other possible goals, such as aesthet- We must consider other experiences which are similar to,
ics, will take care of themselves. This hypothesis may be or could be confused with, a biological diversity experitrue, but I feel it would be unwise not to examine it. If ence. We may consider three of these that already have
the goal of managing for biological diversity is taken to their own aesthetics relatively well developed. The exambe management for as much biological diversity as possi- ples given below are examples of containers of biological
ble and it is possible to develop management guidance for diversity, not the diversity itself.
that goal, then the science means hypothesis may well be
true. However, if (as is often the case) limited resources A nature experience: The writing of Gilbert White
and opportunities force us to make decisions and choices (1789) can be taken as a prototype for the description of
among alternatives, then that goal provides no manage- an experience of nature. Here the general features of an
area and some of the more conspicuous species are dement guidance and the hypothesis is almost surely false.
scribed as they appear on a particular day or season in a
Aesthetic goals are obviously not the only goals necessary particular year. Day to day, season to season, and year to
for devising a management strategy, but if rigorously un- year differences are important, but a sense of the entirety
derstood could provide more management guidance. The of the diversity of life is lacking.
lack of understanding of the aesthetics of biological diversity at present hampers our ability to develop such goals A wilderness experience: The quotes of John Terborgh
given above really are more of a response to a wilderness
and consequent management guidelines.
experience than to a purely biological diversity experience (but see the rest of the book which they introduce:
Terborgh, 1989). Wilderness experiences are a particular
kind of nature experience where the nature experienced
is primeval. Wilderness, with its lack of human impact,
places human beings in starker contrast to the rest of creation and perhaps there also “... the presence of the Creator is much more direct” (Pynchon, 1973).

monumental classic Vertebrate Paleontology gives me the
same sublime feeling. Here are listed all of the vertebrate
genera we are aware of that have ever lived. Reading these
generic names aloud is a distinctly biological diversity experience.
Zallinger: Another mode of experiencing biological diversity is through painting. The best example for me is
Rudolph Zallinger’s great dinosaur mural at Yale. Here
representative species and habitats throughout the Mesozoic are depicted. The impact of this mural goes beyond
those simply interested in the particulars of paleontology
(Scully et al, 1990). Its appeal, in part, is through its suggestion of the great sweep of diversity through time.

A landscape experience: Perhaps the opposite in a classificatory sense of a wilderness experience are experiences of the landscapes as we now find them – showing
the effects of human activities to a greater or lesser extent. Concern for the aesthetics of landscape has a long
tradition (Austin, 1811) and recently has been a focus for
the field of geography (Appleton, 1975; Sadler and Carlson, 1982). Like wildernesses, but to a reduced extent, In fact there are many activities which could be described
landscapes are containers for biological diversity.
as being experiences of biological diversity. Bird watching, going to zoos, gardens, museums, pet stores, dog
Experiences that are more focused on the diversity of life shows, reading field guides all have aspects which are exdo not appear to have been described as often or with periences of biological diversity. We do not yet have anyas much sophistication as the more general nature expe- thing like a comprehensive account of these experiences,
riences described above. Below are five personal exam- but it is clear that there are a great variety of them. Many
ples of what seem to have been biological diversity expe- are direct experiences of diversity through our senses, but
riences.
many are also achieved through the use of our imagination.
Herpetological Collecting Trips: I grew up in southern
California in the days when it was a wonderful place to For any of these experiences to be part of a system of
be interested in herpetology. Collecting trips, mostly to management we must be able to talk of them in a way
the desert, had an excitement in the anticipation of the that communicates the variety and value of the experience.
number of different species that could be caught and in An account of the nature of a biological diversity experithe fulfillment of the species that were caught. Collecting ence together with some understanding of a theory of the
trips seem to me to have been (perhaps, until recently) the aesthetics of biological diversity must combine to result
prototypical biological diversity experience. They are cer- in terms of criticism for biological diversity. At present
tainly the most active of biological diversity experiences. these terms are missing and so we have no language for
comparing experiences and for teaching a deeper apprePrice Lists: At the same time in my life a similar kind of ciation of the aesthetics of biological diversity. We must
excitement occurred when I received in the mail the price have such terms of criticism for without them we cannot
list of a (now defunct) company called Quivira Speciali- communicate the experience or learn from the experiences
ties that sold live amphibians and reptiles. The experience of others.
of dreaming over the list of all of the species that I could
Analogous Examples
order and actually ordering some of them focused on the
diversity of the species on the list. I can remember calculating how much it would cost to buy one of every species. Before discussing terms of criticism for biological diversity it will be worthwhile to review the use of terms of
Schmidt Checklist: I obtained my copy of Karl criticism and their relationship to connoisseurship in other
Schmidt’s 1954 Checklist of North American Amphibians domains of aesthetics. These comparisons will also show
and Reptiles before any of the comprehensive field guides the highly practical use of terms of criticism in worldly
were published. Prior to receiving the checklist I had no affairs. In particular, we will compare an art museum and
idea what all of the species of amphibians and reptiles in a wine tasting.
North America were. Reading this book was a sublime
The Art Museum Analogy Consider the problem of
experience. Here were all of the species.
managing an art museum and that of managing a forest for
Romer’s Vertebrate Paleontology: To this day, in the biological diversity. Both managers must decide which
right circumstances, reading the appendix the Romer’s objects to acquire in the face of limited resources: good paintings and good, diverse tracts of forest are both expensive. This is the problem of acquisition. However, once
acquired both the paintings and the tracts of forest must be
managed so as to retain their character indefinitely: This
is the problem of maintenance.
The Problem of Acquisition
On November 20, 1970 the Metropolitan Museum of Art
in New York City paid $5,592,000 for Velazquez’s Juan
de Pareja. At the time this was the most money ever paid
for a work of art. How was this expensive decision made?
Certainly not through any kind of direct market research
or analysis of viewer preferences. In fact the purchase was
kept secret by the officials at the museum for five months
after it was purchased (Tomkins, p. 362-364). Rather it
was fundamentally an aesthetic decision, based on two
cornerstones of art appreciation: history and connoisseurship. This painting is regarded as coming from one of the
giants in the history of Western art and so its historical
importance is assured. Further it has been specifically an
example of the great in painting for all connoisseurs who
have evaluated it. Shortly after it was painted a contemporary reviewer wrote that it alone among the many painting
with which it was shown was the “truth” (Brown, 1986).
The Problem of Maintenance
Having acquired a painting, the management work continues as decisions are made about maintenance and restoration. Once the Vatican acquired the paintings of Michaelangelo on the ceiling of the Sistine Chapel, problems of
maintenance began. These are not trivial as the recent
heated controversy about the restoration of these paintings has revealed. This controversy over maintenance is
also fundamentally an aesthetic issue. As we get further
into the business of restoration ecology similar controversies will arise. For example, should we introduce a related
subspecies into an ecosystem where the native has gone
extinct? The point here is not so much that maintenance
will be controversial (it will in any circumstance), but that
the terms of the debate will be aesthetic. Further that we
may have something to learn from the way art museums
have faced and solved such problems for some time.

to identify which of several wines they prefer and possibly rate them on some arbitrary scale. This is essentially
purely empirical (and possibly statistical) research. Any
attempt to understand why a particular wine is preferred
comes from an analysis of the data collected. This sort
of effort is analogous to the empirical preference research
that has been done on viewers responses to landscapes or
forest scenes (Ribe, 1989). The second method may be
termed “connoisseurship” (as with painting). Here an attempt is made to understand why a particular wine is better as part of the experience of the wine. The means by
which this is done is through the development and application of terms of criticism. For example, the wine
critic Robert Parker (1985) describes the 1979 Chateau
Lagrange as “... a bit too herbaceous and stalky, but once
past the rather unimpressive boquet, the wine shows good
ripe fruit, a supple, soft texture, and a spicy finish.” Here
an individual critic is using a wide selection of words that
have come to take on particular meaning in the description of the aesthetic properties of a wine. These two approaches undoubtedly represent two ends of a spectrum.
Consideration of this analogy between wine and biological diversity emphasizes that we do not now have “critics of biological diversity,” people whose job it is to review instances of biological diversity experiences and relate them in such a way as to improve our appreciation
of the particular cases under review as well as biological diversity in general. The analogy also emphasizes the
fact that large scale businesses do already exist that owe
their success to the understanding of the aesthetics of their
product. Many people believe that aesthetics is somehow
too slippery a slope on which to build a business, or in our
case a management plan. But, in my experience, there is
no one as hardheaded as a Boston wine merchant.

It is worth noting that connoisseurship is not without serious problems. If not continually directed at a large audience it runs the risk of making the subject less accessible
rather than more. Wine critics can easily become wine
snobs, putting everyone off. Terms of criticism can become so recondite that they are meaningless. Already in
1811 the eminent landscape ecologist Jane Austin could
write “The description of the beauty of landscapes has become mere jargon.” These problems can only be avoided
by scrupulous attention on the part of critics to their auThe Wine Tasting Analogy
dience and by trying to make the audience as broad as
possible. It is also the case that connoisseurship, being
The production of wine is another multi-billion dollar in- partly a property of individuals, necessarily contains muldustry that is fundamentally driven by aesthetics. People tiple views.
buy good wine in large part because they like its taste. So
Kant: Critique of Judgement
the determination of preference is crucially important to
the industry. However, there are (at least) two different
visions of how preference is determined. The first may be To begin our search for possible terms of criticism for aescalled “market research” Here customers are simply asked thetics of biological diversity we look to the aesthetic theory ory of Immanuel Kant which has been so influential in
other fields. First published in 1790 and recently translated anew by Pluhar (1987), Immanuel Kant’s Kritik der
Urteilskraft or Critique of Judgment remains one of the
handful of fundamental sources on aesthetics. Kant does
not, of course, directly address the issue of the aesthetics
of biological diversity as we would understand it (but see
the Critique of Teleology, Section 82 where he appears to
be at pains to account for the diversity of life), but he does
consider the aesthetics of the natural world and his general
discussion can be directly applied. From a manager’s perspective we may treat his account as off-the-shelf technology and use it to begin our understanding and application
of aesthetics.
Kant introduces two major divisions of aesthetic experience: The beautiful and the sublime. The aesthetics of
the beautiful is what we would ordinarily recognize and
has been extensively discussed in the aesthetics literature.
The sublime, while a popular topic in the 18th Century
(see Crawford, 1985), has only recently again become the
focus of philosophical work. Crowther (1989) is the most
recent review of Kant’s theory of the sublime, but focuses
mostly on issues surrounding the sublime in art. Crawford (1985) explicitly discusses the sublime in nature and
argues persuasively that the Kantian sublime is alive in the
writings of naturalists ranging from Thoreau to Leopold.
As we shall see it is this insight that leads to a Kantian
theory of the aesthetics of biological diversity. The central idea of this theory is that while individual entities in
the natural biological world may be beautiful (or not), biodiversity as the set of all of these entities is sublime.
The Beautiful
According to Kant beauty makes sense only for objects
possessed of form. An individual butterfly, tree, or nudibranch is beautiful: “The beautiful in nature concerns the
form of the object, which consists in [the objects’] being
bounded” Kant devotes a good deal of effort to solving the
traditional problems of aesthetics – How can we agree on
something subjective, what is taste, what is the relation of
beauty to purpose, etc. These arguments directly address
individual objects in nature, many of which are components of biological diversity, but which are not diversity
in themselves.

nation in a particularly powerful way. Kant further divides
the sublime into what he calls the mathematically sublime
and the dynamically sublime. As translated, these terms
are not particular felicitous, but the ideas behind both apply to biological diversity.
The Mathematically Sublime: The mathematically sublime applies to those things which cause our imagination
to be overwhelmed by the magnitude, immensity, or sheer
numbers. The experience of the failure of our reason to be
able to grasp the numbers of stars in a brilliant night sky
as our imagination considers not only the stars we actually see but also the imagined existence of the ones we do
not. Thus the sublime is not directly sensed (we see some
stars, and think of a nearly infinite number of others), although it can be triggered by what is sensed. We think of
something which can never be sensed and feel the power
of our imagination to transcend the world we see. For biological diversity consider the experience of contemplating
Romer’s list of vertebrate genera and focusing on the immensity of the list and the fact that many more, possibly
a great many more, left no fossil record. Or just consider
a single pre Cambrian fossil, say a stromatolite, and then
reflect on the millions, possibly billions, of species that
have come after it. And then further consider the number
of species that could come into existence in the next two
billion years of evolution on this planet.
Kant argues that we must estimate the magnitude of things
in nature to experience of the sublime. We can do this by
visiting a given piece of land, say a woods, for a given
period of time, say a weekend, and somehow estimating
the amount of biological diversity that we see. We then
consider the ratio of the area of that woods to the surface of the earth and the ratio of that time to the amount
of time that life has been on earth (perhaps two billion
years). Imagining these ratios as they apply to scaling the
total amount of biological diversity that has existed on the
earth can be a sublime experience.
The Dynamically Sublime:

The Sublime

Consideration of the might or overwhelming power of nature leads to the dynamically sublime. Here “dynamical”
means possessed of power, not just varies with time. Contemplation of violent storms, earthquakes, and volcanic
eruptions from a safe place are among the examples that
Kant gives of a dynamically sublime experience.

The central thesis proposed here is that while individual
plants and animals may be beautiful, biological diversity,
as a whole, is sublime. In contrast to the beautiful, the
sublime is to be found in the contemplation of objects that
are formless and unbounded and which strike the imagi-

An important element of this kind of overwhelming power
and its effect on the mind is fear. Individual components
of biological diversity have often been considered fearful.
Consider Patterson’s (1913, p 68-69) account of the man
eating lions of Tsavo.
I have a very vivid recollection of one particular night when the brutes seized a man from
the railway station and brought him close to
my camp to devour. I could plainly hear them
crunching the bones, and the sound of their
dreadful purring filled the air and rang in my
ears for days afterwards.

kinds, with birds singing on the bushes, with
various insects flitting about, and with worms
crawling through the damp earth, and to reflect that these elaborately constructed forms,
so different from each other, and dependent
on each other in so complex a manner, have
all been produced by laws acting around us.
There is a grandeur in this view of life, with
its several powers, having been originally
breathed into a few forms or into one; and
that, whilst this planet has gone cycling on
according to the fixed law of gravity, from so
simple a beginning endless forms most beautiful and most wonderful have been, and are
being, evolved.

Today this strikes us as dated or even quaint, but what
about AIDS? Here is a disease (certainly a component
of biological diversity) that seems to have come from
nowhere and that no one should regard without apprehension of its actual and potential effects. Our fears of the elements of biological diversity (bioadversity) are undoubtedly culturally and historically dependent, but they are always present in some form and when one is no longer conSystematics, Taxonomy and Terms of Criticism
sidered fearful a new one may replace it. This is because
evolution is an extraordinarily powerful force, perhaps as
In this view systematics and taxonomy can be seen as scipowerful as any we know.
ences which lead to the sublime.
It is the extraordinary power for change that is the core of
the sublimity of biological diversity. Life on earth strikes G.G. Simpson (1961) claimed that systematics was as
us as literally vital, far beyond our immediate experience. much an art as a science.
Evolution has risen to the demand for change (in changIt is this intensity and completeness of oring environments) with its own power for and to cope with
dering that the sciences and the arts have in
change. The relationship between biological diversity and
common. It is also, I believe, this degree and
this power for change is the essence of the sublimity of bithese kinds of ordering that are to be considological diversity.
ered as truly aesthetic (in both arts and sciences). It is pertinent here to insist that taxA useful way to consider this relationship is to consider
onomy, which is ordering par excellence, has
Fisher’s fundamental theorem of natural selection as a
eminent aesthetic value. (p. 4.)
metaphor for the relationship of all aspects of biological diversity to the power of evolution. Fisher (1930)
Taxonomy, in any case, is a science that is
showed that the rate of change of fitness of a characmost explicitly and exclusively devoted to the
ter was proportional to the additive genetic variance for
ordering of complex data, and in this respect
that character. Varieties of the actual theorem and a
it has a special, a particularly aesthetic (as
modern population genetic interpretation can be found in
has been said), and (as might be said) almost
Charlesworth (1980). For our purposes we treat the thesuperscientific place among the sciences. (p.
orem as a metaphor for the empowering that biological
5.)
diversity gives to the processes of evolution: The rate of
evolution or adaptation to change is proportional to the
amount of biological diversity at all levels of organiza- Most research in systematics since then has attempted to
tion. The immense amount of biological diversity that ex- eliminate as much of the art as possible and let the comists gives power to evolution as a process which in turn puter do the talking. This continuing attempt at the objectification of taxonomic decisions has its place, but has not
creates more diversity.
been completely successful. Recent work in taxonomic
The idea of the sublimity of biological diversity and its philosophy has emphasized the complex nature of phyrelation to the power of evolution was recognized by Dar- logeny and the necessity of grounding taxonomic deciwin (1959). The celebrated final paragraph of the Origin sions in phylogeny (e.g. de Queiroz and Gauthier, 1994).
of Species seems to me to capture this idea almost per- This view may ultimately lead to regarding taxonomy as
a straightforward product of data and algorithms with arfectly.
guments now centering on the algorithms to be used (e.g.
de Queiroz, Donoghue, and Kim, 1995).
It is interesting to contemplate an entangled
bank, clothed with many plants of many We focus on the other half of Simpson’s characterization
and ask: How can the art of systematics be developed as a
source of terms of criticism for biological diversity? This
view turns the role of systematics inside out. Instead of
taking systematics as given and science as the sole foundational support for decisions about the value and management of biological diversity, we take systematics as
one important method of creating the aesthetic value of biological diversity. That is, we may find the results of taxonomic work to be the trigger of a sublime experience of
biodiversity. Here naming retains its central role: names
are bestowed, not calculated, and naming remains an act
of creation.

sired ones.
The phrase “managing for the sublime” initially may
strike one as an oxymoron. However, it is certainly possible to manage in such as way that the possibility of a sublime experience is more likely. Encouraging systematics
and taxonomy and the institutions which promote them
(museums, herbaria, field stations) is one way.

Another way to manage for sublimity may be to declare
certain areas as forbidden to entry. The biodiversity in
these areas could then only be imagined and through time
could acquire some sublime impact. A similar motivation
may lie behind the government of Nepal’s decision to deWilson (1989) foresaw an expanded vision for systemat- clare certain peaks as forbidden to climbing. Only with
ics:
the existence of eternally unclimbed peaks, does the possibility of imagination remain fully open. Žižek (1996,
The uniqueness of phylogenetic lineages
p. 98) emphasizes this idea of the importance of the inmeans that history counts, and history in turn
completeness of reality to imagination in his analysis of
generates a sense of the sacredness of place,
the Kantian sublime: In order for something “to continue
of life. No one but the systematist can reveal
to give rise to sublime awe, one has to maintain a proper
the extraordinary value of the alcyonacean
distance towards it”.
corals, chytrid fungi, anthribid weevils, sclerogibbid wasps, melostomes, ricinuleids, eleIf biodiversity is indeed sublime and our aesthetic apprephant fish, and so on down the long and enciation of it does not depend on the immediate sensory
chanted roster.”
encounter of all of its elements, but rather through its effect on our imagination, then the question might be raised:
The sublimity of biodiversity is then the connection be- what difference would the extinction of a species make?
tween the list and the sacredness. Thus the argument of The species would still appear on lists. Indeed, the subRenner and Ricklefs (1994) that the compilation of tax- lime aesthetic appreciation of dinosaurs is real despite
onomic lists is unnecessary and even counterproductive their extinction. Does not this view lead to an indifferfor the protection of biodiversity misses the crucial point: ence as to whether a species is actually alive today? To
the lists themselves may be our best access to the sublime quote Fredrick Brooks (1975, p. 9) only slightly out of
experience of biodiversity
context: “The real tiger is never a match for the paper
one, unless actual use is wanted. Then the virtues of reAesthetics and the Management of Biological Diversity ality have a satisfaction all their own.” However, we do
not just return from the sublime to a particular instance
Today management paradigms for biological diversity are of the beautiful. Rather we come to the conditions that
becoming more objective, more scientific and more quan- make the beautiful possible at all. It does not matter that
titative and this is as it should be. In the future, however, any one of us may find real mosquitoes annoying, ugly,
management strategies will become more complex. As or many things other than beautiful. A similar argument
a well understood theory of the aesthetics of biological may be made for the National Endowment for the Arts
diversity is developed, aesthetic considerations will un- (NEA). It does not matter that some individuals may find
doubtedly become part of the management of biological a photograph by Robert Mapplethorpe ugly, shocking, or
diversity, especially in the setting of management goals. boring while others regard it as among the best of recent
Should the resolution of the science means hypothesis art. What matters is that the conditions for the possibility
show that managing by scientific means can produce all for such art exist. Funding the NEA, say, allows us to pardesired goals, then that resolution will have come about ticipate in this condition of possibility. Similarly, funding
only if there is a way of translating from the language of the conditions that allow species to continue to survive, or
goals to the language of means. If the science means hy- otherwise working to that end, allows us to participate in
pothesis turns out to be false, then the language of goals the condition of possibility. In both cases, we participate
will have to be combined with the language of means. In in the joy of creation itself.
either case terms of criticism for biological diversity will
be needed to compare actual aesthetic experiences to deAcknowledgments

Cognition Everywhere:
The Rise of the Cognitive Nonconscious and the Costs of Consciousness
N. Katherine Hayles
A massive shift is underway in our intellectual and cultural formations. Many
different streams of thought are contributing, coming from diverse intellectual traditions,
holding various kinds of commitments, and employing divergent methodologies. The
differences notwithstanding, they agree on a central tenet: the importance of
nonconscious cognition, its pervasiveness and computational potential, and its ability to
pose new kinds of challenges not just to rationality but to consciousness in general,
including the experience of selfhood, the power of reason, and the evolutionary costs and
systemic blindnesses of consciousness.
The implications for interpretation are profound. Interpretation is deeply linked
with questions of meaning; indeed, many dictionaries define interpretation in terms of
meaning (and meaning in terms of interpretation). For the cognitive nonconscious,
however, meaning has no meaning. As the cognitive nonconscious reaches
unprecedented importance in communication technologies, ambient systems, embedded
devices, and other technological affordances, interpretation has become deeply entwined
with the cognitive nonconscious, opening new avenues for exploring, assessing, debating
and resisting possible configurations between interpretive strategies and the cognitive
nonconscious. In a later section I will identify sites within the humanities where these contestations and reconfigurations are most active and comment upon the strategies
emerging there. Whatever one makes of these changes, one conclusion seems
inescapable: the humanities cannot continue to take the quest for meaning as an
unquestioned premise for their ways of doing business. Before we arrive at this point,
some ground clearing of terminology is necessary, as well as consideration of how the
cognitive nonconscious differs from and interacts with consciousness.

Rethinking the Cognitive Nonconscious
One way into understanding the cognitive nonconscious is through Stanisław
Lem’s Summa Technologiae,1 a work that, as far as the Anglophone world is concerned,
has been caught in a time-warp. Published in Polish in 1964, Summa was never
completely translated into English prior to its present appearance in 2013. Lem
presciently understood that our society was facing what he called an “information
barrier,” a deluge of information that would overwhelm scientific and technological
enterprises unless a way was found to automate cognition. He observed that formal
languages, such as mathematics and explicit equations, do not deal well with complexity.
For example, equations for gravitational interactions do not have explicit solutions when
as few as three bodies are involved. Nevertheless, there are many instances when
complex problems are solved effortlessly by nonconscious means. For example, when a
rabbit chased by a coyote leaps over a chasm, the feat would require many equations and
considerable time to solve explicitly, but the animal does it instantly without a single
calculation.
Reasoning that translating tasks into formal languages may be unnecessary for

solving complex problems, Lem proposes a form of evolutionary computation
programmed in natural media, an “information farm” in which systems could
successfully perform cognitive modeling functions without consciousness. He suggests
modeling a dynamic system by first creating a “diversity generator” such as a fastrunning stream carrying along rocks of various sizes. Then, to match the target system’s
momentum, one places some kind of barrier or “sieve” that selects only rocks of a
specific size and velocity. Other “sieves” select for different variables, and the process
continues until the desired match is achieved.2 One could imagine expanding this kind
of modeling by using living cells, which in carrying out division, excretion, and osmosis
employ many different kinds of “sieves” as selection devices. In fact, contemporary
experiments by Leonard Adleman show it is possible to use DNA sequences to solve
complex topological challenges similar to the traveling salesman problem, another
example of how the cognitive nonconscious can be harnessed to arrive at solutions
difficult or impossible to achieve by explicit means.
What kind of processes do such systems entail, and what is implied by calling
them cognitive? First, these systems operate within evolutionary dynamics, that is, they
are subjected to fitness criteria that select certain states out of the diverse range available.
Second, they are adaptive; they change their behaviors as a result of fitness challenges
such as homeostasis for the cell. Third, they are complex, composed of parts interacting
with each other in multiple recursive feedback loops, or what Andy Clark calls
“continuous reciprocal causation.”3 Consequently, they exhibit emergence, results that
cannot be predicted and that exceed the sum of their parts. Fourth, they are “constraint driven,” which implies that the individual agents’ behaviors are guided by simple
instincts or rules that constrain them to certain productive paths, such as the sieves
mentioned above. Generally, they enact the artificial life mantra: “From simple rules to
complex patterns or behaviors.” Together, these properties enable such systems to
perform modeling and other functions that, if they were performed by a conscious entity,
would unquestionably be called cognitive.
To avoid confusion, I will reserve “thinking” for what conscious entities such as
humans (and some animals) do, and “cognition” as a broader term that does not
necessarily require consciousness but has the effect of performing complex modeling and
other informational tasks. On this view, we can say that while all thinking is cognition,
not all cognition is thinking. In this respect the cognitive nonconscious is qualitatively
different from the unconscious, which communicates with consciousness in a number of
ways.4 Accordingly, I will call consciousness/unconsciousness “modes of awareness.”
By contrast, the cognitive nonconscious operates at a lower level of neuronal
organization not accessible to introspection.
Nonconscious cognitive systems are distinct from the processes that generate
them because they show an “intention toward” not present in the underlying material
processes as such. For example, a termite mound is a complex architectural structure that
emerges as a result of pheromone trails laid down by individual termites enacting simple
behavioral rules.5 It has an “intention toward,” namely the protection and preservation of
the colony. Another example is a beehive, an emergent result created when individual
bees position themselves at a certain distance from their fellows and, moving in a circle,
spit wax. Since the adjacent bees are doing the same, the wax lines press against each other and form a hexagon, the polygon with the most efficient packing.6 The “intention
toward” is instantiated in the beehive, the emergent result of individual bees acting as
autonomous individual agents, each of which need only perform a few simple behaviors
to achieve an effect greater than the sum of the beehive’s parts.
In contrast to the cognitive nonconscious, material processes operating on their
own rather than as part of a complex adaptive system do not demonstrate emergence,
adaptation, or complexity. For example, a glacier sliding downhill generally lacks
adaptive behavior (it cannot choose a shady versus a sunny valley), has negligible
emergent capacity, and its path can be calculated precisely if the relevant forces are
known. The distinction between material processes and complex systems may not
always be so clear cut. Indeed, this framework, positing a tripartite structure of conscious
thinking, nonconscious cognition, and material processes, catalyzes boundary questions
about the delineations between categories as active sites for interpretation and debate.
Enlarged beyond its traditional identification with thought, cognition in some
instances may be located in the system rather than an individual participant, an important
change from a model of cognition centered in the self. As a general concept, the term
“cognitive nonconscious” does not specify whether the cognition occurs inside the mental
world of the participant, between participants, or within the system as a whole. It may
operate wholly independently from consciousness, as in the cases of bees and termites, or
it may be part of the larger system such as a human, where it mediates between material
processes and the emergence of consciousness/unconsciousness. Alternatively, it may be
instantiated in a technological device such as a computer. Nonconscious cognition, then operates across and within the full spectrum of cognitive agents: humans, animals, and
technical devices.

The Costs of Consciousness
Along with an expanded sense of cognition come reassessments of consciousness,
the purposes it serves, and the costs it entails. Most researchers recognize (at least) two
levels of consciousness, a lower level called core or primary consciousness, and a higher
level called extended or secondary consciousness. Humans share core consciousness
with other primates and (arguably) a wide range of mammals and other animals as well.
According to Thomas Metzinger7, a contemporary German philosopher, core
consciousness creates a mental model of itself that he calls a “phenomenal self-model”
(PSM) (107); it also creates a model of its relations to others, the “phenomenal model of
the intentionality relation” (PMIR) (301-5). Neither of these models could exist without
consciousness, since they require the memory of past events and the anticipation of future
ones. From these models, the experience of a self arises, the feeling of an “I” that persists
through time and has a more or less continuous identity. The PMIR allows the self to
operate contextually with others with whom it constructs an intentionality relation.
The sense of self, Metzinger argues, is an illusion, facilitated by the fact that the
construction of the PSM and the PMIR models are transparent to the self (that is, the self
does not recognize them as models but takes them as actually existing entities). This
leads Metzinger to conclude, “nobody ever was or had a self” (1). In effect, by
positioning the self as epiphenomenal, he reduces the phenomenal experience of self back
to the underlying material processes. Philosopher of consciousness Owen Flanagan, following William James, tracks a similar line of reasoning: “the self is a construct, a
model, a product of conceiving of our organically connected mental life in a certain
way.”8 Who thinks the thoughts that we associate with the self? According to Flanagan
(and James), the thoughts think themselves, each carrying along with it the memories,
feelings, and conclusions of its predecessor while bearing them toward its successor.
Antonio Damasio holds a somewhat similar view, in the sense that he considers
the self to be a construct created through experiences, emotions, and feelings a child has
as she grows rather than an essential attribute or possession. Damasio, however, also
thinks that the self (illusion though it may be) evolved because it has a functional
purpose, namely to create a concern for preservation and well-being that propels the
organism into action and thus guarantees “that proper attention is paid to the matters of
individual life.”9 Owen Flanagan agrees: consciousness and the sense of self have
functions, including serving as a clearinghouse of sorts where past experiences are
recalled as memories and future anticipations are generated and compared with memories
in order to arrive at projections and outcomes. In Daniel Dennett’s metaphor,
consciousness and the working memory it enables constitute the “workspace” where past,
present, and future are put together to form meaningful sequences.10
Meaning, then, can be understood at the level of core consciousness as an
emergent result of the relation between the PSM and the PMIR—that is, between the selfmodel and the models the self constructs of objects which it has an “intention toward.”
Damasio puts it more strongly; there is no self without awareness of and engagement
with others.11 The self thus requires core consciousness, which constructs the PSM and
the PMIR; without consciousness, a self could not exist. In humans (and some animals),the core self is overlaid with a higher-level consciousness capable of metalevel reasoning,
including interrogations of meanings that call for interpretations.
In addition to concern for the self, a crucial role of consciousness, which occurs at
both the core and metalevel, is creating and maintaining a coherent picture of the world.
As Gerald Edelman and Giulio Torino put it, “Many neuropsychological disorders
demonstrate that consciousness can bend or shrink, and at times even split but it does not
tolerate breaks of coherence.”12 We can easily see how this quality would have adaptive
advantages. Creating coherence enables the self to model causal interactions reliably,
make reasonable anticipations, and smooth over the gaps and breaks that phenomenal
experiences present. If a car is momentarily hidden by a truck and then reappears,
consciousness recognizes this as the same car, often at a level below focused attention.
This very quality, however, also frequently causes consciousness to misrepresent
anomalous or strange situations.
A number of experiments in cognitive psychology confirm this fact. In one nowfamous situation,13 subjects are shown a video of players passing a basketball and are
asked to keep track of the passes. In the middle of the scene, someone dressed in a
gorilla suit walked across the playing area, but a majority of subjects report that they saw
nothing unusual.14 In another staged situation, a man stops a passerby and asks for
directions.15 While the subject is speaking, two workmen carrying a vertical sheet of
wood pass between them, momentarily blocking the view. When they pass, the
interlocutor has been replaced by another person, but the majority of subjects do not
notice the discrepancy. Useful as is the tendency of consciousness to insist on coherence,
these experiments show that one cost is the screening out of highly unusual events. Without our being aware of it, consciousness edits to make them conform to customary
expectations, a function that makes eyewitness testimony notoriously unreliable. Even in
the most ordinary circumstances, consciousness confabulates more or less continuously,
smoothing out the world to fit our expectations and screening from us the world’s
capacity for infinite surprise.
A second cost is the fact that consciousness is slow relative to perception.
Experiments by Benjamin Libet and colleagues show that before subjects indicate that
they have decided to raise their arms, the muscle action has already started.16 Although
Daniel Dennett is critical of Libet’s experimental design, he agrees that consciousness is
belated, behind perception by several hundred milliseconds, the so-called missing halfsecond.17 This cost, although negligible in many contexts, assumes new importance
when cognitive nonconscious technical devices can operate at temporal regimes
inaccessible to humans and exploit the missing half second to their advantage.
Finally there are the costs, difficult to calculate, of possessing a self aware of
itself and tending to make that self the primary actor in every scene. Damasio comments
that “consciousness, as currently designed, constrains the world of imagination to be first
and foremost about the individual, about an individual organism, about the self in the
broad sense of the term.”18 The anthropocentric bias for which humans are notorious
would not be possible, at least in the same sense, without consciousness and the
impression of a reified self that consciousness creates. The same faculty that makes us
aware of ourselves as selves also partially blinds us to the complexity of the biological,
social, and technological systems in which we are embedded, tending to make us think
we are the most important actors and that we can control the consequences of our actions and those of other agents. As we are discovering, from climate change to ocean
acidification to greenhouse effects, this is far from the case.

Neural Correlates to Consciousness and the Cognitive Nonconscious
Damasio and Edelman, two eminent neurobiologists, have complementary research
projects, Damasio working from brain macrostructures on down, Edelman working from
brain neurons on up. Together, their research presents a compelling picture of how core
consciousness connects with the cognitive nonconscious. Damasio’s work has been
especially influential in deciphering how body states are represented in human and
primate brains through “somatic markers,” indicators emerging from chemical
concentrations in the blood and electrical signals in neuronal formations.19 In a sense,
this is an easier problem to solve than how the brain interacts with the outside world,
because body states normally fluctuate within a narrow range of parameters consistent
with life; if these are exceeded, the organism risks illness or death. The markers, sending
information to centers in the brain, help initiate events such as emotions—bodily states
corresponding to what the markers indicate—and feelings, mental experiences that signal
such sensations as feeling hungry, tired, thirsty and frightened.
From the parts of the brain registering these markers emerge what Damasio calls the
protoself, “an interconnected and temporarily coherent collection of neural patterns
which represent the state of the organism, moment by moment, at multiple levels of the
brain” (174). The protoself, Damasio emphasizes, instantiates being but not
consciousness or knowledge; it corresponds to what I have been calling the cognitive
nonconscious. Its actions may properly be called cognitive in my sense because it has an “intention toward,” namely the representation of body states. Moreover, it is embedded
in highly complex systems that are both adaptive and recursive. When the organism
encounters an object, which Damasio refers to as “something-to-be-known,” the object
“is also mapped within the brain, in the sensory and motor structures activated by the
interaction of the organism with the object” (169). This in turn causes modifications in
the maps pertaining to the organism and generates core consciousness, a recursive cycle
that can also map the maps in second-order interactions and thereby give rise to extended
consciousness. Consciousness in any form only arises, he maintains, “when the object,
the organism, and their relation, can be re-represented” (160). Obviously, to be rerepresented, they must first have been represented, and this mapping gives rise to and
occurs within the protoself. The protoself, then, is the level at which somatic markers are
assembled into body maps, thus mediating between consciousness and the underlying
material processes of neuronal and chemical signals.
This picture of how consciousness arises finds support in the work of Nobel Prize
winner neurologist Gerald M. Edelman and his colleague Giulio Tononi.20 Their
analysis suggests that a group of neurons can contribute to the contents of consciousness
if and only if it forms a distributed functional cluster of neurons interconnected within
themselves and with the thalamocortical system, achieving a high degree of interaction
within hundreds of milliseconds. Moreover, the neurons within the cluster must be
highly differentiated, leading to high values of complexity (146).
To provide a context for these conclusions, we may briefly review Edelman’s
theory of neuronal group selection (TNGS), which he calls “neural Darwinism.”21 The
basic idea is that functional clusters of neurons flourish and grow if they deal effectively with relevant sensory inputs; those less efficient tend to dwindle and die out. In addition
to the neural clusters, Edelman (like Damasio) proposes that the brain develops maps, for
example, clusters of neurons that map input from the retina. Neural groups are connected
between themselves through recursive “reentrant connections” (45-50, esp. 45), flows of
information from one cluster to another and back through massively parallel connections.
The maps are interconnected by similar flows, and maps and clusters are also connected
to each other.
To assess the degree of complexity that a functional neuronal cluster possesses,
Edelman and Tononi have developed a tool they call the functional cluster index (CI).22
This concept allows a precise measure of the relative strength of causal interactions
within elements of the cluster compared to their interactions with other neurons active in
the brain. A value of CI = 1 means that the neurons in the cluster are as active with other
neurons outside the cluster as they are among themselves. Functional clusters
contributing to consciousness have values much greater than one, indicating that they are
strongly interacting among themselves and only weakly interacting with other neurons
active at that time.
From the chaotic storm of firing neurons, the coherence of the clusters mobilize
neurons from different parts of the brain to create coherent maps of body states, and these
maps coalesce into what Edelman calls “scenes,” which in turn coalesce to create what he
calls primary consciousness (in Damasio’s terms, core consciousness). Edelman’s
account adds to Damasio’s the neuronal mechanisms and dynamics that constitute a
protoself from the underlying neurons and neuronal clusters, as well as the processes by which scenes are built from maps through recursive interactions between an organism’s
representations of body states and representations of its relations with objects.
It is worth emphasizing the central role that continuous reciprocal causation plays in both
Damasio’s and Edelman’s accounts. Thirty years ago, Humberto Maturana and
Francisco Varela intuited that recursion was central to cognition,23 a hypothesis now
tested and extended through much-improved imaging technologies, microelectrode
studies, and other contemporary research practices.
Let us now turn to the processes by which re-representation occurs. Recalling
Damasio’s strong claim that there is no consciousness without re-representation, rerepresentation is clearly a major function of the protoself, site of the cognitive
nonconscious and the processes that give rise to core and higher consciousness. In his
theory of “grounded cognition,” Lawrence W. Barsalou in an influential article gives a
compelling account of how re-representation occurs in what he calls “simulation,” “the
re-enactment of perceptual, motor, and introspective states acquired during experience
with the world, body, and mind.”24
In particular, sensory experiences are simulated when concepts relevant to those
experiences are processed and understood. He marshals a host of experimental evidence
indicating that such mental re-enactments are integral parts of cognitive processing,
including even thoughts pertaining to highly abstract concepts. The theory of grounded
cognition “reflects the assumption that cognition is typically grounded in multiple ways,
including simulations, situated action, and, on occasion, bodily states” (619). For
example, perceiving a cup handle “triggers simulations of both grasping and functional
actions,” as indicated by fMRI scans (functional magnetic resonance images). The simulation mechanism is also activated when the subject sees someone else perform an
action; “accurately judging the weight of an object lifted by another agent requires
simulating the lifting action in one’s own motor and somatosensory systems” (624). In
order for a pianist to identify auditory recordings of his own playing, he must “simulate
the motor actions underlying it” (624). Perhaps most surprising, such simulations are
also necessary to grasp abstract concepts, indicating that thinking is deeply entwined with
the recall and reenactment of bodily states and actions. The importance of simulations in
higher-level thinking shows that biological systems have evolved mechanisms to rerepresent perceptual and bodily states, not only to make them accessible to consciousness
but also to support and ground thoughts related to them. These thoughts in turn feed back
to affect somatic states. We can now appreciate the emphasis that Damasio places on rerepresentation, for it serves as an essential part of the communication processes between
the protoself and consciousness, and it also invests abstract thought with grounding in
somatic states.
Although the reenactment mechanisms differ with present-day computational
methods, the idea of recursion is central in artificial media as well. Whereas with
biological organisms bodily states provide the basis for higher-level thinking, with
artificial media recursion operates along a hierarchy that moves from simple to complex,
local individual agents operating according to a few simple rules to global systemic
patterns of complexity. For technical devices, an “intention toward” is necessary but not
sufficient: a hammer and a finance trading algorithm are both designed with an intention
in mind, but only the trading algorithm demonstrates nonconscious cognition. What
makes the difference? Non-conscious cognition operates through many of the same strategies employed by biological organisms, including emergence (call it the “termite
strategy”), re-representation (as in grounded cognition), evolutionary dynamics to
bootstrap cognition, and a variety of other mechanisms. Some nonconscious cognitive
devices have sensors and actuators, so they can interact with their environments and
perform actions in the world. Others live in artificial environments structured to judge
performances according to predetermined fitness criteria, allowing only the most
successful agents to propagate into the next generation. Although the range of technical
devices demonstrating nonconscious cognition is too broad to cover here, the next section
will give a sense of their range and diversity.

Technical Devices and the Cognitive Nonconscious
Evolutionary computations, so called because they instantiate evolutionary
algorithms in a variety of artificial media, have by now been extensively studied. John R.
Koza and coauthors have created genetic algorithms to carry out a variety of tasks, for
example designing electric circuits.25 Their work demonstrates typical strategies to
achieve nonconscious cognition. The seed program generates an array of very simple
circuits. The performance of each circuit is tested according to how well it carries out
certain tasks. The most successful are selected and “married” to each other (that is, their
circuits are combined to create hybrids) which are used to create the next generation,
circuits somewhat similar to the parents but with minor variations among the “children.”
The most successful of these are again selected and again propagate with minor
variations, and so on through hundreds or thousands of generations. Eventually circuits
evolve that can achieve what Koza and his colleagues call “human-competitive” results  which they define as designs publishable in a peer-reviewed professional journal or
circuits judged worthy of a patent.
Similar techniques have been used with algorithms designed to compose music.
Such programs are typically given predefined grammars, but they also can modify these
or even create their own grammar. As noted by John A. Maurer in “A Brief History of
Algorithmic Composition,”26 a genetic algorithm system created by David Cope, called
“Experiments in Musical Intelligence” (EMI), works with a large database of different
composition strategies, which it can draw on and/or modify. Creating from scores fed
into it, it can also create its own database and compose based on that. Compositions in
the style of a number of composers have been created in this fashion, including Bach,
Mozart, Brahms, and others. There are also genetic programs that start with a small
number of functions such as transposition, note generation, and creating or modifying
time values. The program then randomly combines these functions, which are judged
according to some fitness value. The most successful are “married,” as with Koza’s
genetic algorithms, and produce “children” which are evaluated against fitness values in
turn to identify the next pair of parents, and so forth.
Recently programs have been developed that act as critics judging how
commercially successful a given composition (or movie) is likely to be. Christopher
Steiner discusses the case of Polyphonic HMI,27 a company that developed algorithms to
evaluate the likely commercial success of a song. The algorithm works by using Fourier
transforms and other mathematical functions to isolate and analyze tempos, melodies,
beats, rhythms and so forth, creating a three-dimensional visualization showing how
similar the song is to songs that have made it big in the past. Mike McCready, creator of the Polyphonic HMI, used the program to assess an album by Norah Jones, then an
unknown artist, and discovered it had extraordinarily large fitness values. Subsequently,
the album went on to sell twenty million copies and won eight Grammy Awards (83).
Other kinds of evolving agents employ learning processes similar to embodied
biological organisms, using their experiences in the world as physical beings to learn,
draw inferences, achieve simple linguistic skills, and interact with humans. Rodney
Brooks’ “Cog,” a head and torso robot, exemplifies this kind of approach (begun in 1994,
Cog was retired in 2003). Brooks advocates what he calls cheap tricks, emergent results
caused by the interactions of different systems within the robot, often giving the
appearance of human-level intelligence without, however, possessing any conscious
awareness.28 Another version of a language-learning device is Tom Mitchell’s “NELL”
(Never-Ending Language Learning), a program that scans “wild” (i.e., unstructured) text
on the internet 24/7 and draws inferences from it with a minimum of human supervision
(http://www.cmu.edu/homepage/computing/2010/fall/nell-computer-that-learns.shtml).29
Less exotic everyday software demonstrating some of the same properties are programs
that draw inferences from databanks about a user’s preferences, for example programs
used by Amazon to make suggestions for future purchases (“We think you might
like . . .”).
In the financial markets, automated trading algorithms, which now account for
about 70 percent of all trades, also operate in highly competitive ecologies.30 The faster
algorithms can detect pending orders from their slower competitors and “front run” their
orders, for example by purchasing a desired stock at a lower price and then, within
milliseconds, turning around and offering it at a slightly higher price, which the slower algorithm now has no choice but to buy at the new price. Such algorithms typically have
several trading strategies from which to choose, and they will opt for the one that yields
the best final result. In addition, capitalizing on market regulations, they also use their
temporal advantages to force other algorithms to be charged fees while raking in rebates
for themselves (the so-called maker and taker fees and rebates).
The example of trading algorithms demonstrates that, when nonconscious
cognitive devices penetrate far enough into human systems, they can potentially change
the dynamics of human behaviors. As Neil Johnson and his collaborators at Nanex argue
(a firm specializing in studying the behaviors of automated trading algorithms), the effect
of automated trading algorithms has transformed the stock market from a mixed humanmachine ecology to a machine-machine ecology.31 As algorithms account for more and
more trades, the major exchanges (now for-profit corporations themselves) shape their
practices accordingly, for example by offering to sell at a premium rack space next to
their servers, thereby shaving milliseconds off the transmission time, a temporal interval
in which further financial advantages can be gained. The exchanges have also multiplied
the kinds of bids that can be submitted, giving algorithms more ways to turn milliseconds
into megadollars.
The ways in which built environments affect human cognition have of course
been extensively studied in architecture, geography, economics, political science, group
psychology, and a host of other fields. It is scarcely news that humans are affected not
only by social exchanges with each other but also by their interactions with their
environments. What is (relatively) new is the extent to which the built environment
instantiates nonconscious cognition; as the number of such devices grows, so do their effects on human systems. Moreover, the effects are not merely cumulative but
exponential, for increasingly devices operate not just singly but ecologically in niches
and groups.
This tectonic shift greatly magnifies the effect of the technical cognitive
nonconscious on human systems with which it interacts. The general trend is for more
and more communication to flow among intelligent devices, and relatively less among
devices and humans. In part this is because of the slow speed at which humans can
process information relative to devices, and in part because the population of devices is
growing much faster than the population of humans. The internet company Cisco
estimates that by 2015, there will 24 billion intelligent devices connected to the internet;
by contrast, the present human population of the planet is estimated at 7.1 billion.
Compared to the rate at which the human use of the internet is growing, the rate at which
intelligent devices are joining the internet is orders of magnitude higher.
As an example, consider the smart house, where the lighting system connects with
the heating system which connects with the entry/exit system and so on.32 Because these
systems are aware of what the others are doing, they achieve a degree of coordination that
has qualitatively different effects on the human occupants than if each was separate.
Another example is the self-driving car, now in development, that has sensors and
actuators capable of monitoring the environment and reacting according. Moreover, this
capability catalyzes the development of smart roads that can communicate directly with
the car systems. Just as human cognition is massively affected by sociality, so the
nonconscious cognition of intelligent devices operates in different ways when devices
connect and communicate with one another.
 Interactions between Humans and the Cognitive Nonconscious of Intelligent Devices
Because computational media operate in microtemporal regimes inaccessible to
humans, some cultural critics are concerned that the “missing half-second” between
perception and conscious awareness may be exploited for capitalistic purposes. A grand
chess master takes about 650 milliseconds to recognize he is in checkmate; most people’s
responses, less finely tuned, require about a second or more for perceptions to register in
consciousness. By contrast, computer algorithms (in automated stock trading, for
example) can operate in the one to five millisecond range, about three orders of
magnitude faster than humans. One of the ways in which the cognitive nonconscious is
affecting human systems, then, is opening up temporal regimes in which the costs of
consciousness become more apparent and more systemically exploitable.
Luciana Parisi and Steve Goodman sketch these consequences in their discussion
of “affective capitalism.”33 “Affective capitalism is a parasite on the feelings,
movements, and becomings of bodies, tapping into their virtuality by investing
preemptively in futurity. Possessed by seductive brand entities you flip into autopilot, are
abducted from the present, are carried off by an array of prehensions outside
chronological time into a past not lived, a future not sensed. We term this mode of
affective programming “mnemonic control,” a deployment of power that exceeds current
formulations of biopower” (164). In terms I have been using, computational media can
address the protoself at time scales below those at which conscious/unconscious modes of
awareness operate, so that by the time they process the protoself’s input, they are already
preconditioned to pay more attention to one consumer brand than to another. The effects are similar to “subliminal advertising” in the 1950s and 1960s, but now, through the rapid
development of computational media, are operating at temporalities, sensory modalities,
and diverse environmental inputs that would have been unimaginable half a century ago.
Mark B. N. Hansen’s forthcoming book Feed Forward addresses in depth the
implications of these temporal effects of twenty-first century media.34
Of course, not all uses of the cognitive nonconscious are exploitive or capitalistic
in their orientations. Often nonconscious cognitive devices are designed to enhance
productivity, open new avenues for research, and increase safety and well-being for
humans immersed in or affected by them, for example in the computational media
essential to the operations of major airports, where they increase safety as well as
throughput. In the case of computational media involved in the digital humanities, faster
processing speeds allow questions to be posed that simply could not have been asked or
answered using human cognition alone. As the digital humanities increasingly penetrate
the traditional humanities, misunderstandings of what computational media can and
cannot do abound, especially among scholars who have made little or no use of
computational media in their own research other than email and internet searches.
The spectrum of humanistic practices altered by the engagement with
computational media is too vast to be adequately discussed here, so I will focus on one
aspect of special interest in this journal issue: the interplay between description and
interpretation. Sharon Marcus, answering critics who contest her and coauthor Stephen
Best’s call for “surface reading,” takes on the charge that “pure” description is
impossible, because every description already implicitly assumes an interpretive
viewpoint determining what details are noticed, how they are arranged and narrated, and what frameworks account for them. Rather than arguing this is not the case, Marcus
turns the tables by pointing out that every interpretation necessitates description, at least
to the extent that descriptive details support, extend, and help to position the
interpretation.35 Although not the conclusion she draws, her argument can be taken to
imply that description and interpretation are recursively embedded in one another,
description leading to interpretation, interpretation highlighting certain details over
others. Rather than being rivals of one another, then, on this view interpretation and
description are mutually supportive and entwined processes.
This helps to clarify the relation of the digital humanities to traditional modes of
understanding such as close reading and symptomatic interpretation. Many print-based
scholars see algorithmic analyses as rivals to how literary analysis has traditionally been
performed, arguing that digital-humanities algorithms are nothing more than glorified
calculating machines. But this implication misunderstands how algorithms function.
Broadly speaking, an algorithmic analysis can be either confirmatory or exploratory. For
confirmatory projects the goal is not to determine, for example, what literary drama falls
into what generic category, but rather to make explicit the factors characterizing one kind
of dramatic structure rather than another. Often new kinds of correlations appear that
raise questions about traditional criteria for genres, stimulating the search for
explanations about why these correlations pertain. When an algorithmic analysis is
exploratory, it seeks to identify patterns not previously detected by human reading, either
because the corpora is too vast to be read in its entirety, or because long-held
presuppositions constrain too narrowly the range of possibilities considered.
One might suppose that algorithmic analyses are primarily descriptive rather than

interpretative, because they typically produce data about what the subject texts contain
rather than what the data mean. However, just as interpretation and description are
entwined for human readers (as Marcus’s argument implies), so interpretation enters into
algorithmic analyses at several points. First, one must make some initial assumptions in
order to program the algorithms appropriately. In the case of Tom Mitchell’s NeverEnding Language Learning project at Carnegie Mellon, mentioned above, the research
team first constructs ontologies to categorize words into grammatical categories. In
Timothy Lenoir and Eric Gianella’s algorithms designed to detect the emergence of new
technology platforms by analyzing patent applications, they reject ontologies in favor of
determining which patent applications cite the same references.36 The assumption here is
that co-citations will form a network of similar endeavors, and will lead to the
identification of emerging platforms. Whatever the project, the algorithms reflect initial
interpretive assumptions about what kind of data is likely to reveal interesting patterns.
Stanley Fish to the contrary, there are no “all-purpose” algorithms that will work in every
case.37
Second, interpretation strongly comes into play when data are collected from the
algorithmic analysis. When Matthew Jockers found that Gothic literary texts have an
unusually high percentage of definite articles in their titles, for example, their
interpretation suggested this was so because of the prevalence of place names in the titles
(The Castle of Otranto, for example).38 Such conclusions often lead to the choice of
algorithms for the next stage, which are interpreted in turn, and so forth in recursive
cycles.Employing algorithmic analyses thus follows a similar pattern to human

description/interpretation, with the advantage that the nonconscious cognition operates
without the biases inherent in consciousness, where presuppositions can cause some
evidence to be ignored or underemphasized in favor of other evidence more in accord
with the reader’s own presuppositions. To take advantage of this difference, part of the
art of constructing algorithmic analyses is to keep the number of starting assumptions
small, or at least to keep them as independent as possible of the kinds of results that
might emerge. The important distinction with digital humanities projects, then, is not so
much between description versus interpretation but rather the capabilities and costs of
human reading versus the advantages and limitations of nonconscious cognition.
Working together in recursive cycles, conscious analysis and nonconscious cognition can
expand the range and significance of insights beyond what either alone can accomplish.

Staging the Cognitive Nonconscious in the Theater of Consciousness
If my hypothesis is correct about the growing importance of the cognitive
nonconscious, we should be able to detect its influence in contemporary literature and
other creative works. Of course, since these products emerge from
conscious/unconscious modes of awareness, what will be reflected is not the cognitive
nonconscious in itself, but rather its restaging within the theater of consciousness. One of
the sites where this staging is readily apparent is in contemporary conceptual poetics.
Consider, for example, Kenneth Goldsmith’s “uncreative writing.” In Day, Goldsmith
re-typed an entire day (September 1, 2000) of the New York Times; in Fidget, he
recorded every bodily movement for a day; in Soliloquy, every word he spoke for a week 
(but not those spoken to him); and in Traffic, traffic reports, recorded every ten minutes
over an unnamed holiday, from a New York radio station. His work, and his
accompanying manifestos, have initiated a vigorous debate about the work’s value. Who,
for example, would want to read Day? Apparently not even Goldsmith himself, who
professed to type it mechanically, scarcely even looking at the page he was copying. He
often speaks of himself as mechanistic,39 and as the “most boring writer who ever
lived.”40 In his list of favored methodologies, the parallel with database technologies is
unmistakable, as he mentions “information Management, word processing, databasing,
and extreme process . . . Obsessive archiving & cataloging, the debased language of
media & advertising; language more concerned with quantity than quality.”41 Of course
we might, as Marjorie Perloff does, insist there is more at work here than mere copying.42
Still, the author’s own design seems to commit him to enacting something as close to
Stanley Fish’s idea of algorithmic processing as humanly possible—rote calculation,
mindless copying, mechanical repetition. It seems, in other words, that Goldsmith is
determined to stage nonconscious cognition as taking over and usurping consciousness,
perhaps simultaneously with a sly intrusion of conscious design that a reader can notice
only with some effort. That he calls the result “poetry” is all the more provocative, as if
the genre most associated with crafted language and the pure overflow of emotion has
suddenly turned the neural hierarchy upside down. The irony, of course, is that the
cognitive nonconscious is itself becoming more diverse, sophisticated, and cognitively
capable. Ultimately what is mimed here is not the actual cognitive nonconscious but a
parodic version that pulls two double-crosses at once, at both ends of the neuronal
spectrum: consciousness performed as if it was nonconscious, and the nonconscious performed according to criteria selected by consciousness. As Perloff notes, quoting
John Cage, “If something is boring after two minutes, try it for four. If still boring, try it
for eight, sixteen, thirty-two, and so on. Eventually one discovers that it’s not boring at
all but very interesting’” (157). Consciousness wearing a (distorted) mask of the
cognitive nonconscious while slyly peeping through to watch the reaction—that’s
interesting!
Another example of how the cognitive nonconscious is surfacing in contemporary
creative works is Kate Marshall’s project on contemporary novels, which she calls
“Novels by Aliens” (focusing on “the nonhuman as a figure, technique and desire,”
Marshall shows that narrative viewpoints in a range of contemporary novels exhibit what
Fredric Jameson calls the “ever-newer realisms [that] constantly have to be invented to
trace new social dynamics.”
In Colson Whitehead’s Zone One, for example, the

viewpoint for the Quiet Storm’s highway clearing project involves an overhead, far-away
perspective more proper to a high-flying drone than to any human observer. The
protagonist, Mark Spitz, collaborates with Quiet Storm in part because he feels “lust to be
a viewpoint.”44 Although Marshall herself links these literary effects to such
philosophical movements as speculative realism, it is likely that both speculative realism
and literary experiments in nonhuman viewpoints are catalyzed by the expansive
pervasiveness of the cognitive nonconscious in the built environments of developed
countries. In this view, part of the contemporary turn toward the nonhuman is the
realization that an object need not be alive or conscious in order to function as a cognitive
agent.
Reframing Interpretation
Today the humanities stand at a crossroad. On one side the path continues with

traditional understandings of interpretation, closely linked with assumptions about
humans and their relations to the world as represented in cultural artifacts. Indeed, the
majority of interpretive activities within the humanities arguably have to do specifically
with the relation of human selves to the world. This construction assumes that humans
have selves, that selves are necessary for thinking, and that selves originate in
consciousness/unconsciousness. The other path diverges from these assumptions by
enlarging the idea of cognition to include nonconscious activities. In this line of
reasoning, the cognitive nonconscious also carries on complex acts of interpretation,
which syncopate with conscious interpretations in a rich spectrum of possibilities.
What would it mean to say that the cognitive nonconscious interprets? A clue is
given by physicist Edward Fredkin, when in a seminar he casually announced, “The
meaning of information is given by the processes that interpret it.”45 When Claude
Shannon first formulated information theory, Warren Weaver declared that it had nothing
to do with semantic meaning, for Shannon defined information as a function of
probability.46 Although very significant results have followed Shannon’s version of
information,47 for the humanities, a theory totally divorced from meaning has little to
contribute. Fredkin’s approach, however, suggests that flows of information occur within
contexts, and those contexts frequently offer multiple opportunities for interpretation.
One reason that digital technologies have become so pervasive and important is that they
are constructed to make interpretive choices as clear-cut as possible (because digital
technologies use discrete digital encoding, rather the continuous signals that analogue technologies use). In many instances, however, ambiguities remain, and substantive
choices have to be made. Medical diagnostic systems, automated satellite-imagery
identification, ship navigation systems, weather-prediction programs, and a host of other
nonconscious cognitive devices interpret ambiguous information to arrive at conclusions
that rarely if ever are completely certain. Something of this kind also happens with the
protoself in humans. Integrating multiple somatic markers, the protoself too must
synthesize conflicting and/or ambiguous information to arrive at interpretations that feed
forward into the relevant brain centers, emerging as emotions, feelings, and other kinds of
awareness in core and higher consciousness, where further interpretive activities take
place.
What advantages and limitations do these two paths offer? The traditional path
carries the assumption that interpretation, requiring as it does consciousness and a self, is
confined largely if not exclusively to humans (perhaps occasionally extended to some
animals). This path reinforces the idea that humans are special, that they are the source
of almost all cognition on the planet, and that human viewpoints therefore count the most
in determining what the world means. The other path recognizes that cognition is much
broader than human thinking and that other animals as well as technical devices cognize
and interpret all the time. Moreover, it also implies that these interpretations intersect
with and very significantly influence the conscious/unconscious interpretations of
humans, which themselves depend on prior integrations and interpretations by the
protoself. The search for meaning then becomes a pervasive activity among humans,
animals, and technical devices, with many different kinds of agents contributing to a rich
ecology of collaborating, reinforcing, contesting and conflicting interpretations.One of the costs of the traditional path is the isolation of the humanities from the

sciences and engineering. If interpretation is an exclusively human activity and if the
humanities are mostly about interpretation, then there are few resources within the
humanities to understand the complex embeddedness of humans in intelligent
environments and in relationships with other species. If, on the contrary, interpretation is
understood as pervasive in natural and built environments, the humanities can make
important contributions to such fields as architecture, electrical and mechanical
engineering, computer science, industrial design, and many other fields. The
sophisticated methods that the humanities have developed for analyzing different kinds of
interpretations and their ecological relationships with each other then pay rich dividends
for other fields and open onto any number of exciting collaborative projects.
Proceeding down the nontraditional path, in my view much the better choice,
requires a shift in conceptual frameworks so extensive that it might as well be called an
epistemic break. One of the first moves is to break the equivalence between thought and
cognition; another crucial move is to reconceptualize interpretation so that it applies to
information flows as well as to questions about the relations of human selves to the
world. With the resulting shifts of perspective, many of the misunderstandings about the
kinds of interventions the digital humanities are now making in the humanities simply
fade away. In closing, I want to emphasize that the issues involved here are much larger
than the digital humanities in themselves. Important as they are, focusing only on them
distorts what is at stake in talking about “Interpretation and Its Rivals.” The point, as far
as I am concerned, is less about methods that seem to be rivals to interpretation—a
formulation that assumes “interpretation” and “meaning” are stable categories that can be adequately discussed as exclusively human activities—than it is about the scope and
essence of interpretation in itself.

Sustainability, while being definitely a new form of humanity, as it has been
proposed and dealt with in many urban and landscape projects, lacks often of an
essential characteristic of the anthropic space: seduction. We believe that
sustainability has to find its own power of seduction if it is to compete successfully
with the ambiguous but established charms of the unsustainable city. From all the
above it is clear that the importance of the ‘aesthetic of sustainability’ is
fundamental for the success of a new model of green planning not just from an
environmental and economic point of view, but, perhaps and most importantly,
from a social and mental one.
This paper will investigate the possibility to look at sustainability and aesthetics
through the lens of evolutive processes and the complexity theory to inform a new
Bottom-Up/Self-Organized approach as a possible morphogenetic process for
sustainable city design.
Often criticized as the theory of ‘out of control’ the complexity theory applied
to the urban could instead be the enabler of a new paradigm where the notion of
single authorship with intellectual ownership and his aesthetic language is
substituted by the concept of a collective and a new aesthetics of choice where
aesthetics might recover, according to the evolutionary theory, their essence of an
‘adaptive system’ and an ecological category.
Keywords: sustainability, urban planning, complexity theory, systemic thinking,
multi-agent systems, aesthetics.




“It nonetheless remains the case that the immense crisis sweeping the planet –
chronic unemployment, ecological devastation, deregulation of modes of
valorization, uniquely based on profit or State assistance – opens the field up to a
different deployment of aesthetic components.”
(F. Guattari, Chaosmosis: an ethico-aesthetic paradigm, 1992 [7])
At the moment on the international level there would not seem to be a clear and
coded position in order to recognize a specific language and aesthetics in the
sustainable design of city and territory.
The urban and landscape scale, which had already introduced the concept of
ecology between the 1890 and the First World War with the figure of Patrick
Geddes [1], has developed a series of rules more similar to a ‘best practice’
approach, rather than a real and proactive solution with clearly recognizable
aesthetic values. Manifestos like the one of ‘One Planet Living Community’ or the
‘Triple Bottom Line’ and many others exist, but they just encode a series of points,
a dogmatic and little seductive vision, from a morphological point of view, of a
sustainability expressed more through new technological performance rather than
through a new urban language.
Existing examples of sustainable urban developments are just necessary ethical
actions, but they are lacking of an innovative aesthetic language and they are just
partly sustainable.
Sustainability in fact, while being definitely a new form of humanity and more
precisely the fourth human metabolic system [2], as it has been proposed and dealt
with in many urban and landscape projects lacks often of an essential characteristic
of the anthropic space: seduction.
Sustainability has to find its own power of seduction if it is to compete
successfully with the ambiguous but established charms of the unsustainable city.
Talking about sustainability as an ethical necessity is a given, but while dealing
with this theme, we should also care about aesthetics, style and emotions, the
essential elements of seduction that have historically made the city so attractive,
particularly the capitalistic city, and have much to do, paradoxically, with excess
and exuberance, with surplus production, conspicuous consumption and with
waste.
In formal terms when we deal with sustainability we deal as well with that sort
of radicalization which is reminiscent of what already happened with modernism
towards rationalism, as Frédéric Migayrou [3] reminds us: subverting the classicist
logic based on geometry, the humanist balance of proportions directly linked to
the human body was substituted by the idea of a normative measurement.
Following the same path: a logic based on composition and tectonicmorphogenetic research has been replaced by one aesthetically impoverished and
diminished, but normatively legitimated by an ethically performing technology.
Sustainability, as a matter of fact, contains in its performance some functional
rigidity codified by a series of norms placing the ethic as the ultimate irreplaceable
value. However, since functional rigidity tends towards chaos rather than complexity, while Excellency, understood as the Greek value of kalokagathìa (an
expression formed by the crasi of the two concepts of beautiful – kalòs – and – kai
– good – agathòs, one of Buber’s Grundworte :words that have a meaning just if
in couple) implies a certain degree of complexity, in a society such as the
contemporary one oriented most of all in terms of efficiency, speed and economic
gain, the aesthetic value, often anti-functional and anti-economic, tends to be an
obstacle exactly because it implies the acceptance of such complexity [4].
From all the above it is clear the importance of the ‘aesthetics of sustainability’
is fundamental for the success of a new model of green planning not just from an
environmental and economic point of view, but, perhaps and most importantly,
from a social and mental one. It is the aesthetic as it is envisaged by Guattari and
Foucault: ‘a way to hint at the creative potential of expression and enunciation that
has been silenced by the dominant force of signs and signifiers’ [5]. It is an
aesthetic paradigm interwoven with ethical and scientific paradigms: ‘The new
aesthetic paradigm has ethico-political implications because to speak of creation
is to speak of the responsibility of the creative instance with regard to the thing
created The three Ecologies and the importance of the evolutionary
approach: aesthetics as an adaptive system
In the Ecosophical treatise ‘The three Ecologies’ Guattari was de facto advocating
that the increasingly deteriorating condition of human relationships with the
socius, the psyche and the environment is due not only to the pollution and the
objective damage that belongs to this, but to the most worrying praxes of regarding
‘action on the psyche, the socius, and the environment as separate [6]’. Guattari
condemned the notion of ecology simply related to the environment in a sort of
synonymic equation as too reductive and too dangerous. He added: ‘We need to
apprehend the world through the interchangeable lenses of the three Ecologies.’
Such ecologies are governed by a logic of intensities which “concerns itself solely
with the movement and intensity of EVOLUTIVE PROCESSES”. This line of
thought is important because advocating a sort of ‘triplication’ implies as well the
overcoming of the binary system, the classic polarities and in general the
oppositions with all their typical synthesis, therefore it annihilates the dichotomy
between ethic and aesthetics, or, in other words, it declares aesthetics as an ethic
according to the transversal aspect of the three Ecologies and the aesthetic
paradigm always relating to modes of existence and life [7].’
Equally important, this position introduces the idea of ecologies within the neoDarwinian framework of Evolution creating a direct link between ecology and
aesthetics.
According to Orians, Professor Emeritus of Biology at the University of
Washington, results from existing studies have undoubtedly demonstrated the
power of an evolutionary approach to aesthetics: ‘Humans have strong emotional
responses to living organisms and to natural and human-modified environments.
[…] These powerful emotions, which are the foundations of aesthetics,been designed by evolutionary processes’. He specifies that ‘aesthetic emotions
are a major component of how humans solve problems [8].’
Appleton, Emeritus Professor of Geography at Hull University, on the same
subject gives an interesting definition of Beauty as ‘the product of interactions
between traits of objects and the human nervous system that evolved so that
objects we consider beautiful have properties that result in improved performance
in some aspect of living if we respond positively to them [9].’
As a Senior Scientist at the International Institute for Applied Systems Analysis
in Laxenburg, Marchetti seems to share with the people cited above the idea that
aesthetic responses are ‘fundamental to the ways in which organisms know about
and adapt to the world [10].’
If aesthetic responses evolved because they enabled people to better solve life's
problems, exposure to high quality environments should, at least, be restorative
and this brings us back to the link between aesthetics and ecologies.
Hence, within the evolutionary approach, it seems to be possible to define
aesthetics as an adaptive system and, as such, it ‘can function (or continue to exist)
only if it makes a continued adaptation to an environment that exhibits perpetual
novelty The birth of the sustainable agenda in city planning
The same approach interestingly enough is at the base of the birth of the
sustainable agenda in city planning. When in 1915 Patrick Geddes published
‘Cities in Evolution’ he was trying to fight against the social and environmental
chaos and evil of the spontaneous (read: Bottom-Up) sprawl of the city after the
industrial revolution.
He was the first one to consider the city as an environment which could
influence, positively or negatively the organism it contained and in doing so,
although totally unaware of the studies on aesthetics through the evolutionary lens,
he was promoting a certain aesthetic quality of the city space and at the same time
he was linking social evolution to spatial design and quality of the environment as
in ‘The three Ecologies’.
Even though his method can be clearly described as a Top-Down approach to
planning in a very deterministic, organized and predictable way, his book was also
the first publication to shift the accent from a developmental paradigm to an
evolutionary one, following the neo-Darwinian framework where small changes
can lead to big effects.
The importance that processes acquired in Geddes’ studies was also clear in his
attempt to understand better the tie between how a city functions in terms of energy
and its physical problems, in other words, in a more or less intuitive manner he
was trying to investigate the possibility of a method/theory which would link
cities’ morphologies to the process of their functioning: “[…] urban form should
follow the example of plant forms which illustrate how they organize themselves
to process energy more efficiently The Top-Down approach promoted by Geddes, even though not initiated by him,
was challenged for the first time in the 60s by people like Jane Jacobs and
Christopher Alexander, who both had rediscovered the potential of small
spontaneous changes on a vast scale as per the evolutionary paradigm.
Jacobs in her ‘Death and Life of Great American Cities’ in 1961 [32] declared
that ‘the diversity of cities that marked their quality is the diversity that was formed
from countless individual decisions, generated from the bottom-up.’ The sentence
is of particular relevance if we consider proper the definition according to which
there is aesthetics ‘anywhere the qualitative processes of reception and production,
of pleasure and making are examined [13]’ because it contains a logic association
between quality, hence aesthetics and the evolutionary Bottom-Up model.
In about the same years and all through the 80s, the formulation of the
Complexity Theory gave a final push towards the trends for the re-appropriation
of the Bottom-Up model: the essential principle for a complex system to exist is a
group of elements that perform independently of one another but nonetheless
manage to act altogether. The physical diagram of complexity is the feature of selforganization. Such a passage becomes even more remarkable if seen in
concomitance with the interest for clean and renewable energies which seems to
flourish in about the same years.
If we look at history as a sequence of different human metabolic systems we
see that the type of energy resource men used to draw on in the first two metabolic
systems (hunter-gathers societies and agricultural societies) by acting on the
biophysical matrix processes in their territory was always a cycle of production
and consumption limited to the biosphere. With the access to mineral resources
and therefore to the lithosphere, the sustainable cycle of production and
consumption got broken because the biosphere was not able to metabolize the
unwanted waste coming from consumption of the lithosphere materials [2].
Curiously enough the type of prevailing city models in the first two cases was a
Bottom-Up, which was substituted by a Top-Down one after the Industrial
revolution.
The research towards new and renewable types of energies, shifted again in the
biosphere realm, seems to have been accompanied by a renewed awareness of
the potential of the Bottom-Up model of city planning, a more complex and
emerging mode of action (Fig. 1).
In this light we could consider the Bottom-Up/Self-Organized approach as a
possible morphogenetic process for sustainable city design.
What is exactly the self-organized city and how is this model suitable with the
sustainable agenda and most of all with the aesthetics of the sustainable agenda? According to Peter Longley, Professor of GIS at the Bartlett, UCL, ‘selforganized cities are cities that seek to fill their space in the most efficient manner
following rules of self-similarity that show how they arrange their parts to
conserve and utilize the transport of their energy in the most efficient way’. On
the same line Michael Batty, Professor of Planning at the Bartlett and Director of
the Centre for Advanced Spatial Analysis, argues: ‘[The self–organized cities are]
models of cities simulating morphologies that are surprising in that their form
cannot be anticipated from the assumptions and processes adopted in their
representation. […] [12].’
The main differences between a ‘Self-Organized/Bottom-Up’ model and an
‘Organized/Top-Down’ one could be summarized in eight couples of opposite
modes as per Fig. 2.
The notion that cities are always ‘out of equilibrium’ and are constituted by a
multitude of bottom-up decisions leads to the recognition of the need to offer
solutions which would allow various elements of design to self-organize,
guaranteeing a margin of improvisation, so that architecture, city and anthropic
landscape could be understood and designed as ‘amalgams of processes’ which
modify and adjust themselves according to some inputs, as if they were selfgenerating systems, open languages of fluid and dynamic aesthetics based on the
logic of biotopes, ecosystems and ‘loop structures’, typical of sustainability.
The bottom–up logic, as we mentioned previously, needs to be situated in the neoDarwinian framework of evolutionary thinking.
Deleuze and Guattari remind us that ‘Darwinism’s two fundamental
contributions moved in the direction of a science of multiplicities: the substitution
of population for types and the substitution of rates or differential relations for
degrees [14].’
It is what Ernst Mayr, one of the fathers of evolutionary thinking, would later
describe as ‘Population thinking versus Typological thinking’: ‘For the
Typologist, the type (eidos) is real and the variation an illusion, while for
the Populationist the type (average) is an abstraction and only the variation is real
[15].’
Variations, differentiations, and multiplicities are categories of paramount
importance within the evolutionary paradigm. They differ from the term variants,
acceptation more proper to typological thinking, as they imply the replacement of
visual sameness with similarity, in fact while variants represent modifications to
an original artifact/model, variations do not imply the existence of a primitive, a
matrix or an archetype, they rather indicate marking differences of one individual
from another of the same species. Most importantly, shifting from biology back to
architecture, they embody the passage from typicality to non-standard seriality
[16]. It is the passage from the science of models characteristic of a series, where, by models, we mean rules, to the science of codes, where by codes, we mean rules;
in other words, from types to variables.
Sanford Kwinter perfectly summarized this process when stated that ‘The
relation of matter and forms are temporal and it is related to the path they have
done to get there [17].’ In computational terms that path is an algorithm.
Within this scientific framework of complex, emergent, bottom-up logics
algorithmic models are being organized to digitally breed cities, dealing with the
‘organization, quantification and systematization of quanta of data [18]’ and their
advent implies a revolutionary approach for what concerns one of the most
controversial and debated issues in the discipline of architecture and urban
planning: the notion of style and authorship.
In a field like the one of algorithmic morphogenesis, self-organizing and
emergent systems are playing a major role in challenging the ‘modern notion of
architect’s full authorial control and intellectual ownership of the end product
[18]’ and the contribution of the designer to the process could run the risk of being
downgraded to a simple breeder [19].
However, we believe that it would be worthwhile to dwell a bit more on a
couple of points in order to better understand the implications that concepts like
subjectivity and agency could have in morphing the discipline’s future.
First, the same notion of complexity, as it has been developed in different
disciplines and not only in architecture and urbanism has been modeled and
applied initially through the use of Parametric Algorithms (PA) and more recently
through Interactive genetic algorithms (IGA) and they imply a sort of dialogue, a
notational code, between man and machine. This dialogue would be better
described as an interface and has a particular privileged role to play in the
production and use of subjectivity as we find it in the definition of the aesthetic
paradigm of Guattari’s Chaosmosis. An idea of subjectivity strictly linked to the
concept of ecology and virtuality. It recalls indeed the designation of machines of
virtuality, ‘blocks of mutant percepts and affects, half-object half-subject. […] Not
a gestalt configuration, crystallizing the predominance of “good form. It’s about
something more dynamic, that I would prefer to situate in the register of […] the
autopoietic machine to define living systems [8]’.
From a semiotic and ontological point of view the interface and its autopoietic,
self-organizing assemblages are ‘incorporeal ecosystems’ [8], de facto resembling
the notion of virtual ecology, or ecology of values, wished for by Guattari: ‘a
speech between men and machines that would mark the change from the
contemporary world […] to a world characterized by a generalized ecology –
ecosophy – […] as a science of ecosystems, as a bid for political regeneration, and
as an ethical, aesthetic and analytic engagement [8].’
The very notion of the interface, together with the one of bottom-up systems,
entails concepts like open-endedness, participation, interaction and mass
collaboration and reconnects to the concept of Population thinking as the method
of reasoning which remind us that the population, the group, the society is the
medium for the production of forms, not the single person.
This position in the history of art is neither new nor revolutionary as even in
the XVth century Leon Battista Alberti, ‘master builder of the Italian Renaissance’ committed to achieve personal recognition through the affirmation of ‘his role
above the others’ in the construction of a building, believed that creativity was a
social and not an individual process [18].
Often criticized as the theory of ‘out of control’, definition that becomes even
more pregnant in terms of critical agency, the complexity theory applied to the
urban could instead, in my opinion, be the enabler of a new paradigm where
the notion of single authorship with intellectual ownership and his aesthetic
language is substituted by the concept of a collective and a new aesthetics of
choice or ‘aesthetics of decision [21]’, where aesthetics might recover, according
to the evolutionary theory, their essence of an adaptive system and an ecological
category [22]. We would then recuperate that ‘flux of participation’ evoked by
David Abram: ‘Our senses are not for detached cognition but for participation, for
sharing the metamorphic capacity of things that lure us [29].’

6 Transdisciplinarity and new models
There is a need for a new hermeneutics which would bring along a new aesthetics
as a property of matter in evolution according to the ‘fundamental law about the
creation of complexity: all the well-ordered systems that we know in the world,
all those anyway that we view as highly successful, are generated structures, not
fabricated structures [24].’
New models are required in order to breed cities in ‘digital laboratory’, models
that can be borrowed by other disciplines like biology, genetics, economics,
cybernetics, botanics, as Jose Louis Sert said ‘cities [are] living organisms; [they]
are born and … develop, disintegrate and die … In its academic and traditional
sense, city planning has become obsolete. In its place must be substituted urban
biology [25].’
The discourse about the urban has already taken advantage of the migration of
certain models from other disciplines.
A reference could be for example the loan from biology of sugar-scape models,
agent based social simulations that make possible to explore the connection
between the micro-level behavior of individuals and the macro-level patterns that
emerge from the interaction of many individuals; or allometric models, studying
the relationship of body size to shape, anatomy and finally behavior, can be used
to link the size and shape of living objects to the networks they use to deliver
resources to their parts; or again stigmergic models, mostly interesting within the
framework of a sustainable agenda because they represent the social mechanism
of coordination based on interaction through local modifications to a shared
environment.
This last model has been further enriched in terms of cognitive emergent
behaviors when borrowed in turn by IT scientists who introduced the presence of
artifacts as environmental modifiers. The research team headed by Prof. Ricci
working on the concept of Stigmergy as a MASs (multi-agent systems) technique
for realizing forms of emergent coordination in societies composed by simple,
non-rational agents, introduced the use of ‘suitable engineered artifacts’ to
explore instead the concept of Stigmergy in the context of societies composed by
WIT Transactions on Ecology and The Environment.
The Sustainable City IX, Vol. 1
cognitive/rational agents [26].’ The standing hypotheses at the base of the study
were mainly two: the environment as subject to open interpretation and
perception, therefore subject to an aesthetic conventional and collective system of
signs and the environment as mediator of behaviors, articulated and composed
of artifacts which, subjects to human cognitive activity, assemble the social
workspace. Artifacts are therefore entities representing the environment that
mediates agent interaction and enables emergent coordination and represent the
rationality/intentionality of agents’ actions.
In this perspective the environment acquires a key role, acting not only as a
container, a passive landscape against which all the interactions occur, but rather
as a negotiator and a ruler of interactions promoting the emergence of local and
global coordinated behaviors.
This specific research is of particular interest in the field of architecture and
urban design because, as Patrick Schumacher rightly points out, since architecture
and even more urban design are at the genesis of modes of abstract thinking where
conceptual structures and schema can emerge, it follows that architecture sets up
social order and in this line becomes explicit the importance of the role of artifacts
because ‘they are the factors upon which society is built up. The self-organizing city: evaluation metrics
Architecture and the city, where city is considered as the Latin concept of Urbs
and not Civitas, are to all effects artifacts and “are generally considered to be static,
a-biotic components of the constructed or natural ecosystems in which they are
situated [28]”. They are the physical medium where living subjects exchange
relations. Borrowing again the dictionary of biology we could argue that they are
the biotopes, ecosystems’ components characterized by a-biotic factors with their
physical and chemical features but not disjointed from their biotic biological
components, biocenosis: both are the indivisible components of an ecosystem.
However, within the framework of systemic thinking a more adept perspective
would be to redefine buildings and cities as constructed habitats.
This design approach inherently encourages exploration of the performance
potential of incorporating ecological behaviors – multivalent, adaptive, aesthetic,
spatial, parametric and systemic – into the design process, constructing ‘sensuous
ecologies’ [29].
In order to pursue this path we need to answer the question: “Is it possible for
constructed ecosystems to be developed in symbiotic ways?” According to
Mangone and Teuffel [28], this is possible when natural ecosystems and processes
are understood as valuable design elements. This perspective reconnects natural
and human processes and environments, considering them to be interdependent
and in the end indistinguishable. Developing habitats that interweave the natural
ecosystem with the constructed ecosystem is one approach that has the potential
for creating environments with much more intensity and nuance than current static
ones permit [28]. In other words a new aesthetics.

The challenge though here is how to establish the metric system for evaluating
constructed habitats to objectively distinguish parameters and processes that
advance, from those that diminish the aesthetic quality of such habitats.
A possible investigation in order to measure a certain degree of success or
failure in terms of aesthetics could lead us to borrow the method and the
performance metrics from how ecologists measure natural ecosystems as cultural
services and non-use values where cultural-cum-service is defined as the
‘aesthetic, artistic, educational and/or scientific value of ecosystems [30]’ and nonuse values as the ones which ‘encompass all values separated from use [31]’.
This could be the path to recover and regain ‘the way the senses themselves
have, of throwing themselves beyond what is immediately given, in order to make
tentative contact with the other sides of things that we do not sense directly, with
the hidden or invisible aspects of the sensible 




The end of unilateral globalization and the arrival of the Anthropocene force us to talk about cosmopolitics. These two factors correlate with one another and correspond to two different senses of the word “cosmopolitics”: cosmopolitics as a commercial regime, and cosmopolitics as a politics of nature.

First, we are witnessing the end of unilateral globalization. Until now, so-called globalization has been a largely one-sided process, entailing the universalization of particular epistemologies and the elevation, through techno-economic means, of a regional worldview to a putatively global metaphysics. We know that this unilateral globalization has reached its end because of how the 9/11 attacks were misread as an attack on the Occident by an Other. In fact, 9/11 was an “autoimmune” event, internal to the Atlantic bloc, wherein its own anti-communist cells, lingering after the Cold War, turned against their hosts. Still, the spectacular image of the event provided a kind of Rorschach test, onto which the representatives of unilateral globalization could project their growing insecurities about being stranded between the old configuration and the new—exemplifying what Hegel called “the unhappy consciousness.” This is clear in an article entitled “The Straussian Moment” by one of the leading financiers of American neoreaction, Peter Thiel:

The modern West has lost faith in itself. In the Enlightenment and post-Enlightenment period, this loss of faith liberated enormous commercial and creative forces. At the same time, this loss has rendered the West vulnerable. Is there a way to fortify the modern West without destroying it altogether, a way of not throwing the baby out with the bathwater?

Thiel’s unhappy consciousness recalls a past age of commercial glory renounced by the end of unilateral globalization, and aspires to a transhumanist futurism based on technological acceleration on all cosmic scales. This leads to a redefinition of the sovereign nation-state as a result of global technological competition (as the Russian president Vladimir Putin recently claimed, “whoever leads in AI will rule the world”). It is necessary to start imagining a new politics which is no longer a continuation of this same sort of geopolitics with a slightly different power configuration, that is, with the role of the leading power now played by China or Russia instead of the US. We need a new language of cosmopolitics to elaborate this new world order that goes beyond a single hegemon.

Second, the human species on earth is confronting the crisis of the Anthropocene. The earth and the cosmos have been transformed into a gigantic technological system, the culmination of the epistemological and methodological rupture which we call modernity. The loss of the cosmos is the end of metaphysics in the sense that we no longer perceive anything behind or beyond the perfection of science and technology. When historians like Rémi Brague and Alexandre Koyré write about end of the cosmos in seventeen- and eighteenth-century Europe, this should be read in our present Anthropocene context as an invitation to develop a cosmo-politics, not only in the sense of cosmopolitanism but also in the sense of a politics of the cosmos. In response to this invitation, I would like to suggest that in order to develop such a cosmopolitics it is necessary to elucidate the question of cosmotechnics. I have been developing this concept of cosmotechnics in order to reopen the question of technology by undoing certain translations that were driven by the search for equivalence during modernization. This problematization can be presented in terms of a Kantian antinomy:

Thesis: Technology is an anthropological universal, understood as an exteriorization of memory and the liberation of organs, as some anthropologists and philosophers of technology have formulated it;

Antithesis: Technology is not anthropologically universal; it is enabled and constrained by particular cosmologies, which go beyond mere functionality or utility. Therefore, there is no one single technology, but rather multiple cosmotechnics.

In order to elaborate the relation between cosmotechnics and cosmopolitics, I will divide this article into three parts. First, I will demonstrate how the Kantian concept of cosmopolitics is rooted in Kant’s concept of nature. In the second part, I situate the “multi-naturalism” proposed by the “ontological turn” in anthropology as a different cosmopolitics, one which, in contrast to Kant’s pursuit of the universal, suggests a certain relativism as the condition of possibility for coexistence. In the third part, I will try to show why it is necessary to move from cosmology to cosmotechnics as a politics to come. The main difficulty of all cosmopolitics is the reconciliation between the universal and the particular. The universal tends to contemplate the particulars from above, as in the way that Kant regarded the French Revolution, like a spectator considering a violent piece of theater from the mezzanine. Universality is the view of a spectator, never that of an actor. Kant writes, in his “Idea for a Universal History with a Cosmopolitan Aim”:

There is no other way out for the philosopher—who, regarding human beings and their play in the large, cannot at all presuppose any rational aim of theirs—than to try whether he can discover an aim of nature in this nonsensical course of things human; from which aim a history in accordance with a determinate plan of nature might nevertheless be possible even of creatures who do not behave in accordance with their own plan … [Nature] did produce a Kepler, who subjected the eccentric paths of the planets in an unexpected way to determinate laws, and a Newton, who explained these laws from a universal natural cause.

Throughout his political writings, Kant maintains that this relation between nature and cosmopolitics is necessary. If Kant sees the republican constitution and perpetual peace as political forms that may be able to bring forward a universal history of the human species, it is because he understands that such progress is also a progress of reason, the telos of nature. This progress toward an end goal—namely, universal history and a “perfect state constitution”—is the “completion of a hidden plan of nature” (Vollziehung eines verborgenen Plans der Natur). What does it mean for nature to have a hidden plan? And why is the realization of cosmopolitics the teleology of nature?

Authors such as Hannah Arendt and Eckart Förster, among others, suggest that Kant’s political philosophy centers on his concept of nature. Arendt proposes a juxtaposition concerning Kant’s perpetual peace: on the one hand, Besuchsrecht, the right to visit foreign countries and the right to hospitality; and on the other, nature, “the great artist, as the eventual ‘guarantee of perpetual peace.’” If after the 1789 revolution Kant is even more consistent in his affirmation of cosmopolitics as the teleology of nature, it is because he has developed the concept of self-organization, which plays a central role in the second book of his Critique of Judgment, and which affirms the two important categories of relation, namely community (Gemeinschaft) and reciprocity (Wechselwirkung).

Consider Kant’s example of the tree from §64 of the Critique of Judgment. First, the tree reproduces itself according to its genus, meaning that it reproduces another tree. Second, the tree produces itself as an individual; it absorbs energy from the environment and turns it into nutrients that sustain its life. Third, different parts of the tree establish reciprocal relations with one another and thus constitute the whole; as Kant writes, the “preservation of one part is reciprocally dependent on the preservation of the other parts.” In such a totality, a part is always constrained by the whole, and this is true of Kant’s understanding of cosmopolitical wholeness as well: “All states … are in danger of acting injuriously upon one another.” Nature is not something that can be judged from a particular point of view, just as the French Revolution cannot be judged according to its actors. Rather, nature can only be comprehended as a complex whole, and the human species, as one part of it, will ultimately progress towards a universal history that coincides with the teleology of nature.

Here we only want to show that as Kant develops his thinking towards universalism, his conceptualization of the relation between cosmopolitics and the purposiveness of nature is situated within a peculiar moment in history: the simultaneous enchantment and disenchantment of nature. On the one hand, Kant recognizes the importance of the concept of the organic for philosophy; discoveries in the natural sciences allowed him to connect the cosmos to the moral, as indicated by his famous analogy near the end of Critique of Practical Reason: “Two things fill the mind with ever new and increasing wonder and awe, the more often and constantly reflection concerns itself with them: the starry heavens above me and the moral law within me.” Howard Caygill makes an even stronger claim, arguing that this analogy points to a “Kantian physiology of the soul and the cosmos” that unites the “within me” (freedom) and the “above me.” On the other hand, as we saw in Kant’s citation of Kepler and Newton in “Idea for a Universal History with a Cosmopolitan Aim,” the affirmation of “universal history” and advancements in science and technology led in the eighteenth century to what Rémi Brague calls the “death of the cosmos”:

The new astronomy, following Copernicus and his successors, had consequences for the modern view of the world … Ancient and medieval thinkers presented a synchronic schema of the structure of the physical world, which erased the traces of its own genesis; the Moderns, on the other hand, remembered the past and in addition provided a diachronic view of astronomy—as if the evolution of ideas about the cosmos was even more important than the truth about it … Can we still speak of cosmology? It seems that the West ceased to have a cosmology with the end of the world of Aristotle and Ptolemy, an end due to Copernicus, Galileo, and Newton. The “world” then no longer formed a whole.

New discoveries in the natural sciences thanks to the invention of the telescope and the microscope exposed human beings to magnitudes they could not previously comprehend, leading us to a new relation with the “entire span of nature” (in dem ganzen Umfang der Natur). The Kantian scholar Diane Morgan suggests that through the “worlds beyond worlds” revealed by technology, nature ceases to be anthropomorphic, for the relation between humans and nature is thus reversed, with humans now standing before the “unsurveyable magnitude” (Unabsehlich-Groß) of the universe. However, as we indicated above, there is a double moment that deserves our attention: both the enchantment and disenchantment of nature via the natural sciences, leading to a total secularization of the cosmos.

In addition to the revelation of nature and its teleology through technical instruments, technology also plays a decisive role in Kant’s political philosophy, when he asserts that communication is the condition of the realization of the organicist whole. Arendt made explicit the role of the sensus communis in Kant’s philosophy, as both the question of community and consensus. But such a sensus communis is achieved only through particular technologies, and it is on this ground that we should problematize any naive discourse on the common as something already given or preceding technology. The age of Enlightenment, as noted by Arendt (as well as Bernard Stiegler), is the age of “the public use of one’s reason,” and this exercise of reason is expressed in the freedom of speaking and publishing, which necessarily involves the technology of printing. On an international level, in “Toward Perpetual Peace: A Philosophical Sketch” Kant writes that “it was trade that first brought them into peaceful relations with one another and thereby into relationships based on mutual consent, community, and peaceful interactions even with remote peoples,” later adding, “it is the spirit of trade, which cannot coexist with war, which will, sooner or later, take hold of every people.”

This reiteration of Kantian cosmopolitanism is an attempt to demonstrate the role of nature in Kant’s political philosophy. Kant somehow assumes one single nature, which reason compels us to recognize as rational; the rationality corresponds to the organicist teleological universality ostensibly realized in the constitution of both morality and the state. This enchantment of nature is accompanied by a disenchantment of nature, driven by the mechanization enforced by the Industrial Revolution. Brague’s “death of the cosmos” brought about by European modernity and its globalization of modern technology necessarily forms one of the conditions for us to reflect on cosmopolitics today, insofar as it illustrates the inefficacy of a biological metaphor for cosmopolitanism. If we start with Kant rather than with more recent discussions on cosmopolitanism—such as Martha Nussbaum’s rootless cosmopolitanism, Habermas’s constitutional patriotism, or Anthony Appiah’s cosmopolitan patriotism—it is because we want to reconsider cosmopolitanism by examining its relation to nature and technology. In fact, Appiah’s rooted cosmopolitanism is relevant to our discussion below. He holds the view that cosmopolitanism denies the importance of affiliations and particular loyalties; this means that it is necessary to consider cosmopolitics from the point of view of locality. This crucial point is the reason I would like to engage with the idea of “multi-naturalism” recently proposed by anthropologists associated with attempts to present a new way of thinking cosmopolitanism.

The “ontological turn” in anthropology is a movement associated with anthropologists such as Philippe Descola, Eduardo Viveiros de Castro, Bruno Latour, and Tim Ingold, and earlier, Roy Wagner and Marilyn Strathern, among others. This ontological turn is an explicit response to the crisis of modernity that expresses itself largely in terms of ecological crisis, which is now closely associated with the Anthropocene. The ontological-turn movement is an effort to take seriously different ontologies in different cultures (we have to bear in mind that knowing there are different ontologies and taking them seriously are two different things). Descola has convincingly outlined four major ontologies, namely naturalism, animism, totemism, and analogism. The modern is characterized by what he calls “naturalism,” meaning an opposition between culture and nature, and the former’s mastery over the latter. Descola suggests that we must go beyond such an opposition and recognize that nature is no longer opposed or inferior to culture. Rather, in the different ontologies, we can see the different roles that nature plays; for example, in animism the role of nature is based on the continuity of spirituality, despite the discontinuity of physicality.

In Beyond Culture and Nature, Descola has proposed an ontological pluralism that is irreducible to social constructivism. He suggests that recognizing these ontological differences can serve as an antidote to the dominance of naturalism since the advent of European modernity. But does this focus on nature (or the cosmos, we might say) in the interest of opposing European naturalism actually revive the enchantment of nature, this time in the name of indigenous knowledge? This seems to be a hidden problem with the ontological-turn movement: many anthropologists associated with the ontological turn have focused on the question of nature and the politics of the nonhuman (largely animals, plants, minerals, spirits, and the dead). This is evident when we recall that Descola proposes to call his discipline an “anthropology of nature.” Furthermore, this tendency also suggests that the question of technics is not sufficiently addressed in the ontological-turn movement. For example, Descola talks often of practice, which may indicate his (laudable) desire to avoid an opposition between nature and technics; but by doing so, he also obscures the question of technology. Descola shows that analogism, rather than naturalism, was a significant presence in Europe during the Renaissance; if this is the case, the “turn” that took place during European modernity seems to have resulted in a completely different ontology and epistemology. If naturalism has succeeded in dominating modern thought, it is because such a peculiar cosmological imagination is compatible with its techno-logical development: nature should be mastered for the good of man, and it can indeed be mastered according to the laws of nature. Or put another way: nature is regarded as the source of contingency due to its “weakness of concept,” and therefore it has to be overcome by logic.

These oppositions between nature and technics, mythology and reason, give rise to various illusions that belong to one of two extremes. On the one hand, there are rationalists or “progressivists” who hysterically struggle to maintain their monotheism after having murdered god, wishfully believing that the world process will stamp out differences and diversities and lead to a “theodicy.” On the other hand, there are left intellectuals who feel the need to extol indigenous ontology or biology as a way out of modernity. A French revolutionary thinker recently described this situation thus:

A funny thing to see these days is how all these absurd modern leftists, all unable to see anything, all lost in themselves, all feeling so bad, all desperately trying to exist and to find their existence in the eyes of the Other—how all these people are jumping on the “savage,” the “indigenous,” the “traditional” in order to escape and not face themselves. I am not speaking of being critical towards one’s “whiteness,” towards one’s “modernism.” I am talking of the ability to peer inside [transpercer] oneself.

My refusal of the above two extremes does not come out of any postcolonial “political correctness,” but rather out of an attempt to go beyond postcolonialism’s critique. (Indeed, I have elsewhere reproached postcolonialism for its failure to tackle the question of technology.) I hold the thesis that an ontological pluralism can only be realized by reflecting on the question of technology and a politics of technology. Kant was aware of the importance of technology in his comment on trading as communication; however, he didn’t pay much attention to the technological difference that finally led to planetary modernization, and now planetary computation, since what was at stake for him was the question of the whole that absorbs all differences. Kant criticized the impolite guests, the greedy colonizers who brought with them “oppression of the native inhabitants, the incitement of the different states involved to expansive wars, famine, unrest, faithlessness, and the whole litany of evils that weigh upon the human species.” Commenting on the defense strategies of China and Japan, Kant said that both countries have

wisely, limited such interaction. Whereas the former has allowed contact with, but not entrance to its territories, the latter has allowed this contact to only one European people, the Dutch, yet while doing so it excludes them, as if they were prisoners, from associating with the native inhabitants.

When Kant wrote this in 1795, it was too early for him to anticipate the modernization and colonization that would take place in Japan and China. If this phase of globalization was able to take place, it was because of the technological advancement of the West, which allowed it to defeat the Japanese, the Chinese, and other Asian civilizations. Nature, the guarantee of perpetual peace, didn’t really lead us to perpetual peace but rather to wars and more wars. To appeal for a cosmopolitanism today, I think we must reread Kant’s cosmopolitanism according to the process of modernization and revisit the question of nature and technology anew. The arrival of modern technology in non-European countries in recent centuries has created a transformation unthinkable to European observers. The restoration of “indigenous natures” itself has to first be questioned, not because it doesn’t exist but because it is situated in a new epoch and is transformed to the extent that there is hardly any way to go back and restore it.

Let’s review what has been said above regarding the ontological turn. Central to the anthropologists’ concept of “nature” and “ontology” is cosmology, since such “nature” is defined according to different “ecologies of relations” in which we observe different constellations of relations, e.g., the parental relation between females and vegetables, or brotherhood between hunters and animals. These multi-ontologies are expressed as multi-natures; for example, Descola’s four above-named ontologies correspond to different cosmological views. I believe that it is very difficult, if not impossible, to overcome modernity without directly confronting the question of technology, which has become increasingly urgent after the end of unilateral globalization. Therefore, it is necessary to reformulate the question of cosmopolitics in relation to cosmotechnics.

I propose to go beyond the notion of cosmology; instead, it would be more productive to address what I call cosmotechnics. Let me give you a preliminary definition of cosmotechnics: it is the unification of the cosmos and the moral through technical activities, whether craft-making or art-making. There hasn’t been one or two technics, but many cosmotechnics. What kind of morality, which and whose cosmos, and how to unite them vary from one culture to another according to different dynamics. I am convinced that in order to confront the crisis that is before us—namely, the Anthropocene, or the intrusion of Gaia (Latour and Stengers), or the “entropocene” (Stiegler), all presented as the inevitable future of humanity—it is necessary to reopen the question of technology, in order to envisage the bifurcation of technological futures by conceiving different cosmotechnics. I tried to demonstrate such a possibility in my recent book The Question Concerning Technology in China: An Essay in Cosmotechnics. As one can gather from the title, it is an attempt to respond to Heidegger’s famous 1949 lecture “The Question Concerning Technology.” I propose that in order to rethink the project of overcoming modernity, we must undo and redo the translations of technē, physis, and metaphysika (not as merely independent concepts but also concepts within systems); only by recognizing this difference can we arrive at the possibility of a common task of philosophy.

Why, then, do I think it’s necessary to turn to cosmotechnics? For a long time now we have operated with a very narrow—in fact, far too narrow—concept of technics. By following Heidegger’s essay, we can distinguish two notions of technics. First, we have the Greek notion of technē, which Heidegger develops through his reading of the ancient Greeks, notably the Pre-Socratics—more precisely, the three “inceptual” (anfängliche) thinkers, Parmenides, Heraclitus, and Anaximander. In the 1949 lecture, Heidegger proposes to distinguish the essence of Greek technē from modern technology (moderne Technik).

If the essence of technē is poiesis, or bringing forth (Hervorbringen), then modern technology, a product of European modernity, no longer possesses the same essence as technē but is rather an “enframing” (Gestell) apparatus, in the sense that all beings become standing reserves (Bestand) for it. Heidegger doesn’t totalize these two essences of technics, but nor does he give space to other technics, as if there is only a single homogenous Machenschaft after the Greek technē, one that is calculable, international, even planetary. It is astonishing that in Heidegger’s so-called Black Notebooks (Schwarze Hefte)—of which four volumes have been published so far—we find this note: “If communism in China should come to rule, one can assume that only in this way will China become ‘free’ for technology. What is this process?” Heidegger hints at two things here: first, that technology is international (not universal); and second, that the Chinese were completely unable to resist technology after communism seized power in the country. This verdict anticipates technological globalization as a form of neocolonization that imposes its rationality through instrumentality, like what we observe in transhumanist, neoreactionary politics.

My effort to go beyond Heidegger’s discourse on technology is largely based on two motivations: 1) a desire to respond to the ontological turn in anthropology, which aims to tackle the problem of modernity by proposing an ontological pluralism; and 2) a desire to update the insufficient discourse on technology that is largely associated with Heidegger’s critique of technology. I have proposed that we reopen the question of technics, to show that one must consider technics as a variety of cosmotechnics instead of either technē or modern technology. In my book, I used China as a testing ground for my thesis and tried to reconstruct a lineage of technological thought in China. However, this task is not limited to China, since the central idea is that every non-European culture must systematize its own cosmotechnics and the history of such a cosmotechnics. Chinese cosmotechnical thought consists of a long history of intellectual discourse on the unity and relation between Qi and Dao. The unification of Qi and Dao is also the unification of the moral and the cosmic, since Chinese metaphysics is fundamentally a moral cosmology or a moral metaphysics, as the New Confucian philosopher Mou Zongsan has demonstrated. Mou suggests that if in Kant we find a metaphysics of the moral, it is at most a metaphysical exploration of the moral but not a moral metaphysics, since a moral metaphysics can only start with the moral. Mou’s demarcation between Chinese and Western philosophy situates his conviction that Chinese philosophy recognizes and cultivates the intellectual intuition that Kant associated with knowing the noumenon, even as Kant dismissed the possibility that human beings could possess such an intuition. For Mou, the moral arises out of the experience of the infinity of the cosmos, which necessitates infinitization as the condition of possibility for Dasein’s finitude.

Dao is not a thing. It is not a concept. It is not the différance. In the Cixi of YiZhuan (易傳‧繫辭), Dao is simply said to be “above forms,” while Qi is what is “below forms.” We should notice here that xin er shang xue (the study of what is above forms) is the word used to translate “metaphysics” (one of the equivalences that must be undone). Qi is something that takes space, as we can see from the character and also read in an etymological dictionary—it has four mouths or containers and in the middle there is a dog guarding the utensils. There are multiple meanings of Qi in different doctrines; for example, in classic Confucianism there is Li Qi (禮器), in which Qi is crucial for Li (a rite), which is not merely a ceremony but rather a search for unification between the heavens and the human. For our purposes, it will suffice to simply say that Dao belongs to the noumenon according to the Kantian distinction, while Qi belongs to the phenomenon. But it is possible to infinitize Qi so as to infinitize the self and enter into the noumenon—this is the question of art.

In order to better understand what I mean by this, we can refer here to the story of the butcher Pao Ding, as told in the Zhuangzi. However, we will have to remind ourselves that this is only an example from antiquity, and a much larger historical view is necessary to comprehend it.

Pao Ding is excellent at butchering cows. He claims that the key to being a good butcher doesn’t lie in mastering certain skills, but rather in comprehending the Dao. Replying to a question from Duke Wen Huei about the Dao of butchering cows, Pao Ding points out that having a good knife is not necessarily enough; it is more important to understand the Dao in the cow, so that one does not use the blade to cut through the bones and tendons, but rather to pass alongside them in order to enter into the gaps between them. Here, the literal meaning of “Dao”—“way” or “path”—meshes with its metaphysical sense:

What I love is Dao, which is much more splendid than my skill. When I first began to carve a bullock, I saw nothing but the whole bullock. Three years later, I no longer saw the bullock as a whole but in parts. Now I work on it by intuition and do not  look at it with my eyes. My visual organs stop functioning while my intuition goes its own way. In accordance with the principle of heaven (nature), I cleave along the main seams and thrust the knife into the big cavities. Following the natural structure of the bullock, I never touch veins or tendons, much less the big bones!

Hence, Pao Ding concludes that a good butcher doesn’t rely on the technical objects at his disposal, but rather on Dao, since Dao is more fundamental than Qi (the tool). Pao Ding adds that a good butcher has to change his knife once a year because he cuts through tendons, while a bad butcher has to change his knife every month because he cuts through bones. Pao Ding, on the other hand—an excellent butcher—has not changed his knife in nineteen years, and it looks as if it has just been sharpened with a whetstone. Whenever Pao Ding encounters any difficulty, he slows down the knife and gropes for the right place to move further.

Duke Wen Huei, who had posed the question, replies that “having heard from Pao Ding, now I know how to live”; and indeed, this story is included in a section titled “Master of Living.” It is thus the question of “living,” rather than that of technics, that is at the center of the story. If there is a concept of “technics” here, it is one that is detached from the technical object: although the technical object is not without importance, one cannot seek the perfection of technics through the perfection of a tool or a skill, since perfection can only be accomplished by Dao. Pao Ding’s knife never cuts tendons or bones; instead, it seeks the void and enters it with ease. In so doing, the knife accomplishes the task of butchering the cow without endangering itself—i.e., without becoming blunt and needing to be replaced. It thus fully realizes itself as a knife. 

What I have said above is not sufficient to be formulated into a program, since it is only an explanation for the motivation behind the much larger project that I tried to initiate in The Question Concerning Technology in China. Also, we must pay attention to the historical development of the relationship between Qi and Dao. Specifically, the search for unity between Qi and Dao has gone through different phases in Chinese history in response to historical crises (the decline of the Zhou Dynasty, the proliferation of Buddhism, modernization, etc.); it was widely discussed after the Opium Wars of the mid-nineteenth century, but such a unification was not resolved due to a very limited understanding of technology at the time and an eagerness to look for equivalences between China and the West. I have attempted to reread the history of Chinese philosophy not only as intellectual history, but also through the lens of the Qi-Dao episteme, with the aim of reconstructing a tradition of technological thought in China. As I have emphasized elsewhere, this question is by no means only a Chinese affair. Rather, every culture must reflect on the question of cosmotechnics for a new cosmopolitics to come, since I believe that to overcome modernity without falling back into war and fascism, it is necessary to reappropriate modern technology through the renewed framework of a cosmotechnics consisting of different epistemologies and epistemes. Therefore, my project is not one of substantializing tradition, as in the case of traditionalists like René Guénon or Aleksandr Dugin; it doesn’t refuse modern technology, but rather looks into the possibility of different technological futures. The Anthropocene is the planetarization of standing reserves, and Heidegger’s critique of technology is more significant today than ever before. The unilateral globalization that has come to an end is being succeeded by the competition of technological acceleration and the allures of war, technological singularity, and transhumanist (pipe) dreams. The Anthropocene is a global axis of time and synchronization that is sustained by this view of technological progress towards the singularity. To reopen the question of technology is to refuse this homogeneous technological future that is presented to us as the only option.

The definition of the Anthropocene, together with recent technological developments in synthetic biology and artificial intelligence, presents scenarios in which the boundaries between natural and artificial, landscape and city, human and non-human realms are blurred. In this context, the object of architecture becomes ambiguous and its aesthetic language now embodies feelings of estrangement, discomfort and disruption. With this design method I propose a productive form of alienation where, alien to us, micro-organisms such as bacteria, fungi, spiders and moulds can act both as a be- havioural model for architecture and as active agents of architectural production. This has direct influence on the ever-critical relationship between form and performance in contemporary disciplinary discourse. With the goal of unpacking this influence, I will begin by observing the effects of collective intelligence in natural formations such as coral colonies, ants’ colonies and other biological micro-organisms. I will then extend these observations to the relationship between the human and the in-human in the context of contemporary cities. Over the years my practice has become increasingly concerned with the actualisation of this morphogenetic paradigm in architectural and urban design. “Through experimenting with biological organisms and digital apparatus we envision a new generation of bio-digital ‘designed prototypes’ with transformative agency for an architectural discipline yet to come.” [32] Bateson argued that patterns are a form of analogue computation and a significant expression of cybernetic communication or metalanguage. In an embryogenetic process, this analogue computation expresses itself in the form of fluid patterns, flowing in-between the womb and the embryo in order to influence the specific growth of the embryo itself, and a morphogenetic process emerges out of this material interaction. Socially, the aesthetic, as a form of material interaction, becomes a way of expressing, exploring and developing such metalanguages. “So, by ‘aesthetic’, I mean responsiveness to the pattern which connects. The pattern which connects is a meta-pattern. It is a pattern of patterns. It is that meta-pattern which defines the vast generalization that indeed it is the pattern that connect.” Gregory Bateson The aesthetic becomes here a means to establish a cybernetic conversation within which human and non-human ecologies constitute co-evolutionary systems, a form of extended mind. Following Bateson’s reasoning I find contemporary philosophers Slavoj Zizek and Timothy Morton particularly relevant. They each promote a view of ecology without nature, suggesting, albeit rather differently, a greater role for the aesthetic in the reframing of ecological issues. Their approach articulates the shift from a problem- solving framework to a cybernetic one So while we campaign to make our world ‘cleaner’ and less toxic, less harmful to sentient beings, our philosophical adventure should in some way be quite the reverse. We should be finding ways to stick around the sticky mess we are in and that we are, making thinking dirtier, identify with ugliness [...] dark ecology.” Timothy Morton [8] Morton agues clearly that we cannot sanitise global ecosystems; rather, in order to fully address the issue of disrupted ecologies we should expand our consciousness to their ‘darker side’. Following a similar line of thought Zizek [9] recognises the paradox of the current condition and proposes what we may define a design-driven solution. He identifies our current condition with ‘disavowal’, arguing that we know very well what the threats of an imminent ecological catastrophe are, but we still do not act energetically upon them because we cannot rationally believe them. We are told by various media about the horrible consequences of climate change, but in our daily life we have almost no experience of them. That is why to him true ecologists should look for trash and not for trees, which is to say that they should look for the hidden by- products of our society rather than cover them in green propaganda. “There is more and more of this trash, and I think even the greatest challenge is to discover trash as an aesthetic object. [...] We should develop a much more terrifying new abstract materialism, a kind of mathematical universe where there is nothing, there are just formulas, technical forms and so on, and the difficult thing is to find poetry, spirituality, in this dimension” Slavoj Zizek [10] Could this ‘terrifying form of abstract materialism’ be a brief to inform the role and methods of contemporary architecture? During this time of global climate change we are told that no ecosystem is unaffected by human actions. But what kind of change are we referring to? Aren’t change, transformation and adaptation all inherent qualities of the planet we inhabit? Our current stage of technological evolution, notably in the form of synthetic biology or artificial intelligence, is opening scenarios where traditional dichotomies such as natural and artificial, material and digital, human and non-human become obsolete. Similarly, the machines that allow us to compute the changes in the global climate also contribute to it with their intense energy consumption and carbon footprint. At the same time, these computational processes promote new visual languages, revealing emerging territorial and urban narratives that are profoundly ‘natural’. If, for example, we look at large growing cities via a satellite image, we realise that it has become increasingly difficult to define what is natural and what is artificial. From this perspective, global cities, despite being large artificial systems often depicted as the antithesis of nature, develop patterns of growth (or shrinkage) that recall natural formations of a radically different kind. Cities appear as complex synthetic organisms. This perspective contradicts the model of urbanity that we inherited from modernity where zones are clearly defined and morphologically demarcated. In this context, areas of production or treatment of waste have traditionally been located, by planners, further out of the city centres, technically preventing possible contamination of the living quarters, but also removing the by-product of urbanization from our sight and from our consciousness at a very fundamental level.Today we still rely on a sanitised vision of the world’s ecosystems where bacteria and micro-organisms are commonly considered dangerous. We therefore talk about re-greening cities and re-naturalizing forests as if such processes could lead to the re-equilibrium of a temporarily perturbed biosphere. But most global systems today, both natural and artificial as we have noted above, are non-linear and composed of billions of interlocking feedback loops. Destruction, decay, digestion and dissolution are some of their most fundamental processes and a critical part of their circularity; these processes often constitute the dark side of ecology, the one that we have all but erased from our consciousness. On the other hand, micro-organisms have exceptional properties that we are still discovering in laboratories, and which make them capable of turning what we consider pollution or waste into nutrient and raw material; they are the missing link to redefining contemporary urban metabolism. Biotechnology and AI design are enabling a new vision of urbanity in which fungi, bacteria, spiders, swarm machines, and other forms of intelligence become, alongside humans, bio-citizens, contributing to the morphogenesis of the urban. With this argument, I would like to propose a shift from morphologies to morphogenesis. While the first is a catalogueof static shapes, the second is the process of becoming of form under the pressuresof matter, information and energy. The relationship between the human and the non- human contributes to the definition of novel minds and novel morphogenetic systems. Therefore, in the Anthropocene, an epoch when our civilisation has impacted on metabolic processes at a planetary scale, we are depending, perhaps paradoxically, upon non-anthropocentric forms of intelligence. As Bruno Latour discussed in an interview published in October 2018 in the New York Times: “The medical revolution commonly attributed to the genius of Pasteur”, Latour argued, “should instead be seen as a result of an association between not just doctors, nurses and hygienists, but also worms, milk, sputum, parasites, cows and farms. Science was social then, not merely because it was performed by people [this, he thought, was a reductive misunderstanding of the word ‘social’]; rather science was social because it brought together a multitude of human and non-human entities and harnessed their collective power to act on and transform the world.” One might ask whether on this view architecture, urbanism and design in general, as forms of material language, could play a role in enabling the above communication and co-evolution between human and non-human entities. From this perspective, my work looks at Physarum Polycephalum (PP) as a form of design intelligence and ithas, therefore, been pursing architecture as a research-based practice exploring the interdependence of digital and biological intelligence in design by working directly with non-human living organisms. The PP is considered by many to be the next biological computer, or a form of bio-artificial intelligence. It is a unicellular organism, but through the interaction of millions of nuclei can perform quite sophisticated tasks of network optimization and resource redistribution. Computer Scientists have been naming this behaviour ‘collective intelligence’[12], a form of non-centralised intelligence emerging from the local interaction of multiple simple units. What is especially interesting about this line of research is not only how powerfulthis form of collective intelligence or the computation of the PP is, but particularly the fact that in this case computation is embedded in matter and it can be embeddedin the matter of architecture itself. In the PP, the aesthetic becomes a measure of ecological intelligence which manifest itself as morphological convolutions that defy both the classical canons of beauty and the rational logics of efficient engineering.In ecoLogicStudio we observe the diagrammatic capacity of these organisms in the process of growing and becoming part of complex bio-digital architectures. We train our sensibility at recognising patterns of reasoning across disciplines, materiality and technological regimes, thus expanding our practice’s repertoire of aesthetic qualities. Recent developments in evolutionary psychology, described by Anjan Chatterjee in the book ‘The Aesthetic Brain’], demonstrate that our sense of beauty and pleasureis part of a co-evolutionary system of our mind and the surrounding environment. In these terms our senses of beauty and pleasure have evolved as a selection mechanism. Cultivating and enhancing them enables us to compensate and integrate our logical thinking to gain a more systemic view of our planet and the dramatic changes it is currently undergoing.With this research project I wish to illustrate how a renewed appreciation of the aesthetic as a form of material language in architecture is evolving into an operational tool to design and measure actual ecological intelligence. I developed my interest in material forms of computation prior to my encounter with bio-computation. It dates back to a period when I was studying and teaching at the Architectural Association in London. At that time (2001 to 2012 approximately) the discourse about materiality, computation and non-linearity was already ongoing. One of the main authors that caught my attention was Frei Otto, who had been working with beautiful and effective models of analogue computation at the architectural and territorial scale. “The settlement, the human city, is natural. To recognise it is a natural science. To tend it is an art, analogous to horticulture. This cannot exist without knowledge of the plants, soil and water involved. The art of urban development requires knowledge of all living organisms in nature, of non-living nature, the present state and the possibilities of technology.” Frei Otto The quote is from Otto’s book ‘Occupying and Connecting’, in which he argued for the relevance and trans-scalarity of processes of material computation and underlined a new role for the process of gardening as a material practice, related not only to tending plants and vegetables, but also to an expanded field of dynamic analogueand digital processes which will redefine our cities and our urban landscapes. As such, gardening, or cyber-gardening, acquired for us, in ecoLogicStudio, a critical relevance in reframing our relationship to emergent digital design practices. Material computation possesses a rather peculiar nature which makes coding specific to a certain milieu, beit the ecology of a landscape or of an architectural material system. This peculiarity is exemplified by comparing the methods of a bioengineer synthesizing artificial tissues in a laboratory with those of a gardener reviving a patch of dried land: while both are running generative protocols, the first requires a perfectly controlled and refined testing ground for his procedures to acquire general applicability, while the second needs to consider the unexpected fluctuation of the ecology of his garden.The bioengineer, within the controlled setting of the lab, can trigger a series of reactions and morphogenetic processes and slowly grow coherent and therefore potentially functioning tissues. If all the variables are well controlled and managedthe final outcome can reproduce precisely the results predicted by early computer simulations of specific scenarios of development; a little unexpected mistake or variation in the testing bed and the final results would become unpredictable, incoherent and often lose their applicability. The gardener, instead, operates withina very different framework; his testing bed is very differentiated and partially unpredictable in its daily or seasonal fluctuations; his operational protocols need to consider this fluctuations and differences in their formula. The gardener operates through a process of intensification of difference; his only chance to reconcile his desire of beautification and the natural expressivity of living processes resides in movement, in its biological and physical sense. Gilles Clement, in his book ‘The Planetary Gardener’, suggests that the formalization of the garden becomes a process of formalised transmission of biological messages or, in our terms, of algorithmic coding; for the gardener, algorithms are machines for breeding a continuous bio-diversity. The relevant narrative for our project concerns the struggle to use such algorithms asa language rather than as a tool for optimising procedures. Going beyond what was discussed by Frei Otto, we focus on the aesthetic language of the patterns emerging from models of analogue computation. In the past century, French philosopher Henry Bergson, associated with the philosophy of ‘vitalism’, argued for the relevance of the continuous process of life which he called ‘élan vital’. In Creative Evolution (1907), long before Bateson, Bergson formulated a severe critique of the positivistic way of thinking and its static way of interpreting life. For him, one of the intellectual traps of positivism was its attitude of always working on ‘freezing’ systems, i.e. systems in which both the spatial and the temporal dimension are reduced to constant values. These static models are ‘death’ accordingly to Bergson[16]. In fact, if life is nothing but ‘élan vital’ it cannot be represented in a static model alone. He argues in terms of spatial variability: the positivistic science is wrong to isolate systems while studying their behaviour, because these systems exist only in relationship to the entire universe. Similarly, in terms of temporal variability, observing a living system implies taking into consideration its behaviour in all of its duration, so that by looking only at critical points scientists are cutting life from the studied process. Bergson underlines in this way a dichotomy between the scientific method of understanding life and the complexity of life itself.He describes this contradiction as a dichotomy between ‘intuition’, which he definedas the knowledge which allows us to grasp the duration and complexity of life but cannot be express through logical words, and ‘conceptual knowledge’ which he defines as an abstract and schematic language useful in pragmatic terms but unproductive in metaphysical ones. Bergson’s discourse then develops toward a proposal for the concept of ‘consciousness as duration’. “Doubtless we think only a small part of our past, but it is with our entire past including the original bent of our soul, that we desire, we will and act. Our past, there, as whole, is made manifest to us in its impulse, it is felt in the form of tendency, although a small part is known in the form of idea.” Bergson [17] Inorganic reality seems to be qualitatively different from the continuous motion characterizing our consciousness, but if we look carefully, we can observe the necessity of duration also in inorganic systems. By exploiting this property of inorganic life many forms of material computer have been devised over the millennia of human society. Such a form of computation harvests the natural ability of material to self-organise and enables a reading of problems of structural stability by means of a process of analogue form-finding.Analogue computers were material assemblages capable of multiplying the effectsof material computation, thus achieving enough problem-solving performance while retaining a specific aesthetic. Such computers were often very large. In fact, since antiquity, whole buildings have been conceived as architectural computing machines.A well-known example of this is the so-called ‘Nilometer’ in acient Egypt, as definedin Wikipedia. These beautifully ornamental structures were in fact large graduated tanks that, once connected by a complex network of pipelines to the river Nile, were able to measure the extent of each year’s flood. This was then used to calculate the taxation rate to be imposed on the local farmers benefitting from the flooding. In good years, large flooding would make the ground more fertile and higher taxes could be imposed. In this way, the river Nile’s topography, its network of infrastructural canals, the cultivate landscape and the Nilometers operates together as a large analogue computer, embedded within the material substratum of the landscape that it was designed to monitor and cultivate. The digital revolution has dramatically increasedthe proportion of power to size of analogue computation; a computer as complexand large as the Nilometer system can now be reasonably well simulated by a laptop processor. This progression has disconnected material computation from the fabric of our built environment. Similarly, zoning has disconnected other types of infrastructure, from our spatial awareness and consciousness. The architecture of computation is now beyond the scale of human spatial perception, and therefore of architecture and urban design. Fascinated by these analogue technologies and their potential. ecoLogicStudio began working on ecoMachines: “ecoMachines provide a material and operational framework to deal with change and transformation, the two main defining qualities of our new understanding of urban ecology; moreover they support interaction between heterogeneous systems, such as social, infrastructural, architectural and environmental ones; they allow us to sense, register and manipulate in our daily life the unfolding processes defining our cities, our houses and our artificial environments. ecoMachines turn us all into ecologists in the most operational sense of the term.” C.Pasquero, M. Poletto, Venice Biennale 2008 One of the most successful lines of ecoMachines [19] looked at photosynthetic cyno- bacteria, micro-algae, and defined our encounter with bio-computation. The firstseries of prototypes investigating photosynthetic architecture was called STEM and began in the summer of 2006 with a project for the London Architectural Biennalethat year. STEM London 2006 was built as a two-dimensional wall system made of hexagonal cells. Each cell was primed with a biological medium, which would interact with the CO2 and the incident solar radiation to activate photosynthesis. In turn this process would change the colour of the cell itself. The system as a whole acts as an urban oxygenator and the shades of green that each cell acquire are proportional to the amount of photosynthesis taking place in that cell itself. The system appears as a matrix of green bio-pixels able to carry information, such as incident solar radiation and quantities of photosynthesis and oxygenation. In STEM, information is materially observed and experienced, as well as simulated digitally. At the time we knew little of the many other properties of Cyanobacteria and of the existence of algae-farming asa growing industry, which we only discovered during subsequent projects. However,the relationship between biological growth and digital data that we began to explore at that time continues to be a part of the way we conceive photosynthetic architecture today. These types of relationship were further implemented in other projects, including the Cyber-Garden series [20], which looked at augmented architecture, and the MetaFolly pavilion (presented by ecoLogicStudio at ArchiLab9 exhibition at the FRAC in Orleans in 2012), which investigated the formation of languages between material and digital, human and non-human. Our research on biologically inspired computation and biotechnologies converged with our aim of reconciling these different approaches in the form of bio-digital architecture, which led us to discover the slime-mould Physarum Polycephalum. The first project to deploy the intelligence of PP looked at extending these ideas from the realm of computation and biology to urban design and was called ‘Physa-City’.The project emerged from a contemporary paradox of urban design: today, in the urban age, rather than talking about individual cities we should talk of one huge global ‘urbansphere’ that wraps around the world engaged in constant exchange with the natural biosphere at an incredible rate, and it has become near-impossible to tell these two global systems apart. We asked: How can we design this so that it becomes more efficient, resilient and adaptive? Can we re-conceive it as the bio-city of the future, in such a way that it can operate as a co-evolutionary system with the biosphere? And, if so, how? What could be a model for this biocity of the future and what form mightit take? It turned out that we could find a model in the most unlikely of creatures, the slime mould Physarum Polycephalum. This is one of the simplest creatures on earth, a protist; in fact, it is a single-cell organism. How could one cell be a blueprint for a city? Well, of course it is not just any cell. within this cell we can find thousands, if not millions of nuclei; they float in a protoplasmic liquid. The whole thing is kept together by a membrane of actin. In the plasmodium phase, when enough nutrients are to be found, the membrane stretches to take on virtually any shape. Further, the nuclei interact with each other and the environment via chemical reactions that generate gradients of pressure that in turn regulate the protoplasmic flows.PP can sense locally the presence and amount of nutrients in a given place and reactto it. It also leaves traces in the environment that constitute a form of spatial and distributed memory, which the slime mould develops to optimise its behaviour. This slime mould is an example of embedded computation. Scientists discovered that this simplest of organisms can perform, thanks to this distributed memory, extremely sophisticated tasks such as network optimisations, nutrient regulation, and anticipate events. These are precisely the kind of tasks that urban systems are required to perform and optimise all of the time. However, the way slime mould achieves this is unlike any urban system, since it is the product of emergent collective behaviour and distributes spatial memory rather than applying top-down planning. We asked: How can we explore and apply this behaviour in a real urban context? To begin exploring this question our research team developed a new kind of urban design apparatus, which we tested on a prototypical scenario in the copper mining corridor of Arizona, USA. The apparatus enabled us to feed the slime mould with data feeds from the territory itself. In one of these experiments, a slime mould is introduced onto a 3D-printed petri dish that reproduces the topography of the copper mining region near the city of Phoenix, Arizona. The slime mould is kept wet and food is dropped into the corresponding locations of present and future mining. The food is also coloured accordingly to the specific mineral content of that mine. As the mould expands to reach out for food it forms a network and begins dissolving and distributing the nutrients along it. A high-resolution webcam captures the mould’s behaviour, morphology and the colour nuances at any moment in time. The resulting network is optimised similar to human- made networks although the behaviour involved in achieving the solution is altogether different; and it is this behaviour that interested us. We looked at ways of capturing the behaviour and plotting it back onto the territory. To do this, we developed high- resolution images and a digital code able to recognise the morphology of the slime mould at any moment in time and to simulate the swarm of nuclei as they move along the network. Our virtual agents in the simulation would leave traces just as the real mould would, recreating the form of spatial memory that enables the slime mould to compute, optimise and anticipate. The emergent masterplan drawings appear fuzzy, their edges in continuous fluctuation, and as you zoom in you begin to discover more details and areas of more stable settlement. These representations are in constant development and reveal a new urban morphogenesis that operates within the milieu or a specific environment. This type of project has allowed us to evolve our understanding of biological computation, enabling us to develop, among other things, a system in which biological systems provide a model to be abstracted and an apparatus where biological organisms become co- designers. Cognition in Architecture. How does this form of computation relate to the current paradigm of digital design? And what kind of paradigm shift would this entail? In 2012, when I started working with PP, I did so within the framework of the Bartlett’s BPRO, a postgraduate program directed by Frederic Migayoru and focused on the application of recent findings in computation to the design discipline. My approach investigates methods of bio-computation within the design realm. It also questions the linear association between computation and digital design in order to propose a biodigital paradigm where the relevance of post-natural patterns, in their virtual as well as material form, assume a cognitive value. I will refer to the field of unconventional computing, cybernetic as well as neural science, and in this chapter will focus on the translation of findings in analogue computation, as well as unconventional computing, to architecture and urban design as a cognitive engine. The long history of computational design, technologies and theories has marked and shaped theway we think, design and practice architecture, with regard also to those pioneers in different periods who have disrupted the discipline and explored different directions from the mainstream. Here, we undertake to understand the word ‘computation’ inits synthetic Latin origin ‘com-puto’ (know-with) and to suggest alternative modes of “puto”, i.e. alternative modes of knowing. The first human machines which have been called ‘computer’ relied on the capability of human beings, mainly women, to compile schematic documents to be fed into the digital computer, it was a human-machine hybrid—a form of bio-digital computation. The discipline of architecture and urban design has evolved independently from any practice of bio-digital computation. So, even while it is becoming possible to simulate complex organisms like cities throughthe application of specifically customised algorithms, the main obstacle to their effective application to the design of our urban environment has been overcoming the conceptual difficulty of expressing urban and architectural design problems in the form of algorithmically solvable questions; in other words, to embed these algorithms in a coherent form of material organization. We are therefore seeking to move away from thinking of algorithms as problem-solving machines and toward algorithms as cognitive systems.Half a century ago a group of pioneering architects mobilised computational theory and technology and succeeded not only in revising the way we draft and present architecture, but also showed architects and designers how to rethink design methods. The then newly-available technologies, mainly in computational frameworks, allowed for the translation of ideological critiques and strong theoretical points into design methodologies. Computation was a means with which to prove theory. Inspired by research in cybernetics and neuroscience, architects such as Johan Fraser and scientists such as Gordon Pask became interested in the potential changes that the computer could bring to epistemological questions involving research on cognition and learning. It is precisely the need to understand the process through which an organism learns, rather than uncovering the hidden structure of cities and picturesque villages, that drive our research. We therefore argue that computational design should not evolve in a homogenous and linear manner, but through transversal connections with technology, whereby alternative theories are developed, different ways of thinking have been constructed and novel orientations for the discipline of architecture are explored.What needs to be re-oriented through technology is the thinking that sees in the technology a way of implementing norms, standards and types through the mimesis of an ideal naturalism. I refer here to Christopher Alexander’s understanding of the urban phenomenon as an actual organism [21] that is organised my means of specific part to whole relationships; this happens by means of local actions, through the naturalization of design process and the normativity of the model. In the light of this we should be reminded of Frederick Migayrou’s claim that: “We have lost count of the number of studies that [...] endeavoured to apply the theories of complex systems and the models of biological organization to the urban arena. Behind the idea of the ‘fractal city’, there is often a normativeness where the models seem to stand in for the old norms of geometric space.” (Migayrou 2000) [22] In our epistemological quest Physarum Polycephalum is investigated, a non-neuronal thinking entity that has its own biases and its own workings. These functions may be no more objective or original than our own but are certainly different. In this way, alien modes of reasoning may be introduced into design methodologies. Polycephalum does not give us what we are missing optimality and objectivity. On the contrary, it becomes the locus for an escape towards alien forms of invention. Audrey Dussutour [23] of CNRS in France, has recently been demonstrating the cognitive capability of the PP and other microorganisms, discovering through experimental tests that cognition is not restricted to organisms holding a complex neural network or brain. Indeed, before the emergence of this recent debate on PP’s cognitive capabilities, the slime mould had already been investigated by computer scientists for its computational skills. This type of research began in the 1980s in Japan and has been led in Europe by the work of A. Adamatzky, who has defined the computation of the PP as ‘unconventional computation’. In his ‘Physarum Machines: Computers from Slime Mould’, Adamatzky frames the aims of this research, arguing that “unconventional computing aims to uncover novel principles of efficient information processing and computation in physical, chemical and biological systems, to develop novel non-standard algorithms and computing architectures, and also to implement conventional algorithms in non-silicon, or wet, substrates.’ (Adamatzky 2010). [24] Tero et al.(2006) [25] and Adamtzky (2010) are perhaps the best-known researchers in this field to have worked and produced research on the PP slide mould. However, as Steven Shaviro (2009) argues, their work can be summarised in terms of four main attitudes towards the programmable aspect of slime mould: 1) We can observe slime mould’s behaviour, work out algorithms and code those in a computer. Isomorphism between the actual behaviour of the organism and that of the computational results is the aim.2) We can treat the organism as a black box. It’s phenomenal behaviour, like that of foraging behaviour, can be mobilised into solving series of problems. Our epistemological blind spot concerning the process that triggers the actual behaviour does not affect the computation.3) We can also proceed by connecting the substrate on which the slime mould acts with electrical input and outputs. In this sense, the slime mould plays a two-fold role where it is used both as electrical conductor and as computing device. 4) We can treat the oscillatory behaviour of the organism as ‘NOT’ and ‘NAND’ logic gates capable of computing complex tasks.With this classification of attitudes in mind, we may also investigate in another direction. We recognise, in line with S. Shaviro’s argument, the alien intelligence within the slime mould and therefore we argue that the above computational approaches are all too human. Human cognitive biases are imposed on the slime mould’s behaviour and solutions are given within human defined problems. The value that we see in the Polycephalum’s behaviour is in its capacity to problematize the operation of its own biases and its own capacity to discriminate its world. It is within this approach that we treat unconventional computing with slime moulds as speculative apparatuses. We are not seeking to extract efficient models to structure our worlds from our research on the Polycephalum, nor to reveal any hidden structures upon which our worldsmay depend. We are instead concerned with its capacity to improvise a solutionto an encounter, not in its generality, but in its singularity. This intelligence without neural networks exhibits a form of surplus value that affects and orients thinking, and that can suggest alternative modes of reasoning. It is this surplus value that affects Polycephalum’s behaviour and therefore affords it the potential to escape from its stereotypical responses. Contrary to what has been argued by previous generations of computational designers, such as example Bernard Cache, [26] it is not enough to argue for non-standard modes and relations of production, but to find in the forces of production lines of escape. We therefore argue that unconventional computation is necessary as the next phase of design computation. What we have attempted to present here both speculatively and operationally is the behaviour of Physarum Polycephalum in terms of the modes of re-oriention it offers for design method and information processing. Steven Shaviro writes in ‘Discognition’: “Physarum thus offer a simpler instance of what, in the case of the human beings, has been called the extended mind. According to extended mind theory, cognition does not take place only in the brain, but involves the ‘coupling of biological organisms and external resources.’ My mental acts extend well beyond the limits of my own brain and body.” [27] If we understand slime moulds as a form of intelligence, if we understand cynobacteria as a form of intelligence, if we understand AI as a form of intelligence, and if what matters is the language which manifests itself through material and digital patterns, then: How are we as humans and designers to enter this conversation? Can we remain simply a human figure in a rendered image, the hand that holds the pencil, the body that wears the oculus? If we are to play a part in the game and participate in human to non-human forms of conversation, it may not be enough for us to take a non-anthropocentric perspective. It may be necessary to rethink the way we describe ourselves as species and as bodies. Andy Clark, in his book ‘Natural Born Cyborg’, provides an intriguing description and possible redefinition of the self in this situation: “There is no self, if by self we mean some central cognitive essence that makes me who and what I am. In its place there is just the ‘soft self’: a rough-and-tumble, control- sharing coalition of processes—some neural, some bodily, some technological—and an ongoing drive to tell a story, to paint a picture in which ‘I’ am the central player. [...] The notion of a real, yet wafer-thin self is a profound mistake. It is a mistake that blinds us to our real nature and leads us to radically undervalue and misconceive the roles of context, culture, environment, and technology in the constitution of individual human persons.’ [28] The definition of the Anthropocene, together with recent technological developments in synthetic biology and artificial intelligence, presents scenarios in which the boundaries between natural and artificial, landscape and city, human and non-human realms are blurred. In this context, the object of architecture becomes ambiguous and its aesthetic language now embodies feelings of estrangement, discomfort and disruption. With this design method I propose a productive form of alienation where, alien to us, micro-organisms such as bacteria, fungi, spiders and moulds can act both as a be- havioural model for architecture and as active agents of architectural production. This has direct influence on the ever-critical relationship between form and performance in contemporary disciplinary discourse. With the goal of unpacking this influence, I will begin by observing the effects of collective intelligence in natural formations such as coral colonies, ants’ colonies and other biological micro-organisms. I will then extend these observations to the relationship between the human and the in-human in the context of contemporary cities. Over the years my practice has become increasingly concerned with the actualisation of this morphogenetic paradigm in architectural and urban design. “Through experimenting with biological organisms and digital apparatus we envision a new generation of bio-digital ‘designed prototypes’ with transformative agency for an architectural discipline yet to come.” It is a process that has no beginning and no end; rather it evolves in time from con- ception to construction and beyond, throughout the architectural life cycle. This notion implies moving away from ideas of responsive or adaptive environments toward devel- oping participatory and cybernetic frameworks where the observer is no longer a mere user but becomes an active co-creator of the spaces he or she is inhabiting. This shift makes novelty possible as a new kind of spatial conversation emerges with unexpected consequences for the way space is perceived and utilized. Ultimately computational design strategies are recast in a broader practice of embedding bio-digital intelligence into physical space. This work represents an attempt to ‘culturalize’ the in-human, that is to redescribe the boundaries of generative design practices beyond human rationality and creativity. Architecture thus begins to operate at multiple interconnected scales at once, from the micro-regimes of algae cells to the macro-behaviours of smart urban infrastructures, expanding both its scope and reach. In the future, all architectural protocols should seek to enable this extended participation and embody its artificial intelligence—a form of intelligence that is distributed into the fabric of the spaces we inhabit and that is co-evolutionary to them. overcome the positivistic functionalisation of biomimetic models and biotechnological solutions. Referring to the work of Bergson and Bateson, which I discussed in chap-ter one, I see optimal solutions as only a temporary state of otherwise continuously- dynamic systems. My aim is therefore to integrate scientific findings within a design method. That is, to define through architectural experimentation a spatial language able to engage in dialogue with the complex systems that constitute global cities and their infrastructures. Through a more complex dialogue I am able to influence the ten- dency of the system’s overall dynamic rather than simply develop a static and therefore necessarily temporary solution. The design experimentation described here follows a scientific approach and is cou- pled with a philosophical interpretation of its results. Our current civilization is the product of an intense influx of energy into the world delivered by the discovery of oil and other hydrocarbons. The storage of such energy is the product of an immense catastrophe that took place millions of years ago and killed a vast number of living organisms. Their complex structures were then digested and processed by bacteria for millions of years under specific conditions of pressure and temperature; the energy output of this process was stored as oil. The Polycephalum Design Method strives to recreate this cycle from waste to energy in a man-made structure within the urban milieu, re-defining the notions of waste, pollution and renewable energy in the pro- cess. It confronts green ideology as much as it engages with science and technology to intensify and accelerate our symbiotic relationship with the microbiological world. It abstracts the material articulation of a digestive apparatus as much as it unfolds its material effects in space and time. It overcomes disciplinary boundaries to define new sciences and new languages to interact with the planet we inha question the current relationship between research and practice in architecture, be- tween ethic and aesthetic as well as the role architectural design must play in articulat- ing more effective solutions to the current ecological crises. The current ecological crises has been often accompanied by political activism, this thesis implicitly argues that this approach is all too human, and that a different per- spective is needed whereas we ‘human’ with our ‘polis’ (city in Greek) are not the ‘cen- tre of’ and cannot be the ‘solution to’ the problem, the problem must be addressed from a non-anthropocentric perspective where bacteria, fungi and artificial machine are given a bio-citizenship. Aesthetic acquires, here, through material meta-languag- es, an ethical role, enabling the emergence of in-human cities. In term of format this thesis allowed the definition of the forth phase of my practice which started in 2019, and it’s characterized by the ‘PhotoSynthEtica’ project and consortium, springing from a stronger theoretical and methodological framework which has been set up through the PhD development. This phase is characterized by a restructuring of the commercial as well as academic branches of the Polycephalum, accompanied by the definition of three main streams of projects for the practice co- inciding with bio-digital sculptures, photosynthetic façade systems, biotic urban in- terfaces, and supported by three main areas of research: Hardware (digital produce substratum), Software (computational interface), Wetware (biological medium). Methodologically the PRS process (discussed in the introduction) has been essential to the development of this thesis, with each symposium contributing a new step in its full development. Now that I have reached the conclusion of this journey, I can see how the first two steps were to me probably the most difficult but also the most significant. The first symposium, whose impact is discussed in the prologue, was dedicated to case studies; in other words: “putting the venturous practice’s body of work on the table”.I began to ‘map’ the practice, initiating a process of identifying the tacit knowledge it embodies. For the second symposium I collated evidence of the community around my practice. This phase enabled me to point out the key influences on my work, situating bit. it at the intersection of several disciplines and pointing out their relevance. This was probably the most difficult of the symposia for me. However, it sparked a renewed in- terest in further investigating the community of practices around my work, which even- tually led me to the role of Head Curator of the Tallinn Architectural Biennale 2017. The main curatorial exhibition was conceived as a laboratory of experimentation where multiple designers, scientists and researchers collaborated and confronted each other on a specific design subject I had formulated. It is through this curatorial project that I started to think of my expanded community of practices as inclusive of non-hu- man agents. For the third symposia, focussed on identifying public behaviours, I included trans- disciplinary impacts, which is particularly significant with respect to the relevance of teaching-based research. The culmination of this process was the identification of the transformative triggers that have marked the evolution of my practice. These includes comments received about my projects, as well as partial failures. An example of this is the recurrent misunderstanding of the role of the aesthetic in my work, especially in re- lationship to those projects that referred more directly to infrastructural topics, such as urban farming. This triggered a deeper investigation into bio-computation and cogni- tion, envisioning specific aspects of tacit knowledge I had not previously recognised. The research process itself has consisted of three main activities: First, looking back at an existing body of work and reflecting on the practice as the work has emerged from it (‘reflection on’). Second, since this reflection has taken place while continuing to practice, I have also been immersed in reflection on my current practice (‘reflection in’). Third, throughout the research activities I have kept my eyes on the future, think- ing toward future practices (‘reflection for’). [66] The phrase ‘venturous practice’, coined originally by Richard Blythe, builds on ‘ventur- ous Australia’, a phrase coined by Terry Cutler in 2008: “To be clear what I am inter- ested in when I talk about design research is the part of design that you might describe as being venturous—that is to say, the kind of designing that changes the way we think about our world and the ways in which we practice designing itself.” This process of reflection has enabled me to see my research practice in a different way. In traditional academic environments designers often must cope with a para- digm that relies on a well-formed question and a problem-solution framework. But a research method which rests on a single research question would be inappropriate for my practice as it promotes the kind of research best-suited to investigating a fully formed and pre-determined environment. In this realm, design research cannot be, by definition, speculative. At most it can extrapolate solutions from an already defined space of possibilities. With this disserta- tion I propose a radical shift of this model, to mobilize what we can call “differential prototypes” (presented in chapter four) within a practice-based research environment. My aim is to question the constitution of the problems, thus turning the act of question- ing into an affirmative proposal. Design in this framework contributes radically to an epistemological reformulation of the problem and, consequently, to the production of new knowledge. This mode of design investigation is carried forward to the application and to the exploration of specific design prototypes. These transgress the traditional technologi- cal boundaries assigned to prototyping in the field of architecture and product design. With these material systems, I propose a re-appraisal of experimentation through diverse apparatuses and machines. These non-human prototypes become the means through which I problematize the problem and instigate design’s transformative agen- cy. For example, the experimentation I have conducted with biological organisms like micro-algal cultures or spiders’ silk, and the development of related technological apparatuses, like the cyber-gardens, enables a description of the problem of carbon neutrality from a non-human perspective. My practice has designed a series of ur- ban bio-digital prototypes which act as a research medium, structuring a continuous feedback between research and practice as well as between design brief and research question, ultimately defining the way my practice operates. Every project that I have developed with my practice and presented in this thesis can then be defined as a “weird prototype”, referring to the concept of weird media and the idea of “the mediation of what cannot be mediated.” [68] On those terms my pro- jects promote a type of communication that cannot be mediated and therefore can be achieved only by negation, or in this case a negation of dualities such as the subject- object, the human-nonhuman, the formal-functional and the ethical-aesthetic. “This is quite different in principle from the modern view of mediation given by cyber- netics and information theory. There one has a mediation between two points within a single, shared, consensual reality. While there may also be messages, channels, send- ers, and receivers, in [weird] media have one important difference: the mediation is not between two points in a single reality, but between two realities.” [70] In this sense, weird media enables the expansion of the human sensorial domain and reconstructs a single human-prototype agent that transforms and augments the human to feel more than what is humanly possible. My ambition is thus to construct ‘intra- active’ ecologies, interfaces between us (humans) and those alien realities that sur- round us but which we cannot perceive, a prominent example being climate change. For example, ecoLogicStudio’s photosynthetic architectures become an apparatus for materializing speculative architectural environments that we can inhabit, while at the same time contributing to the re-metabolization of flow of carbon dioxide in the urban atmosphere. Thus, they don’t simply embody an interdisciplinary convergence or an ecology of participants. They constitute what can be defined as an intra-active field that constructs an ecology of participants. “There are individual independently existing entities or agents that pre-exist their acting upon one another. By contrast, the notion of ‘intra-action’ queers the familiar sense of causality (where one or more causal agents precede and produce an effect), and more generally unsettles the metaphysics of individualism.” [60] In this sense, these prototype projects become an assemblage of processes which contribute to the construction of the world. “It is through this turn [...] that the prototypes become alien and as such suggest a materialisation of creatures that not only overcomes the traditional distinctions of materialisation of creatures that not only overcomes the traditional distinctions of na- ture-culture, organic-inorganic, but opens a new path to design research as problem- making prior to problem-solving” C. Pasquero, M. Zaroukas, ‘Design Prototype’ [72] This ontological turn allows us to rethink the role of apparatuses and media in the de- sign process. Instead of researching the non-human world with in-human apparatuses to produce human knowledge as research by design would typically suggest, we turn that around and argue for the importance of prototypes in research as weird media. “The task of design research, as it is presented, is not finding a new or improved ver- sion of the world-for-us, and it is not to relentlessly pursue the phantom objectivity of the world-in-itself. The real challenge lies in confronting this enigmatic concept of the world-without-us, and understanding why this world-without-us continues to persist in the shadows of the world-for-us and the world-in-itself.” [73] That is a key realisation brought to the fore by this dissertation and which, I feel, will propel my research for years to come. “Inquiry and knowledge cannot be addressed by architectural objects and apparatuses as discrete objects in the word-for-us. In the world-without-us their intra-actions mate- rialise representations capable of having transformative agency in the world-for-us”. 





